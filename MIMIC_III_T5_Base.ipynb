{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFluu0QgVZsy"
      },
      "source": [
        "# Reliable Text-to-SQL Modeling on MIMIC III dataset without schema\n",
        "\n",
        "This Jupyter notebook serves as a comprehensive guide to run Text-to-SQL model for Electronic Health Records (EHRs).\n",
        "\n",
        "## Steps in This Jupyter Notebook\n",
        "- [x] Step 1: Initial Setup\n",
        "- [x] Step 2: Load and Prepare Datasets\n",
        "- [x] Step 3: Construct a Text-to-SQL Model\n",
        "- [x] Step 4: Model Evaluation\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Begin your journey with the EHRSQL task by following these structured steps (from Step 1 to Step 8). Each section is designed to guide you smoothly through the process, from setup to submission. We're eager to see the innovative solutions you'll bring to the field of Text-to-SQL modeling for electronic health records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuGjZ24xTJjp"
      },
      "source": [
        "## Step 1: Initial Setup\n",
        "\n",
        "As part of the intial setup we will pull code to Colab environment, install required Python dependencies and mount a GCP bucket to store intermediate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJAip9BJCRDy"
      },
      "source": [
        "Clone the GitHub Repository created for this project.  It is a modified version of the original EHRSQL repo created by the writers of the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhH44DzGXx98",
        "outputId": "f1fc41c0-97c1-46bf-c677-96adc8a2018b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Apr  3 13:37 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Apr  4 19:35 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 4 root root 4096 Apr  3 13:37 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Apr  3 13:37 \u001b[01;34msample_data\u001b[0m/\n",
            "/content/EHRSQL\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!rm -rf EHRSQL\n",
        "\n",
        "%ls -al\n",
        "\n",
        "# Cloning the GitHub repository\n",
        "!git clone -q https://github.com/BizUnix/EHRSQL.git\n",
        "%cd EHRSQL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyxAMm_ksM7a"
      },
      "source": [
        "Install Required Python Packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyFGlZ2wUBn2",
        "outputId": "337e15ac-925e-4ab8-fa46-4882bbf5df81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for func_timeout (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Installing dependencies\n",
        "\n",
        "!pip install -q transformers\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q func_timeout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwyXT96YsPJu"
      },
      "source": [
        "Use the `%load_ext` magic command to automatically reload modules before executing a new line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYYVQTvWZApy"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount a GCP bucket for storing intermediate results and output"
      ],
      "metadata": {
        "id": "l3Kv5rWEfuAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfJAT6kOzKKU",
        "outputId": "89d49e22-c4cd-4640-85d1-e8d0d76a60e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 5072k  100 5072k    0     0  5576k      0 --:--:-- --:--:-- --:--:-- 29.2M\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack gcsfuse_1.0.1_amd64.deb ...\n",
            "Unpacking gcsfuse (1.0.1) ...\n",
            "Setting up gcsfuse (1.0.1) ...\n"
          ]
        }
      ],
      "source": [
        "# prompt: get gcsfuse\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq -y fuse\n",
        "!curl -LO https://github.com/googlecloudplatform/gcsfuse/releases/download/v1.0.1/gcsfuse_1.0.1_amd64.deb\n",
        "!dpkg -i gcsfuse_1.0.1_amd64.deb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXbRLFgGYD0k",
        "outputId": "11fd850e-2e5e-4793-8bd7-99c73c27b333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "gcsfuse version 1.0.1 (Go version go1.20.5)\n",
            "I0404 19:37:24.807445 2025/04/04 19:37:24.807417 Start gcsfuse/1.0.1 (Go version go1.20.5) for app \"\" using mount point: /content/cse6250_h1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth;\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Replace with your project ID and bucket name\n",
        "!gcloud config set project striking-yen-455401-m9\n",
        "!gcsfuse -v\n",
        "\n",
        "!mkdir -p /content/cse6250_h1\n",
        "\n",
        "# Mount bucket\n",
        "!gcsfuse cse6250_h1 /content/cse6250_h1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4_7-GnpUc2d"
      },
      "source": [
        "## Step 2: Load Data and Prepare Datasets\n",
        "\n",
        "Now that we have our environment and paths set up, the next step is to load the data and prepare it for our model.  This involves preprocessing the MIMIC-III database and storing it into SQLLITE, reading the data from JSON files, splitting it into training and validation sets, and then initializing our dataset object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjdHdnbN_Ue7"
      },
      "source": [
        "### Preprocess MIMIC-III Data\n",
        "\n",
        "Use a secure copy of the MIMIC-III data on Google Cloud Platform bucket to preprocess data and create the SQLLITE Mimic III dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnprCPZhYG1d",
        "outputId": "face63ef-766d-4950-84a0-e0eabe407825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehrsql/preprocess\n",
            "Processing PATIENTS, ADMISSIONS, ICUSTAYS, TRANSFERS\n",
            "num_cur_patient: 0\n",
            "num_non_cur_patient: 1000\n",
            "num_patient: 1000\n",
            "PATIENTS, ADMISSIONS, ICUSTAYS, TRANSFERS processed (took 15.0231 secs)\n",
            "Processing D_ICD_DIAGNOSES, D_ICD_PROCEDURES, D_LABITEMS, D_ITEMS\n",
            "D_ICD_DIAGNOSES, D_ICD_PROCEDURES, D_LABITEMS, D_ITEMS processed (took 2.0864 secs)\n",
            "Processing DIAGNOSES_ICD table\n",
            "DIAGNOSES_ICD processed (took 2.9386 secs)\n",
            "Processing PROCEDURES_ICD table\n",
            "PROCEDURES_ICD processed (took 2.2436 secs)\n",
            "Processing LABEVENTS table\n",
            "LABEVENTS processed (took 200.0603 secs)\n",
            "Processing PRESCRIPTIONS table\n",
            "PRESCRIPTIONS processed (took 55.0977 secs)\n",
            "Processing COST table\n",
            "COST processed (took 5.326 secs)\n",
            "Processing CHARTEVENTS table\n",
            "[########################################] | 100% Completed | 24m 45s\n",
            "CHARTEVENTS processed (took 1496.1085 secs)\n",
            "Processing INPUTEVENTS_CV table\n",
            "INPUTEVENTS_CV processed (took 165.6402 secs)\n",
            "Processing OUTPUTEVENTS table\n",
            "OUTPUTEVENTS processed (took 35.9414 secs)\n",
            "Processing MICROBIOLOGYEVENTS table\n",
            "MICROBIOLOGYEVENTS processed (took 5.2078 secs)\n",
            "0               PATIENTS\n",
            "1             ADMISSIONS\n",
            "2        D_ICD_DIAGNOSES\n",
            "3       D_ICD_PROCEDURES\n",
            "4             D_LABITEMS\n",
            "5                D_ITEMS\n",
            "6          DIAGNOSES_ICD\n",
            "7         PROCEDURES_ICD\n",
            "8              LABEVENTS\n",
            "9          PRESCRIPTIONS\n",
            "10                  COST\n",
            "11           CHARTEVENTS\n",
            "12        INPUTEVENTS_CV\n",
            "13          OUTPUTEVENTS\n",
            "14    MICROBIOLOGYEVENTS\n",
            "15              ICUSTAYS\n",
            "16             TRANSFERS\n",
            "Name: name, dtype: object\n",
            "Processing complete!\n",
            "/content/ehrsql\n"
          ]
        }
      ],
      "source": [
        "%cd preprocess\n",
        "!python preprocess_db.py --data_dir \"/content/cse6250_h1/mimic-iii\" --db_name \"mimic_iii\"\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save SQLLite DB to GCP Bucket\n",
        "\n",
        "This will help skip the preprocess step upon restarts\n"
      ],
      "metadata": {
        "id": "xsNvhmZUUYSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Move SQLLite to Google Bucket\n",
        "!cp -r /content/ehrsql/dataset/ehrsql/mimic_iii/mimic_iii.sql* /content/cse6250_h1/\n"
      ],
      "metadata": {
        "id": "T67Z9ulJUdp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gTpu3aSU-yN"
      },
      "source": [
        "## Step 3: Construct a Text-to-SQL Baseline Model\n",
        "\n",
        "In this step, we set up and train a T5 model to translate natural language queries into SQL statements. The process involves several key stages including argument parsing, model initialization, data preparation, and the actual training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjVT23wFqKmB"
      },
      "source": [
        "### Check GPU and connect to Google Drive (to store output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmCZlERJpZMp",
        "outputId": "bc0d4d2f-ec43-4c2c-d2c2-465c56315072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch Version: 2.6.0+cu124\n",
            "GPU ID: 0\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"Torch Version: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_id = torch.cuda.current_device()\n",
        "    print(f\"GPU ID: {gpu_id}\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(gpu_id)}\")\n",
        "else:\n",
        "    print(\"No GPU detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the files /content/ehrsql/dataset/ehrsql/mimic_iii/mimic_iii.sql* exist\n",
        "# if they don't download them from Google Cloud Platform bucket\n",
        "\n",
        "!if [ ! -f \"/content/cse6250_h1/mimic_iii.sql\" ]; then\n",
        "  cp -r /content/cse6250_h1/mimic_iii.sql* /content/cse6250_h1/"
      ],
      "metadata": {
        "id": "NBNTFnbSjQUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAFqWuI9CfrI"
      },
      "source": [
        "### Training the (T5) Model\n",
        "\n",
        "Finally, we train the model on the dataset. The training process involves learning to generate SQL queries from textual descriptions through iterative forward and backward passes, loss computation, and parameter updates.  This version of the training use the pretrained T5 base model and trains it on the EHRSQL dataset without schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK1DTHQA7RMo",
        "outputId": "ddac87bb-fd42-4526-b667-2a82bea5ecfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_ehrsql_mimic3_t5_base__mimic3_valid  eval_ehrsql_mimic3_t5_base_schema__mimic3_valid\n"
          ]
        }
      ],
      "source": [
        "!rm -rf outputs/ehrsql_mimic3_t5_base\n",
        "!ls outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Xzqt1vpBIY",
        "outputId": "c7cf6521-d88a-4dbf-d680-ea2a92a6d6da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device: cuda:0\n",
            "2025-04-01 01:33:51 | INFO | Namespace(exp_name='ehrsql_mimic3_t5_base', load_model_path=None, device='cuda', num_workers=50, random_seed=0, report_every_step=50, eval_batch_size=8, save_every_step=-1, save_every_epoch=False, show_eval_sample=True, eval_every_step=5000, eval_metric='loss', keep_last_ckpt=-1, early_stop_patience=-1, training_data_ratio=1.0, bf16=False, use_wandb=False, wandb_project=None, dataset='ehrsql', db_id='mimic_iii', train_data_path='dataset/ehrsql/mimic_iii/train.json', valid_data_path='dataset/ehrsql/mimic_iii/valid.json', output_dir='outputs', output_file='prediction_raw.json', model_name='t5-base', db_path=None, add_schema=False, add_column_type=False, shuffle_schema=False, tables_path=None, condition_value=True, warmup_steps=0, total_epoch=-1, total_step=100000, train_batch_size=4, accumulation_steps=8, lr=0.0001, scheduler_steps=None, optim='adam', scheduler='fixed', max_grad_norm=1.0, weight_decay=0.1, init_weights=False, num_beams=5, max_length=512, repetition_penalty=1.0, length_penalty=1.0, early_stopping=True, num_samples=1, config='T5/config/ehrsql/training/ehrsql_mimic3_t5_base.yaml', CUDA_VISIBLE_DEVICES='0', mode='train')\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 45.9MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 5.85MB/s]\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 9.52MB/s]\n",
            "2025-04-01 01:33:56 | INFO | NumExpr defaulting to 12 threads.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "100% 9318/9318 [00:00<00:00, 55397.70it/s]\n",
            "100% 1122/1122 [00:00<00:00, 558377.92it/s]\n",
            "2025-04-01 01:34:03 | INFO | loaded 9318 training examples from dataset/ehrsql/mimic_iii/train.json\n",
            "2025-04-01 01:34:03 | INFO | loaded 760 valid examples from dataset/ehrsql/mimic_iii/valid.json\n",
            "2025-04-01 01:34:04.072522: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-01 01:34:04.090183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743471244.112306    8224 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743471244.118972    8224 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-01 01:34:04.141287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 892M/892M [00:08<00:00, 104MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.24MB/s]\n",
            "start training\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 50 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "2025-04-01 01:34:29 | INFO | epoch: 1 (step: 50) | train loss: 0.757485 | lr: 0.000100\n",
            "2025-04-01 01:34:35 | INFO | epoch: 1 (step: 100) | train loss: 0.374449 | lr: 0.000100\n",
            "2025-04-01 01:34:41 | INFO | epoch: 1 (step: 150) | train loss: 0.275505 | lr: 0.000100\n",
            "2025-04-01 01:34:47 | INFO | epoch: 1 (step: 200) | train loss: 0.215482 | lr: 0.000100\n",
            "2025-04-01 01:34:53 | INFO | epoch: 1 (step: 250) | train loss: 0.175698 | lr: 0.000100\n",
            "2025-04-01 01:34:59 | INFO | epoch: 1 (step: 300) | train loss: 0.142932 | lr: 0.000100\n",
            "2025-04-01 01:35:05 | INFO | epoch: 1 (step: 350) | train loss: 0.127306 | lr: 0.000100\n",
            "2025-04-01 01:35:11 | INFO | epoch: 1 (step: 400) | train loss: 0.108796 | lr: 0.000100\n",
            "2025-04-01 01:35:17 | INFO | epoch: 1 (step: 450) | train loss: 0.093196 | lr: 0.000100\n",
            "2025-04-01 01:35:23 | INFO | epoch: 1 (step: 500) | train loss: 0.081630 | lr: 0.000100\n",
            "2025-04-01 01:35:29 | INFO | epoch: 1 (step: 550) | train loss: 0.070698 | lr: 0.000100\n",
            "2025-04-01 01:35:36 | INFO | epoch: 1 (step: 600) | train loss: 0.065214 | lr: 0.000100\n",
            "2025-04-01 01:35:41 | INFO | epoch: 1 (step: 650) | train loss: 0.058325 | lr: 0.000100\n",
            "2025-04-01 01:35:47 | INFO | epoch: 1 (step: 700) | train loss: 0.053231 | lr: 0.000100\n",
            "2025-04-01 01:35:54 | INFO | epoch: 1 (step: 750) | train loss: 0.049107 | lr: 0.000100\n",
            "2025-04-01 01:36:00 | INFO | epoch: 1 (step: 800) | train loss: 0.047089 | lr: 0.000100\n",
            "2025-04-01 01:36:06 | INFO | epoch: 1 (step: 850) | train loss: 0.047034 | lr: 0.000100\n",
            "2025-04-01 01:36:12 | INFO | epoch: 1 (step: 900) | train loss: 0.042601 | lr: 0.000100\n",
            "2025-04-01 01:36:18 | INFO | epoch: 1 (step: 950) | train loss: 0.039270 | lr: 0.000100\n",
            "2025-04-01 01:36:24 | INFO | epoch: 1 (step: 1000) | train loss: 0.036195 | lr: 0.000100\n",
            "2025-04-01 01:36:30 | INFO | epoch: 1 (step: 1050) | train loss: 0.034600 | lr: 0.000100\n",
            "2025-04-01 01:36:36 | INFO | epoch: 1 (step: 1100) | train loss: 0.034364 | lr: 0.000100\n",
            "2025-04-01 01:36:42 | INFO | epoch: 1 (step: 1150) | train loss: 0.032368 | lr: 0.000100\n",
            "2025-04-01 01:36:48 | INFO | epoch: 1 (step: 1200) | train loss: 0.032167 | lr: 0.000100\n",
            "2025-04-01 01:36:54 | INFO | epoch: 1 (step: 1250) | train loss: 0.035039 | lr: 0.000100\n",
            "2025-04-01 01:37:00 | INFO | epoch: 1 (step: 1300) | train loss: 0.025667 | lr: 0.000100\n",
            "2025-04-01 01:37:06 | INFO | epoch: 1 (step: 1350) | train loss: 0.025319 | lr: 0.000100\n",
            "2025-04-01 01:37:12 | INFO | epoch: 1 (step: 1400) | train loss: 0.025843 | lr: 0.000100\n",
            "2025-04-01 01:37:18 | INFO | epoch: 1 (step: 1450) | train loss: 0.023868 | lr: 0.000100\n",
            "2025-04-01 01:37:24 | INFO | epoch: 1 (step: 1500) | train loss: 0.023395 | lr: 0.000100\n",
            "2025-04-01 01:37:30 | INFO | epoch: 1 (step: 1550) | train loss: 0.022936 | lr: 0.000100\n",
            "2025-04-01 01:37:36 | INFO | epoch: 1 (step: 1600) | train loss: 0.021265 | lr: 0.000100\n",
            "2025-04-01 01:37:42 | INFO | epoch: 1 (step: 1650) | train loss: 0.020204 | lr: 0.000100\n",
            "2025-04-01 01:37:48 | INFO | epoch: 1 (step: 1700) | train loss: 0.019479 | lr: 0.000100\n",
            "2025-04-01 01:37:55 | INFO | epoch: 1 (step: 1750) | train loss: 0.018744 | lr: 0.000100\n",
            "2025-04-01 01:38:01 | INFO | epoch: 1 (step: 1800) | train loss: 0.016555 | lr: 0.000100\n",
            "2025-04-01 01:38:07 | INFO | epoch: 1 (step: 1850) | train loss: 0.016609 | lr: 0.000100\n",
            "2025-04-01 01:38:13 | INFO | epoch: 1 (step: 1900) | train loss: 0.015490 | lr: 0.000100\n",
            "2025-04-01 01:38:19 | INFO | epoch: 1 (step: 1950) | train loss: 0.015762 | lr: 0.000100\n",
            "2025-04-01 01:38:25 | INFO | epoch: 1 (step: 2000) | train loss: 0.014942 | lr: 0.000100\n",
            "2025-04-01 01:38:31 | INFO | epoch: 1 (step: 2050) | train loss: 0.015324 | lr: 0.000100\n",
            "2025-04-01 01:38:37 | INFO | epoch: 1 (step: 2100) | train loss: 0.014413 | lr: 0.000100\n",
            "2025-04-01 01:38:43 | INFO | epoch: 1 (step: 2150) | train loss: 0.013789 | lr: 0.000100\n",
            "2025-04-01 01:38:49 | INFO | epoch: 1 (step: 2200) | train loss: 0.014153 | lr: 0.000100\n",
            "2025-04-01 01:38:55 | INFO | epoch: 1 (step: 2250) | train loss: 0.012997 | lr: 0.000100\n",
            "2025-04-01 01:39:01 | INFO | epoch: 1 (step: 2300) | train loss: 0.012208 | lr: 0.000100\n",
            "2025-04-01 01:39:09 | INFO | epoch: 2 (step: 2350) | train loss: 0.012224 | lr: 0.000100\n",
            "2025-04-01 01:39:15 | INFO | epoch: 2 (step: 2400) | train loss: 0.012801 | lr: 0.000100\n",
            "2025-04-01 01:39:21 | INFO | epoch: 2 (step: 2450) | train loss: 0.011712 | lr: 0.000100\n",
            "2025-04-01 01:39:27 | INFO | epoch: 2 (step: 2500) | train loss: 0.011602 | lr: 0.000100\n",
            "2025-04-01 01:39:33 | INFO | epoch: 2 (step: 2550) | train loss: 0.011328 | lr: 0.000100\n",
            "2025-04-01 01:39:39 | INFO | epoch: 2 (step: 2600) | train loss: 0.009983 | lr: 0.000100\n",
            "2025-04-01 01:39:46 | INFO | epoch: 2 (step: 2650) | train loss: 0.010568 | lr: 0.000100\n",
            "2025-04-01 01:39:52 | INFO | epoch: 2 (step: 2700) | train loss: 0.010173 | lr: 0.000100\n",
            "2025-04-01 01:39:58 | INFO | epoch: 2 (step: 2750) | train loss: 0.010270 | lr: 0.000100\n",
            "2025-04-01 01:40:04 | INFO | epoch: 2 (step: 2800) | train loss: 0.010072 | lr: 0.000100\n",
            "2025-04-01 01:40:10 | INFO | epoch: 2 (step: 2850) | train loss: 0.009945 | lr: 0.000100\n",
            "2025-04-01 01:40:16 | INFO | epoch: 2 (step: 2900) | train loss: 0.009029 | lr: 0.000100\n",
            "2025-04-01 01:40:22 | INFO | epoch: 2 (step: 2950) | train loss: 0.009449 | lr: 0.000100\n",
            "2025-04-01 01:40:28 | INFO | epoch: 2 (step: 3000) | train loss: 0.008765 | lr: 0.000100\n",
            "2025-04-01 01:40:34 | INFO | epoch: 2 (step: 3050) | train loss: 0.008311 | lr: 0.000100\n",
            "2025-04-01 01:40:40 | INFO | epoch: 2 (step: 3100) | train loss: 0.008412 | lr: 0.000100\n",
            "2025-04-01 01:40:46 | INFO | epoch: 2 (step: 3150) | train loss: 0.008393 | lr: 0.000100\n",
            "2025-04-01 01:40:52 | INFO | epoch: 2 (step: 3200) | train loss: 0.008634 | lr: 0.000100\n",
            "2025-04-01 01:40:59 | INFO | epoch: 2 (step: 3250) | train loss: 0.007991 | lr: 0.000100\n",
            "2025-04-01 01:41:05 | INFO | epoch: 2 (step: 3300) | train loss: 0.006895 | lr: 0.000100\n",
            "2025-04-01 01:41:11 | INFO | epoch: 2 (step: 3350) | train loss: 0.006874 | lr: 0.000100\n",
            "2025-04-01 01:41:17 | INFO | epoch: 2 (step: 3400) | train loss: 0.008159 | lr: 0.000100\n",
            "2025-04-01 01:41:23 | INFO | epoch: 2 (step: 3450) | train loss: 0.007358 | lr: 0.000100\n",
            "2025-04-01 01:41:29 | INFO | epoch: 2 (step: 3500) | train loss: 0.006857 | lr: 0.000100\n",
            "2025-04-01 01:41:35 | INFO | epoch: 2 (step: 3550) | train loss: 0.006848 | lr: 0.000100\n",
            "2025-04-01 01:41:41 | INFO | epoch: 2 (step: 3600) | train loss: 0.006642 | lr: 0.000100\n",
            "2025-04-01 01:41:47 | INFO | epoch: 2 (step: 3650) | train loss: 0.006770 | lr: 0.000100\n",
            "2025-04-01 01:41:53 | INFO | epoch: 2 (step: 3700) | train loss: 0.007277 | lr: 0.000100\n",
            "2025-04-01 01:41:59 | INFO | epoch: 2 (step: 3750) | train loss: 0.005796 | lr: 0.000100\n",
            "2025-04-01 01:42:05 | INFO | epoch: 2 (step: 3800) | train loss: 0.006366 | lr: 0.000100\n",
            "2025-04-01 01:42:11 | INFO | epoch: 2 (step: 3850) | train loss: 0.006430 | lr: 0.000100\n",
            "2025-04-01 01:42:17 | INFO | epoch: 2 (step: 3900) | train loss: 0.006143 | lr: 0.000100\n",
            "2025-04-01 01:42:23 | INFO | epoch: 2 (step: 3950) | train loss: 0.006281 | lr: 0.000100\n",
            "2025-04-01 01:42:29 | INFO | epoch: 2 (step: 4000) | train loss: 0.006379 | lr: 0.000100\n",
            "2025-04-01 01:42:35 | INFO | epoch: 2 (step: 4050) | train loss: 0.005510 | lr: 0.000100\n",
            "2025-04-01 01:42:41 | INFO | epoch: 2 (step: 4100) | train loss: 0.005099 | lr: 0.000100\n",
            "2025-04-01 01:42:47 | INFO | epoch: 2 (step: 4150) | train loss: 0.005901 | lr: 0.000100\n",
            "2025-04-01 01:42:53 | INFO | epoch: 2 (step: 4200) | train loss: 0.005671 | lr: 0.000100\n",
            "2025-04-01 01:42:59 | INFO | epoch: 2 (step: 4250) | train loss: 0.005187 | lr: 0.000100\n",
            "2025-04-01 01:43:05 | INFO | epoch: 2 (step: 4300) | train loss: 0.005213 | lr: 0.000100\n",
            "2025-04-01 01:43:11 | INFO | epoch: 2 (step: 4350) | train loss: 0.005082 | lr: 0.000100\n",
            "2025-04-01 01:43:17 | INFO | epoch: 2 (step: 4400) | train loss: 0.005029 | lr: 0.000100\n",
            "2025-04-01 01:43:23 | INFO | epoch: 2 (step: 4450) | train loss: 0.005201 | lr: 0.000100\n",
            "2025-04-01 01:43:29 | INFO | epoch: 2 (step: 4500) | train loss: 0.005034 | lr: 0.000100\n",
            "2025-04-01 01:43:35 | INFO | epoch: 2 (step: 4550) | train loss: 0.004708 | lr: 0.000100\n",
            "2025-04-01 01:43:41 | INFO | epoch: 2 (step: 4600) | train loss: 0.005213 | lr: 0.000100\n",
            "2025-04-01 01:43:47 | INFO | epoch: 2 (step: 4650) | train loss: 0.004968 | lr: 0.000100\n",
            "2025-04-01 01:43:55 | INFO | epoch: 3 (step: 4700) | train loss: 0.004904 | lr: 0.000100\n",
            "2025-04-01 01:44:01 | INFO | epoch: 3 (step: 4750) | train loss: 0.004595 | lr: 0.000100\n",
            "2025-04-01 01:44:07 | INFO | epoch: 3 (step: 4800) | train loss: 0.004322 | lr: 0.000100\n",
            "2025-04-01 01:44:13 | INFO | epoch: 3 (step: 4850) | train loss: 0.004764 | lr: 0.000100\n",
            "2025-04-01 01:44:19 | INFO | epoch: 3 (step: 4900) | train loss: 0.004350 | lr: 0.000100\n",
            "2025-04-01 01:44:25 | INFO | epoch: 3 (step: 4950) | train loss: 0.004174 | lr: 0.000100\n",
            "2025-04-01 01:44:31 | INFO | epoch: 3 (step: 5000) | train loss: 0.003948 | lr: 0.000100\n",
            "2025-04-01 01:44:38 | INFO | epoch: 3 (step: 5000) | valid_loss: 0.044672 | valid_loss: 0.044672\n",
            "2025-04-01 01:44:42 | INFO | epoch: 3 (step: 5000) | best metric updated (loss) - 0.044672\n",
            "2025-04-01 01:44:53 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t3.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime,'start of year') = datetime(current_time,'start of year','-1 year') )\n",
            "2025-04-01 01:44:59 | INFO | epoch: 3 (step: 5050) | train loss: 0.004536 | lr: 0.000100\n",
            "2025-04-01 01:45:05 | INFO | epoch: 3 (step: 5100) | train loss: 0.003826 | lr: 0.000100\n",
            "2025-04-01 01:45:11 | INFO | epoch: 3 (step: 5150) | train loss: 0.003754 | lr: 0.000100\n",
            "2025-04-01 01:45:17 | INFO | epoch: 3 (step: 5200) | train loss: 0.003799 | lr: 0.000100\n",
            "2025-04-01 01:45:23 | INFO | epoch: 3 (step: 5250) | train loss: 0.004181 | lr: 0.000100\n",
            "2025-04-01 01:45:29 | INFO | epoch: 3 (step: 5300) | train loss: 0.003956 | lr: 0.000100\n",
            "2025-04-01 01:45:35 | INFO | epoch: 3 (step: 5350) | train loss: 0.004085 | lr: 0.000100\n",
            "2025-04-01 01:45:42 | INFO | epoch: 3 (step: 5400) | train loss: 0.003827 | lr: 0.000100\n",
            "2025-04-01 01:45:48 | INFO | epoch: 3 (step: 5450) | train loss: 0.003973 | lr: 0.000100\n",
            "2025-04-01 01:45:54 | INFO | epoch: 3 (step: 5500) | train loss: 0.004116 | lr: 0.000100\n",
            "2025-04-01 01:46:00 | INFO | epoch: 3 (step: 5550) | train loss: 0.003892 | lr: 0.000100\n",
            "2025-04-01 01:46:06 | INFO | epoch: 3 (step: 5600) | train loss: 0.004030 | lr: 0.000100\n",
            "2025-04-01 01:46:12 | INFO | epoch: 3 (step: 5650) | train loss: 0.003688 | lr: 0.000100\n",
            "2025-04-01 01:46:18 | INFO | epoch: 3 (step: 5700) | train loss: 0.003653 | lr: 0.000100\n",
            "2025-04-01 01:46:24 | INFO | epoch: 3 (step: 5750) | train loss: 0.003812 | lr: 0.000100\n",
            "2025-04-01 01:46:30 | INFO | epoch: 3 (step: 5800) | train loss: 0.003362 | lr: 0.000100\n",
            "2025-04-01 01:46:37 | INFO | epoch: 3 (step: 5850) | train loss: 0.003396 | lr: 0.000100\n",
            "2025-04-01 01:46:43 | INFO | epoch: 3 (step: 5900) | train loss: 0.003323 | lr: 0.000100\n",
            "2025-04-01 01:46:49 | INFO | epoch: 3 (step: 5950) | train loss: 0.003368 | lr: 0.000100\n",
            "2025-04-01 01:46:55 | INFO | epoch: 3 (step: 6000) | train loss: 0.003616 | lr: 0.000100\n",
            "2025-04-01 01:47:01 | INFO | epoch: 3 (step: 6050) | train loss: 0.002911 | lr: 0.000100\n",
            "2025-04-01 01:47:07 | INFO | epoch: 3 (step: 6100) | train loss: 0.002932 | lr: 0.000100\n",
            "2025-04-01 01:47:13 | INFO | epoch: 3 (step: 6150) | train loss: 0.002994 | lr: 0.000100\n",
            "2025-04-01 01:47:19 | INFO | epoch: 3 (step: 6200) | train loss: 0.003217 | lr: 0.000100\n",
            "2025-04-01 01:47:25 | INFO | epoch: 3 (step: 6250) | train loss: 0.002936 | lr: 0.000100\n",
            "2025-04-01 01:47:31 | INFO | epoch: 3 (step: 6300) | train loss: 0.003016 | lr: 0.000100\n",
            "2025-04-01 01:47:37 | INFO | epoch: 3 (step: 6350) | train loss: 0.002875 | lr: 0.000100\n",
            "2025-04-01 01:47:43 | INFO | epoch: 3 (step: 6400) | train loss: 0.002915 | lr: 0.000100\n",
            "2025-04-01 01:47:49 | INFO | epoch: 3 (step: 6450) | train loss: 0.002882 | lr: 0.000100\n",
            "2025-04-01 01:47:56 | INFO | epoch: 3 (step: 6500) | train loss: 0.003084 | lr: 0.000100\n",
            "2025-04-01 01:48:02 | INFO | epoch: 3 (step: 6550) | train loss: 0.002783 | lr: 0.000100\n",
            "2025-04-01 01:48:08 | INFO | epoch: 3 (step: 6600) | train loss: 0.002890 | lr: 0.000100\n",
            "2025-04-01 01:48:14 | INFO | epoch: 3 (step: 6650) | train loss: 0.002552 | lr: 0.000100\n",
            "2025-04-01 01:48:20 | INFO | epoch: 3 (step: 6700) | train loss: 0.002980 | lr: 0.000100\n",
            "2025-04-01 01:48:26 | INFO | epoch: 3 (step: 6750) | train loss: 0.002576 | lr: 0.000100\n",
            "2025-04-01 01:48:32 | INFO | epoch: 3 (step: 6800) | train loss: 0.002558 | lr: 0.000100\n",
            "2025-04-01 01:48:38 | INFO | epoch: 3 (step: 6850) | train loss: 0.002890 | lr: 0.000100\n",
            "2025-04-01 01:48:44 | INFO | epoch: 3 (step: 6900) | train loss: 0.002611 | lr: 0.000100\n",
            "2025-04-01 01:48:50 | INFO | epoch: 3 (step: 6950) | train loss: 0.002597 | lr: 0.000100\n",
            "2025-04-01 01:48:58 | INFO | epoch: 4 (step: 7000) | train loss: 0.002538 | lr: 0.000100\n",
            "2025-04-01 01:49:04 | INFO | epoch: 4 (step: 7050) | train loss: 0.002951 | lr: 0.000100\n",
            "2025-04-01 01:49:10 | INFO | epoch: 4 (step: 7100) | train loss: 0.002513 | lr: 0.000100\n",
            "2025-04-01 01:49:16 | INFO | epoch: 4 (step: 7150) | train loss: 0.002757 | lr: 0.000100\n",
            "2025-04-01 01:49:22 | INFO | epoch: 4 (step: 7200) | train loss: 0.002328 | lr: 0.000100\n",
            "2025-04-01 01:49:28 | INFO | epoch: 4 (step: 7250) | train loss: 0.002287 | lr: 0.000100\n",
            "2025-04-01 01:49:34 | INFO | epoch: 4 (step: 7300) | train loss: 0.002826 | lr: 0.000100\n",
            "2025-04-01 01:49:40 | INFO | epoch: 4 (step: 7350) | train loss: 0.002471 | lr: 0.000100\n",
            "2025-04-01 01:49:46 | INFO | epoch: 4 (step: 7400) | train loss: 0.002159 | lr: 0.000100\n",
            "2025-04-01 01:49:52 | INFO | epoch: 4 (step: 7450) | train loss: 0.001948 | lr: 0.000100\n",
            "2025-04-01 01:49:58 | INFO | epoch: 4 (step: 7500) | train loss: 0.002483 | lr: 0.000100\n",
            "2025-04-01 01:50:04 | INFO | epoch: 4 (step: 7550) | train loss: 0.001887 | lr: 0.000100\n",
            "2025-04-01 01:50:10 | INFO | epoch: 4 (step: 7600) | train loss: 0.002550 | lr: 0.000100\n",
            "2025-04-01 01:50:16 | INFO | epoch: 4 (step: 7650) | train loss: 0.002275 | lr: 0.000100\n",
            "2025-04-01 01:50:22 | INFO | epoch: 4 (step: 7700) | train loss: 0.002343 | lr: 0.000100\n",
            "2025-04-01 01:50:28 | INFO | epoch: 4 (step: 7750) | train loss: 0.001788 | lr: 0.000100\n",
            "2025-04-01 01:50:35 | INFO | epoch: 4 (step: 7800) | train loss: 0.002276 | lr: 0.000100\n",
            "2025-04-01 01:50:41 | INFO | epoch: 4 (step: 7850) | train loss: 0.002157 | lr: 0.000100\n",
            "2025-04-01 01:50:47 | INFO | epoch: 4 (step: 7900) | train loss: 0.002144 | lr: 0.000100\n",
            "2025-04-01 01:50:53 | INFO | epoch: 4 (step: 7950) | train loss: 0.001995 | lr: 0.000100\n",
            "2025-04-01 01:50:59 | INFO | epoch: 4 (step: 8000) | train loss: 0.001928 | lr: 0.000100\n",
            "2025-04-01 01:51:05 | INFO | epoch: 4 (step: 8050) | train loss: 0.001977 | lr: 0.000100\n",
            "2025-04-01 01:51:11 | INFO | epoch: 4 (step: 8100) | train loss: 0.001945 | lr: 0.000100\n",
            "2025-04-01 01:51:17 | INFO | epoch: 4 (step: 8150) | train loss: 0.002256 | lr: 0.000100\n",
            "2025-04-01 01:51:23 | INFO | epoch: 4 (step: 8200) | train loss: 0.001802 | lr: 0.000100\n",
            "2025-04-01 01:51:29 | INFO | epoch: 4 (step: 8250) | train loss: 0.001941 | lr: 0.000100\n",
            "2025-04-01 01:51:35 | INFO | epoch: 4 (step: 8300) | train loss: 0.002057 | lr: 0.000100\n",
            "2025-04-01 01:51:41 | INFO | epoch: 4 (step: 8350) | train loss: 0.001996 | lr: 0.000100\n",
            "2025-04-01 01:51:47 | INFO | epoch: 4 (step: 8400) | train loss: 0.002038 | lr: 0.000100\n",
            "2025-04-01 01:51:53 | INFO | epoch: 4 (step: 8450) | train loss: 0.001868 | lr: 0.000100\n",
            "2025-04-01 01:51:59 | INFO | epoch: 4 (step: 8500) | train loss: 0.002024 | lr: 0.000100\n",
            "2025-04-01 01:52:05 | INFO | epoch: 4 (step: 8550) | train loss: 0.001832 | lr: 0.000100\n",
            "2025-04-01 01:52:11 | INFO | epoch: 4 (step: 8600) | train loss: 0.001614 | lr: 0.000100\n",
            "2025-04-01 01:52:17 | INFO | epoch: 4 (step: 8650) | train loss: 0.001989 | lr: 0.000100\n",
            "2025-04-01 01:52:23 | INFO | epoch: 4 (step: 8700) | train loss: 0.001832 | lr: 0.000100\n",
            "2025-04-01 01:52:29 | INFO | epoch: 4 (step: 8750) | train loss: 0.001815 | lr: 0.000100\n",
            "2025-04-01 01:52:36 | INFO | epoch: 4 (step: 8800) | train loss: 0.001511 | lr: 0.000100\n",
            "2025-04-01 01:52:42 | INFO | epoch: 4 (step: 8850) | train loss: 0.001711 | lr: 0.000100\n",
            "2025-04-01 01:52:48 | INFO | epoch: 4 (step: 8900) | train loss: 0.001829 | lr: 0.000100\n",
            "2025-04-01 01:52:54 | INFO | epoch: 4 (step: 8950) | train loss: 0.001673 | lr: 0.000100\n",
            "2025-04-01 01:53:00 | INFO | epoch: 4 (step: 9000) | train loss: 0.001661 | lr: 0.000100\n",
            "2025-04-01 01:53:06 | INFO | epoch: 4 (step: 9050) | train loss: 0.001536 | lr: 0.000100\n",
            "2025-04-01 01:53:12 | INFO | epoch: 4 (step: 9100) | train loss: 0.001656 | lr: 0.000100\n",
            "2025-04-01 01:53:18 | INFO | epoch: 4 (step: 9150) | train loss: 0.001585 | lr: 0.000100\n",
            "2025-04-01 01:53:24 | INFO | epoch: 4 (step: 9200) | train loss: 0.001618 | lr: 0.000100\n",
            "2025-04-01 01:53:30 | INFO | epoch: 4 (step: 9250) | train loss: 0.001513 | lr: 0.000100\n",
            "2025-04-01 01:53:36 | INFO | epoch: 4 (step: 9300) | train loss: 0.001601 | lr: 0.000100\n",
            "2025-04-01 01:53:43 | INFO | epoch: 5 (step: 9350) | train loss: 0.001372 | lr: 0.000100\n",
            "2025-04-01 01:53:50 | INFO | epoch: 5 (step: 9400) | train loss: 0.001330 | lr: 0.000100\n",
            "2025-04-01 01:53:56 | INFO | epoch: 5 (step: 9450) | train loss: 0.001676 | lr: 0.000100\n",
            "2025-04-01 01:54:02 | INFO | epoch: 5 (step: 9500) | train loss: 0.001456 | lr: 0.000100\n",
            "2025-04-01 01:54:08 | INFO | epoch: 5 (step: 9550) | train loss: 0.001491 | lr: 0.000100\n",
            "2025-04-01 01:54:14 | INFO | epoch: 5 (step: 9600) | train loss: 0.001654 | lr: 0.000100\n",
            "2025-04-01 01:54:20 | INFO | epoch: 5 (step: 9650) | train loss: 0.001513 | lr: 0.000100\n",
            "2025-04-01 01:54:26 | INFO | epoch: 5 (step: 9700) | train loss: 0.001338 | lr: 0.000100\n",
            "2025-04-01 01:54:32 | INFO | epoch: 5 (step: 9750) | train loss: 0.001484 | lr: 0.000100\n",
            "2025-04-01 01:54:38 | INFO | epoch: 5 (step: 9800) | train loss: 0.001500 | lr: 0.000100\n",
            "2025-04-01 01:54:45 | INFO | epoch: 5 (step: 9850) | train loss: 0.001524 | lr: 0.000100\n",
            "2025-04-01 01:54:51 | INFO | epoch: 5 (step: 9900) | train loss: 0.001654 | lr: 0.000100\n",
            "2025-04-01 01:54:57 | INFO | epoch: 5 (step: 9950) | train loss: 0.001277 | lr: 0.000100\n",
            "2025-04-01 01:55:03 | INFO | epoch: 5 (step: 10000) | train loss: 0.001391 | lr: 0.000100\n",
            "2025-04-01 01:55:10 | INFO | epoch: 5 (step: 10000) | valid_loss: 0.015317 | valid_loss: 0.015317\n",
            "2025-04-01 01:55:14 | INFO | epoch: 5 (step: 10000) | best metric updated (loss) - 0.015317\n",
            "2025-04-01 01:55:19 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 01:55:25 | INFO | epoch: 5 (step: 10050) | train loss: 0.001498 | lr: 0.000100\n",
            "2025-04-01 01:55:31 | INFO | epoch: 5 (step: 10100) | train loss: 0.001289 | lr: 0.000100\n",
            "2025-04-01 01:55:37 | INFO | epoch: 5 (step: 10150) | train loss: 0.001307 | lr: 0.000100\n",
            "2025-04-01 01:55:43 | INFO | epoch: 5 (step: 10200) | train loss: 0.001213 | lr: 0.000100\n",
            "2025-04-01 01:55:49 | INFO | epoch: 5 (step: 10250) | train loss: 0.001246 | lr: 0.000100\n",
            "2025-04-01 01:55:55 | INFO | epoch: 5 (step: 10300) | train loss: 0.001247 | lr: 0.000100\n",
            "2025-04-01 01:56:01 | INFO | epoch: 5 (step: 10350) | train loss: 0.001219 | lr: 0.000100\n",
            "2025-04-01 01:56:07 | INFO | epoch: 5 (step: 10400) | train loss: 0.001368 | lr: 0.000100\n",
            "2025-04-01 01:56:13 | INFO | epoch: 5 (step: 10450) | train loss: 0.001183 | lr: 0.000100\n",
            "2025-04-01 01:56:19 | INFO | epoch: 5 (step: 10500) | train loss: 0.001200 | lr: 0.000100\n",
            "2025-04-01 01:56:25 | INFO | epoch: 5 (step: 10550) | train loss: 0.001178 | lr: 0.000100\n",
            "2025-04-01 01:56:32 | INFO | epoch: 5 (step: 10600) | train loss: 0.001055 | lr: 0.000100\n",
            "2025-04-01 01:56:38 | INFO | epoch: 5 (step: 10650) | train loss: 0.001128 | lr: 0.000100\n",
            "2025-04-01 01:56:44 | INFO | epoch: 5 (step: 10700) | train loss: 0.001153 | lr: 0.000100\n",
            "2025-04-01 01:56:50 | INFO | epoch: 5 (step: 10750) | train loss: 0.001358 | lr: 0.000100\n",
            "2025-04-01 01:56:56 | INFO | epoch: 5 (step: 10800) | train loss: 0.001079 | lr: 0.000100\n",
            "2025-04-01 01:57:02 | INFO | epoch: 5 (step: 10850) | train loss: 0.001104 | lr: 0.000100\n",
            "2025-04-01 01:57:08 | INFO | epoch: 5 (step: 10900) | train loss: 0.001173 | lr: 0.000100\n",
            "2025-04-01 01:57:14 | INFO | epoch: 5 (step: 10950) | train loss: 0.001050 | lr: 0.000100\n",
            "2025-04-01 01:57:20 | INFO | epoch: 5 (step: 11000) | train loss: 0.001047 | lr: 0.000100\n",
            "2025-04-01 01:57:26 | INFO | epoch: 5 (step: 11050) | train loss: 0.001181 | lr: 0.000100\n",
            "2025-04-01 01:57:32 | INFO | epoch: 5 (step: 11100) | train loss: 0.001010 | lr: 0.000100\n",
            "2025-04-01 01:57:39 | INFO | epoch: 5 (step: 11150) | train loss: 0.001153 | lr: 0.000100\n",
            "2025-04-01 01:57:44 | INFO | epoch: 5 (step: 11200) | train loss: 0.001069 | lr: 0.000100\n",
            "2025-04-01 01:57:50 | INFO | epoch: 5 (step: 11250) | train loss: 0.001069 | lr: 0.000100\n",
            "2025-04-01 01:57:56 | INFO | epoch: 5 (step: 11300) | train loss: 0.001229 | lr: 0.000100\n",
            "2025-04-01 01:58:02 | INFO | epoch: 5 (step: 11350) | train loss: 0.001061 | lr: 0.000100\n",
            "2025-04-01 01:58:08 | INFO | epoch: 5 (step: 11400) | train loss: 0.001135 | lr: 0.000100\n",
            "2025-04-01 01:58:15 | INFO | epoch: 5 (step: 11450) | train loss: 0.001083 | lr: 0.000100\n",
            "2025-04-01 01:58:21 | INFO | epoch: 5 (step: 11500) | train loss: 0.001213 | lr: 0.000100\n",
            "2025-04-01 01:58:27 | INFO | epoch: 5 (step: 11550) | train loss: 0.001022 | lr: 0.000100\n",
            "2025-04-01 01:58:33 | INFO | epoch: 5 (step: 11600) | train loss: 0.001015 | lr: 0.000100\n",
            "2025-04-01 01:58:40 | INFO | epoch: 6 (step: 11650) | train loss: 0.001033 | lr: 0.000100\n",
            "2025-04-01 01:58:47 | INFO | epoch: 6 (step: 11700) | train loss: 0.001094 | lr: 0.000100\n",
            "2025-04-01 01:58:52 | INFO | epoch: 6 (step: 11750) | train loss: 0.001067 | lr: 0.000100\n",
            "2025-04-01 01:58:59 | INFO | epoch: 6 (step: 11800) | train loss: 0.000989 | lr: 0.000100\n",
            "2025-04-01 01:59:05 | INFO | epoch: 6 (step: 11850) | train loss: 0.001000 | lr: 0.000100\n",
            "2025-04-01 01:59:11 | INFO | epoch: 6 (step: 11900) | train loss: 0.000898 | lr: 0.000100\n",
            "2025-04-01 01:59:17 | INFO | epoch: 6 (step: 11950) | train loss: 0.001010 | lr: 0.000100\n",
            "2025-04-01 01:59:23 | INFO | epoch: 6 (step: 12000) | train loss: 0.000869 | lr: 0.000100\n",
            "2025-04-01 01:59:29 | INFO | epoch: 6 (step: 12050) | train loss: 0.000967 | lr: 0.000100\n",
            "2025-04-01 01:59:35 | INFO | epoch: 6 (step: 12100) | train loss: 0.001078 | lr: 0.000100\n",
            "2025-04-01 01:59:41 | INFO | epoch: 6 (step: 12150) | train loss: 0.000927 | lr: 0.000100\n",
            "2025-04-01 01:59:47 | INFO | epoch: 6 (step: 12200) | train loss: 0.001032 | lr: 0.000100\n",
            "2025-04-01 01:59:53 | INFO | epoch: 6 (step: 12250) | train loss: 0.000978 | lr: 0.000100\n",
            "2025-04-01 01:59:59 | INFO | epoch: 6 (step: 12300) | train loss: 0.000923 | lr: 0.000100\n",
            "2025-04-01 02:00:05 | INFO | epoch: 6 (step: 12350) | train loss: 0.000986 | lr: 0.000100\n",
            "2025-04-01 02:00:12 | INFO | epoch: 6 (step: 12400) | train loss: 0.000824 | lr: 0.000100\n",
            "2025-04-01 02:00:17 | INFO | epoch: 6 (step: 12450) | train loss: 0.000820 | lr: 0.000100\n",
            "2025-04-01 02:00:23 | INFO | epoch: 6 (step: 12500) | train loss: 0.000860 | lr: 0.000100\n",
            "2025-04-01 02:00:30 | INFO | epoch: 6 (step: 12550) | train loss: 0.000967 | lr: 0.000100\n",
            "2025-04-01 02:00:36 | INFO | epoch: 6 (step: 12600) | train loss: 0.000832 | lr: 0.000100\n",
            "2025-04-01 02:00:42 | INFO | epoch: 6 (step: 12650) | train loss: 0.000890 | lr: 0.000100\n",
            "2025-04-01 02:00:48 | INFO | epoch: 6 (step: 12700) | train loss: 0.000952 | lr: 0.000100\n",
            "2025-04-01 02:00:54 | INFO | epoch: 6 (step: 12750) | train loss: 0.000801 | lr: 0.000100\n",
            "2025-04-01 02:01:00 | INFO | epoch: 6 (step: 12800) | train loss: 0.001002 | lr: 0.000100\n",
            "2025-04-01 02:01:06 | INFO | epoch: 6 (step: 12850) | train loss: 0.000896 | lr: 0.000100\n",
            "2025-04-01 02:01:12 | INFO | epoch: 6 (step: 12900) | train loss: 0.000868 | lr: 0.000100\n",
            "2025-04-01 02:01:18 | INFO | epoch: 6 (step: 12950) | train loss: 0.000903 | lr: 0.000100\n",
            "2025-04-01 02:01:24 | INFO | epoch: 6 (step: 13000) | train loss: 0.000911 | lr: 0.000100\n",
            "2025-04-01 02:01:30 | INFO | epoch: 6 (step: 13050) | train loss: 0.000840 | lr: 0.000100\n",
            "2025-04-01 02:01:36 | INFO | epoch: 6 (step: 13100) | train loss: 0.000889 | lr: 0.000100\n",
            "2025-04-01 02:01:42 | INFO | epoch: 6 (step: 13150) | train loss: 0.000862 | lr: 0.000100\n",
            "2025-04-01 02:01:48 | INFO | epoch: 6 (step: 13200) | train loss: 0.000881 | lr: 0.000100\n",
            "2025-04-01 02:01:54 | INFO | epoch: 6 (step: 13250) | train loss: 0.000774 | lr: 0.000100\n",
            "2025-04-01 02:02:00 | INFO | epoch: 6 (step: 13300) | train loss: 0.000778 | lr: 0.000100\n",
            "2025-04-01 02:02:07 | INFO | epoch: 6 (step: 13350) | train loss: 0.000646 | lr: 0.000100\n",
            "2025-04-01 02:02:13 | INFO | epoch: 6 (step: 13400) | train loss: 0.000739 | lr: 0.000100\n",
            "2025-04-01 02:02:19 | INFO | epoch: 6 (step: 13450) | train loss: 0.000736 | lr: 0.000100\n",
            "2025-04-01 02:02:25 | INFO | epoch: 6 (step: 13500) | train loss: 0.000839 | lr: 0.000100\n",
            "2025-04-01 02:02:31 | INFO | epoch: 6 (step: 13550) | train loss: 0.000749 | lr: 0.000100\n",
            "2025-04-01 02:02:37 | INFO | epoch: 6 (step: 13600) | train loss: 0.000739 | lr: 0.000100\n",
            "2025-04-01 02:02:43 | INFO | epoch: 6 (step: 13650) | train loss: 0.000709 | lr: 0.000100\n",
            "2025-04-01 02:02:49 | INFO | epoch: 6 (step: 13700) | train loss: 0.000668 | lr: 0.000100\n",
            "2025-04-01 02:02:55 | INFO | epoch: 6 (step: 13750) | train loss: 0.000692 | lr: 0.000100\n",
            "2025-04-01 02:03:01 | INFO | epoch: 6 (step: 13800) | train loss: 0.000720 | lr: 0.000100\n",
            "2025-04-01 02:03:07 | INFO | epoch: 6 (step: 13850) | train loss: 0.000757 | lr: 0.000100\n",
            "2025-04-01 02:03:13 | INFO | epoch: 6 (step: 13900) | train loss: 0.000700 | lr: 0.000100\n",
            "2025-04-01 02:03:19 | INFO | epoch: 6 (step: 13950) | train loss: 0.000711 | lr: 0.000100\n",
            "2025-04-01 02:03:27 | INFO | epoch: 7 (step: 14000) | train loss: 0.000720 | lr: 0.000100\n",
            "2025-04-01 02:03:33 | INFO | epoch: 7 (step: 14050) | train loss: 0.000617 | lr: 0.000100\n",
            "2025-04-01 02:03:39 | INFO | epoch: 7 (step: 14100) | train loss: 0.000613 | lr: 0.000100\n",
            "2025-04-01 02:03:45 | INFO | epoch: 7 (step: 14150) | train loss: 0.000692 | lr: 0.000100\n",
            "2025-04-01 02:03:51 | INFO | epoch: 7 (step: 14200) | train loss: 0.000671 | lr: 0.000100\n",
            "2025-04-01 02:03:57 | INFO | epoch: 7 (step: 14250) | train loss: 0.000749 | lr: 0.000100\n",
            "2025-04-01 02:04:04 | INFO | epoch: 7 (step: 14300) | train loss: 0.000546 | lr: 0.000100\n",
            "2025-04-01 02:04:10 | INFO | epoch: 7 (step: 14350) | train loss: 0.000665 | lr: 0.000100\n",
            "2025-04-01 02:04:16 | INFO | epoch: 7 (step: 14400) | train loss: 0.000765 | lr: 0.000100\n",
            "2025-04-01 02:04:22 | INFO | epoch: 7 (step: 14450) | train loss: 0.000663 | lr: 0.000100\n",
            "2025-04-01 02:04:28 | INFO | epoch: 7 (step: 14500) | train loss: 0.000684 | lr: 0.000100\n",
            "2025-04-01 02:04:34 | INFO | epoch: 7 (step: 14550) | train loss: 0.000793 | lr: 0.000100\n",
            "2025-04-01 02:04:40 | INFO | epoch: 7 (step: 14600) | train loss: 0.000656 | lr: 0.000100\n",
            "2025-04-01 02:04:46 | INFO | epoch: 7 (step: 14650) | train loss: 0.000714 | lr: 0.000100\n",
            "2025-04-01 02:04:53 | INFO | epoch: 7 (step: 14700) | train loss: 0.000733 | lr: 0.000100\n",
            "2025-04-01 02:04:59 | INFO | epoch: 7 (step: 14750) | train loss: 0.000602 | lr: 0.000100\n",
            "2025-04-01 02:05:05 | INFO | epoch: 7 (step: 14800) | train loss: 0.000685 | lr: 0.000100\n",
            "2025-04-01 02:05:11 | INFO | epoch: 7 (step: 14850) | train loss: 0.000623 | lr: 0.000100\n",
            "2025-04-01 02:05:17 | INFO | epoch: 7 (step: 14900) | train loss: 0.000602 | lr: 0.000100\n",
            "2025-04-01 02:05:23 | INFO | epoch: 7 (step: 14950) | train loss: 0.000623 | lr: 0.000100\n",
            "2025-04-01 02:05:29 | INFO | epoch: 7 (step: 15000) | train loss: 0.000655 | lr: 0.000100\n",
            "2025-04-01 02:05:37 | INFO | epoch: 7 (step: 15000) | valid_loss: 0.008856 | valid_loss: 0.008856\n",
            "2025-04-01 02:05:41 | INFO | epoch: 7 (step: 15000) | best metric updated (loss) - 0.008856\n",
            "2025-04-01 02:05:45 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:05:52 | INFO | epoch: 7 (step: 15050) | train loss: 0.000472 | lr: 0.000100\n",
            "2025-04-01 02:05:58 | INFO | epoch: 7 (step: 15100) | train loss: 0.000642 | lr: 0.000100\n",
            "2025-04-01 02:06:04 | INFO | epoch: 7 (step: 15150) | train loss: 0.000552 | lr: 0.000100\n",
            "2025-04-01 02:06:10 | INFO | epoch: 7 (step: 15200) | train loss: 0.000534 | lr: 0.000100\n",
            "2025-04-01 02:06:16 | INFO | epoch: 7 (step: 15250) | train loss: 0.000683 | lr: 0.000100\n",
            "2025-04-01 02:06:22 | INFO | epoch: 7 (step: 15300) | train loss: 0.000662 | lr: 0.000100\n",
            "2025-04-01 02:06:28 | INFO | epoch: 7 (step: 15350) | train loss: 0.000492 | lr: 0.000100\n",
            "2025-04-01 02:06:34 | INFO | epoch: 7 (step: 15400) | train loss: 0.000572 | lr: 0.000100\n",
            "2025-04-01 02:06:41 | INFO | epoch: 7 (step: 15450) | train loss: 0.000565 | lr: 0.000100\n",
            "2025-04-01 02:06:47 | INFO | epoch: 7 (step: 15500) | train loss: 0.000605 | lr: 0.000100\n",
            "2025-04-01 02:06:53 | INFO | epoch: 7 (step: 15550) | train loss: 0.000639 | lr: 0.000100\n",
            "2025-04-01 02:06:59 | INFO | epoch: 7 (step: 15600) | train loss: 0.000632 | lr: 0.000100\n",
            "2025-04-01 02:07:05 | INFO | epoch: 7 (step: 15650) | train loss: 0.000444 | lr: 0.000100\n",
            "2025-04-01 02:07:11 | INFO | epoch: 7 (step: 15700) | train loss: 0.000647 | lr: 0.000100\n",
            "2025-04-01 02:07:17 | INFO | epoch: 7 (step: 15750) | train loss: 0.000602 | lr: 0.000100\n",
            "2025-04-01 02:07:23 | INFO | epoch: 7 (step: 15800) | train loss: 0.000604 | lr: 0.000100\n",
            "2025-04-01 02:07:29 | INFO | epoch: 7 (step: 15850) | train loss: 0.000698 | lr: 0.000100\n",
            "2025-04-01 02:07:35 | INFO | epoch: 7 (step: 15900) | train loss: 0.000578 | lr: 0.000100\n",
            "2025-04-01 02:07:41 | INFO | epoch: 7 (step: 15950) | train loss: 0.000497 | lr: 0.000100\n",
            "2025-04-01 02:07:47 | INFO | epoch: 7 (step: 16000) | train loss: 0.000550 | lr: 0.000100\n",
            "2025-04-01 02:07:54 | INFO | epoch: 7 (step: 16050) | train loss: 0.000552 | lr: 0.000100\n",
            "2025-04-01 02:08:00 | INFO | epoch: 7 (step: 16100) | train loss: 0.000577 | lr: 0.000100\n",
            "2025-04-01 02:08:06 | INFO | epoch: 7 (step: 16150) | train loss: 0.000528 | lr: 0.000100\n",
            "2025-04-01 02:08:12 | INFO | epoch: 7 (step: 16200) | train loss: 0.000571 | lr: 0.000100\n",
            "2025-04-01 02:08:18 | INFO | epoch: 7 (step: 16250) | train loss: 0.000508 | lr: 0.000100\n",
            "2025-04-01 02:08:24 | INFO | epoch: 7 (step: 16300) | train loss: 0.000484 | lr: 0.000100\n",
            "2025-04-01 02:08:31 | INFO | epoch: 8 (step: 16350) | train loss: 0.000590 | lr: 0.000100\n",
            "2025-04-01 02:08:38 | INFO | epoch: 8 (step: 16400) | train loss: 0.000573 | lr: 0.000100\n",
            "2025-04-01 02:08:44 | INFO | epoch: 8 (step: 16450) | train loss: 0.000555 | lr: 0.000100\n",
            "2025-04-01 02:08:50 | INFO | epoch: 8 (step: 16500) | train loss: 0.000639 | lr: 0.000100\n",
            "2025-04-01 02:08:56 | INFO | epoch: 8 (step: 16550) | train loss: 0.000552 | lr: 0.000100\n",
            "2025-04-01 02:09:02 | INFO | epoch: 8 (step: 16600) | train loss: 0.000554 | lr: 0.000100\n",
            "2025-04-01 02:09:08 | INFO | epoch: 8 (step: 16650) | train loss: 0.000701 | lr: 0.000100\n",
            "2025-04-01 02:09:14 | INFO | epoch: 8 (step: 16700) | train loss: 0.000502 | lr: 0.000100\n",
            "2025-04-01 02:09:20 | INFO | epoch: 8 (step: 16750) | train loss: 0.000432 | lr: 0.000100\n",
            "2025-04-01 02:09:26 | INFO | epoch: 8 (step: 16800) | train loss: 0.000514 | lr: 0.000100\n",
            "2025-04-01 02:09:32 | INFO | epoch: 8 (step: 16850) | train loss: 0.000424 | lr: 0.000100\n",
            "2025-04-01 02:09:38 | INFO | epoch: 8 (step: 16900) | train loss: 0.000543 | lr: 0.000100\n",
            "2025-04-01 02:09:44 | INFO | epoch: 8 (step: 16950) | train loss: 0.000436 | lr: 0.000100\n",
            "2025-04-01 02:09:50 | INFO | epoch: 8 (step: 17000) | train loss: 0.000641 | lr: 0.000100\n",
            "2025-04-01 02:09:56 | INFO | epoch: 8 (step: 17050) | train loss: 0.000493 | lr: 0.000100\n",
            "2025-04-01 02:10:02 | INFO | epoch: 8 (step: 17100) | train loss: 0.000494 | lr: 0.000100\n",
            "2025-04-01 02:10:08 | INFO | epoch: 8 (step: 17150) | train loss: 0.000602 | lr: 0.000100\n",
            "2025-04-01 02:10:15 | INFO | epoch: 8 (step: 17200) | train loss: 0.000534 | lr: 0.000100\n",
            "2025-04-01 02:10:21 | INFO | epoch: 8 (step: 17250) | train loss: 0.000547 | lr: 0.000100\n",
            "2025-04-01 02:10:27 | INFO | epoch: 8 (step: 17300) | train loss: 0.000455 | lr: 0.000100\n",
            "2025-04-01 02:10:33 | INFO | epoch: 8 (step: 17350) | train loss: 0.000460 | lr: 0.000100\n",
            "2025-04-01 02:10:39 | INFO | epoch: 8 (step: 17400) | train loss: 0.000509 | lr: 0.000100\n",
            "2025-04-01 02:10:45 | INFO | epoch: 8 (step: 17450) | train loss: 0.000498 | lr: 0.000100\n",
            "2025-04-01 02:10:52 | INFO | epoch: 8 (step: 17500) | train loss: 0.000496 | lr: 0.000100\n",
            "2025-04-01 02:10:58 | INFO | epoch: 8 (step: 17550) | train loss: 0.000543 | lr: 0.000100\n",
            "2025-04-01 02:11:04 | INFO | epoch: 8 (step: 17600) | train loss: 0.000336 | lr: 0.000100\n",
            "2025-04-01 02:11:10 | INFO | epoch: 8 (step: 17650) | train loss: 0.000403 | lr: 0.000100\n",
            "2025-04-01 02:11:16 | INFO | epoch: 8 (step: 17700) | train loss: 0.000418 | lr: 0.000100\n",
            "2025-04-01 02:11:22 | INFO | epoch: 8 (step: 17750) | train loss: 0.000536 | lr: 0.000100\n",
            "2025-04-01 02:11:28 | INFO | epoch: 8 (step: 17800) | train loss: 0.000457 | lr: 0.000100\n",
            "2025-04-01 02:11:34 | INFO | epoch: 8 (step: 17850) | train loss: 0.000446 | lr: 0.000100\n",
            "2025-04-01 02:11:40 | INFO | epoch: 8 (step: 17900) | train loss: 0.000445 | lr: 0.000100\n",
            "2025-04-01 02:11:46 | INFO | epoch: 8 (step: 17950) | train loss: 0.000491 | lr: 0.000100\n",
            "2025-04-01 02:11:52 | INFO | epoch: 8 (step: 18000) | train loss: 0.000456 | lr: 0.000100\n",
            "2025-04-01 02:11:58 | INFO | epoch: 8 (step: 18050) | train loss: 0.000385 | lr: 0.000100\n",
            "2025-04-01 02:12:04 | INFO | epoch: 8 (step: 18100) | train loss: 0.000465 | lr: 0.000100\n",
            "2025-04-01 02:12:10 | INFO | epoch: 8 (step: 18150) | train loss: 0.000365 | lr: 0.000100\n",
            "2025-04-01 02:12:16 | INFO | epoch: 8 (step: 18200) | train loss: 0.000453 | lr: 0.000100\n",
            "2025-04-01 02:12:22 | INFO | epoch: 8 (step: 18250) | train loss: 0.000439 | lr: 0.000100\n",
            "2025-04-01 02:12:28 | INFO | epoch: 8 (step: 18300) | train loss: 0.000388 | lr: 0.000100\n",
            "2025-04-01 02:12:34 | INFO | epoch: 8 (step: 18350) | train loss: 0.000462 | lr: 0.000100\n",
            "2025-04-01 02:12:41 | INFO | epoch: 8 (step: 18400) | train loss: 0.000445 | lr: 0.000100\n",
            "2025-04-01 02:12:47 | INFO | epoch: 8 (step: 18450) | train loss: 0.000483 | lr: 0.000100\n",
            "2025-04-01 02:12:53 | INFO | epoch: 8 (step: 18500) | train loss: 0.000403 | lr: 0.000100\n",
            "2025-04-01 02:12:59 | INFO | epoch: 8 (step: 18550) | train loss: 0.000527 | lr: 0.000100\n",
            "2025-04-01 02:13:05 | INFO | epoch: 8 (step: 18600) | train loss: 0.000436 | lr: 0.000100\n",
            "2025-04-01 02:13:13 | INFO | epoch: 9 (step: 18650) | train loss: 0.000396 | lr: 0.000100\n",
            "2025-04-01 02:13:19 | INFO | epoch: 9 (step: 18700) | train loss: 0.000361 | lr: 0.000100\n",
            "2025-04-01 02:13:25 | INFO | epoch: 9 (step: 18750) | train loss: 0.000439 | lr: 0.000100\n",
            "2025-04-01 02:13:31 | INFO | epoch: 9 (step: 18800) | train loss: 0.000467 | lr: 0.000100\n",
            "2025-04-01 02:13:37 | INFO | epoch: 9 (step: 18850) | train loss: 0.000481 | lr: 0.000100\n",
            "2025-04-01 02:13:43 | INFO | epoch: 9 (step: 18900) | train loss: 0.000375 | lr: 0.000100\n",
            "2025-04-01 02:13:49 | INFO | epoch: 9 (step: 18950) | train loss: 0.000340 | lr: 0.000100\n",
            "2025-04-01 02:13:56 | INFO | epoch: 9 (step: 19000) | train loss: 0.000406 | lr: 0.000100\n",
            "2025-04-01 02:14:01 | INFO | epoch: 9 (step: 19050) | train loss: 0.000370 | lr: 0.000100\n",
            "2025-04-01 02:14:07 | INFO | epoch: 9 (step: 19100) | train loss: 0.000470 | lr: 0.000100\n",
            "2025-04-01 02:14:13 | INFO | epoch: 9 (step: 19150) | train loss: 0.000468 | lr: 0.000100\n",
            "2025-04-01 02:14:19 | INFO | epoch: 9 (step: 19200) | train loss: 0.000391 | lr: 0.000100\n",
            "2025-04-01 02:14:26 | INFO | epoch: 9 (step: 19250) | train loss: 0.000355 | lr: 0.000100\n",
            "2025-04-01 02:14:32 | INFO | epoch: 9 (step: 19300) | train loss: 0.000441 | lr: 0.000100\n",
            "2025-04-01 02:14:38 | INFO | epoch: 9 (step: 19350) | train loss: 0.000487 | lr: 0.000100\n",
            "2025-04-01 02:14:44 | INFO | epoch: 9 (step: 19400) | train loss: 0.000333 | lr: 0.000100\n",
            "2025-04-01 02:14:50 | INFO | epoch: 9 (step: 19450) | train loss: 0.000350 | lr: 0.000100\n",
            "2025-04-01 02:14:56 | INFO | epoch: 9 (step: 19500) | train loss: 0.000396 | lr: 0.000100\n",
            "2025-04-01 02:15:03 | INFO | epoch: 9 (step: 19550) | train loss: 0.000385 | lr: 0.000100\n",
            "2025-04-01 02:15:09 | INFO | epoch: 9 (step: 19600) | train loss: 0.000406 | lr: 0.000100\n",
            "2025-04-01 02:15:15 | INFO | epoch: 9 (step: 19650) | train loss: 0.000377 | lr: 0.000100\n",
            "2025-04-01 02:15:21 | INFO | epoch: 9 (step: 19700) | train loss: 0.000470 | lr: 0.000100\n",
            "2025-04-01 02:15:27 | INFO | epoch: 9 (step: 19750) | train loss: 0.000331 | lr: 0.000100\n",
            "2025-04-01 02:15:33 | INFO | epoch: 9 (step: 19800) | train loss: 0.000419 | lr: 0.000100\n",
            "2025-04-01 02:15:39 | INFO | epoch: 9 (step: 19850) | train loss: 0.000389 | lr: 0.000100\n",
            "2025-04-01 02:15:45 | INFO | epoch: 9 (step: 19900) | train loss: 0.000375 | lr: 0.000100\n",
            "2025-04-01 02:15:52 | INFO | epoch: 9 (step: 19950) | train loss: 0.000416 | lr: 0.000100\n",
            "2025-04-01 02:15:58 | INFO | epoch: 9 (step: 20000) | train loss: 0.000389 | lr: 0.000100\n",
            "2025-04-01 02:16:05 | INFO | epoch: 9 (step: 20000) | valid_loss: 0.005770 | valid_loss: 0.005770\n",
            "2025-04-01 02:16:09 | INFO | epoch: 9 (step: 20000) | best metric updated (loss) - 0.005770\n",
            "2025-04-01 02:16:13 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:16:19 | INFO | epoch: 9 (step: 20050) | train loss: 0.000462 | lr: 0.000100\n",
            "2025-04-01 02:16:25 | INFO | epoch: 9 (step: 20100) | train loss: 0.000400 | lr: 0.000100\n",
            "2025-04-01 02:16:31 | INFO | epoch: 9 (step: 20150) | train loss: 0.000357 | lr: 0.000100\n",
            "2025-04-01 02:16:37 | INFO | epoch: 9 (step: 20200) | train loss: 0.000321 | lr: 0.000100\n",
            "2025-04-01 02:16:43 | INFO | epoch: 9 (step: 20250) | train loss: 0.000407 | lr: 0.000100\n",
            "2025-04-01 02:16:49 | INFO | epoch: 9 (step: 20300) | train loss: 0.000418 | lr: 0.000100\n",
            "2025-04-01 02:16:55 | INFO | epoch: 9 (step: 20350) | train loss: 0.000297 | lr: 0.000100\n",
            "2025-04-01 02:17:02 | INFO | epoch: 9 (step: 20400) | train loss: 0.000387 | lr: 0.000100\n",
            "2025-04-01 02:17:08 | INFO | epoch: 9 (step: 20450) | train loss: 0.000400 | lr: 0.000100\n",
            "2025-04-01 02:17:14 | INFO | epoch: 9 (step: 20500) | train loss: 0.000381 | lr: 0.000100\n",
            "2025-04-01 02:17:20 | INFO | epoch: 9 (step: 20550) | train loss: 0.000336 | lr: 0.000100\n",
            "2025-04-01 02:17:26 | INFO | epoch: 9 (step: 20600) | train loss: 0.000351 | lr: 0.000100\n",
            "2025-04-01 02:17:31 | INFO | epoch: 9 (step: 20650) | train loss: 0.000354 | lr: 0.000100\n",
            "2025-04-01 02:17:38 | INFO | epoch: 9 (step: 20700) | train loss: 0.000311 | lr: 0.000100\n",
            "2025-04-01 02:17:43 | INFO | epoch: 9 (step: 20750) | train loss: 0.000389 | lr: 0.000100\n",
            "2025-04-01 02:17:49 | INFO | epoch: 9 (step: 20800) | train loss: 0.000343 | lr: 0.000100\n",
            "2025-04-01 02:17:55 | INFO | epoch: 9 (step: 20850) | train loss: 0.000415 | lr: 0.000100\n",
            "2025-04-01 02:18:01 | INFO | epoch: 9 (step: 20900) | train loss: 0.000311 | lr: 0.000100\n",
            "2025-04-01 02:18:07 | INFO | epoch: 9 (step: 20950) | train loss: 0.000315 | lr: 0.000100\n",
            "2025-04-01 02:18:15 | INFO | epoch: 10 (step: 21000) | train loss: 0.000341 | lr: 0.000100\n",
            "2025-04-01 02:18:21 | INFO | epoch: 10 (step: 21050) | train loss: 0.000371 | lr: 0.000100\n",
            "2025-04-01 02:18:27 | INFO | epoch: 10 (step: 21100) | train loss: 0.000328 | lr: 0.000100\n",
            "2025-04-01 02:18:33 | INFO | epoch: 10 (step: 21150) | train loss: 0.000316 | lr: 0.000100\n",
            "2025-04-01 02:18:39 | INFO | epoch: 10 (step: 21200) | train loss: 0.000245 | lr: 0.000100\n",
            "2025-04-01 02:18:45 | INFO | epoch: 10 (step: 21250) | train loss: 0.000313 | lr: 0.000100\n",
            "2025-04-01 02:18:51 | INFO | epoch: 10 (step: 21300) | train loss: 0.000402 | lr: 0.000100\n",
            "2025-04-01 02:18:57 | INFO | epoch: 10 (step: 21350) | train loss: 0.000286 | lr: 0.000100\n",
            "2025-04-01 02:19:03 | INFO | epoch: 10 (step: 21400) | train loss: 0.000349 | lr: 0.000100\n",
            "2025-04-01 02:19:09 | INFO | epoch: 10 (step: 21450) | train loss: 0.000389 | lr: 0.000100\n",
            "2025-04-01 02:19:16 | INFO | epoch: 10 (step: 21500) | train loss: 0.000268 | lr: 0.000100\n",
            "2025-04-01 02:19:22 | INFO | epoch: 10 (step: 21550) | train loss: 0.000418 | lr: 0.000100\n",
            "2025-04-01 02:19:28 | INFO | epoch: 10 (step: 21600) | train loss: 0.000360 | lr: 0.000100\n",
            "2025-04-01 02:19:34 | INFO | epoch: 10 (step: 21650) | train loss: 0.000345 | lr: 0.000100\n",
            "2025-04-01 02:19:40 | INFO | epoch: 10 (step: 21700) | train loss: 0.000241 | lr: 0.000100\n",
            "2025-04-01 02:19:46 | INFO | epoch: 10 (step: 21750) | train loss: 0.000278 | lr: 0.000100\n",
            "2025-04-01 02:19:52 | INFO | epoch: 10 (step: 21800) | train loss: 0.000383 | lr: 0.000100\n",
            "2025-04-01 02:19:58 | INFO | epoch: 10 (step: 21850) | train loss: 0.000331 | lr: 0.000100\n",
            "2025-04-01 02:20:04 | INFO | epoch: 10 (step: 21900) | train loss: 0.000366 | lr: 0.000100\n",
            "2025-04-01 02:20:10 | INFO | epoch: 10 (step: 21950) | train loss: 0.000338 | lr: 0.000100\n",
            "2025-04-01 02:20:16 | INFO | epoch: 10 (step: 22000) | train loss: 0.000475 | lr: 0.000100\n",
            "2025-04-01 02:20:22 | INFO | epoch: 10 (step: 22050) | train loss: 0.000387 | lr: 0.000100\n",
            "2025-04-01 02:20:28 | INFO | epoch: 10 (step: 22100) | train loss: 0.000260 | lr: 0.000100\n",
            "2025-04-01 02:20:34 | INFO | epoch: 10 (step: 22150) | train loss: 0.000302 | lr: 0.000100\n",
            "2025-04-01 02:20:40 | INFO | epoch: 10 (step: 22200) | train loss: 0.000267 | lr: 0.000100\n",
            "2025-04-01 02:20:47 | INFO | epoch: 10 (step: 22250) | train loss: 0.000251 | lr: 0.000100\n",
            "2025-04-01 02:20:53 | INFO | epoch: 10 (step: 22300) | train loss: 0.000273 | lr: 0.000100\n",
            "2025-04-01 02:20:59 | INFO | epoch: 10 (step: 22350) | train loss: 0.000285 | lr: 0.000100\n",
            "2025-04-01 02:21:05 | INFO | epoch: 10 (step: 22400) | train loss: 0.000247 | lr: 0.000100\n",
            "2025-04-01 02:21:11 | INFO | epoch: 10 (step: 22450) | train loss: 0.000228 | lr: 0.000100\n",
            "2025-04-01 02:21:17 | INFO | epoch: 10 (step: 22500) | train loss: 0.000277 | lr: 0.000100\n",
            "2025-04-01 02:21:23 | INFO | epoch: 10 (step: 22550) | train loss: 0.000259 | lr: 0.000100\n",
            "2025-04-01 02:21:29 | INFO | epoch: 10 (step: 22600) | train loss: 0.000293 | lr: 0.000100\n",
            "2025-04-01 02:21:35 | INFO | epoch: 10 (step: 22650) | train loss: 0.000297 | lr: 0.000100\n",
            "2025-04-01 02:21:41 | INFO | epoch: 10 (step: 22700) | train loss: 0.000228 | lr: 0.000100\n",
            "2025-04-01 02:21:47 | INFO | epoch: 10 (step: 22750) | train loss: 0.000298 | lr: 0.000100\n",
            "2025-04-01 02:21:53 | INFO | epoch: 10 (step: 22800) | train loss: 0.000295 | lr: 0.000100\n",
            "2025-04-01 02:21:59 | INFO | epoch: 10 (step: 22850) | train loss: 0.000253 | lr: 0.000100\n",
            "2025-04-01 02:22:05 | INFO | epoch: 10 (step: 22900) | train loss: 0.000350 | lr: 0.000100\n",
            "2025-04-01 02:22:11 | INFO | epoch: 10 (step: 22950) | train loss: 0.000281 | lr: 0.000100\n",
            "2025-04-01 02:22:17 | INFO | epoch: 10 (step: 23000) | train loss: 0.000273 | lr: 0.000100\n",
            "2025-04-01 02:22:23 | INFO | epoch: 10 (step: 23050) | train loss: 0.000293 | lr: 0.000100\n",
            "2025-04-01 02:22:29 | INFO | epoch: 10 (step: 23100) | train loss: 0.000279 | lr: 0.000100\n",
            "2025-04-01 02:22:35 | INFO | epoch: 10 (step: 23150) | train loss: 0.000326 | lr: 0.000100\n",
            "2025-04-01 02:22:42 | INFO | epoch: 10 (step: 23200) | train loss: 0.000278 | lr: 0.000100\n",
            "2025-04-01 02:22:48 | INFO | epoch: 10 (step: 23250) | train loss: 0.000302 | lr: 0.000100\n",
            "2025-04-01 02:22:55 | INFO | epoch: 11 (step: 23300) | train loss: 0.000365 | lr: 0.000100\n",
            "2025-04-01 02:23:01 | INFO | epoch: 11 (step: 23350) | train loss: 0.000245 | lr: 0.000100\n",
            "2025-04-01 02:23:07 | INFO | epoch: 11 (step: 23400) | train loss: 0.000291 | lr: 0.000100\n",
            "2025-04-01 02:23:13 | INFO | epoch: 11 (step: 23450) | train loss: 0.000332 | lr: 0.000100\n",
            "2025-04-01 02:23:19 | INFO | epoch: 11 (step: 23500) | train loss: 0.000294 | lr: 0.000100\n",
            "2025-04-01 02:23:25 | INFO | epoch: 11 (step: 23550) | train loss: 0.000247 | lr: 0.000100\n",
            "2025-04-01 02:23:31 | INFO | epoch: 11 (step: 23600) | train loss: 0.000281 | lr: 0.000100\n",
            "2025-04-01 02:23:38 | INFO | epoch: 11 (step: 23650) | train loss: 0.000262 | lr: 0.000100\n",
            "2025-04-01 02:23:44 | INFO | epoch: 11 (step: 23700) | train loss: 0.000275 | lr: 0.000100\n",
            "2025-04-01 02:23:49 | INFO | epoch: 11 (step: 23750) | train loss: 0.000238 | lr: 0.000100\n",
            "2025-04-01 02:23:56 | INFO | epoch: 11 (step: 23800) | train loss: 0.000331 | lr: 0.000100\n",
            "2025-04-01 02:24:01 | INFO | epoch: 11 (step: 23850) | train loss: 0.000232 | lr: 0.000100\n",
            "2025-04-01 02:24:08 | INFO | epoch: 11 (step: 23900) | train loss: 0.000291 | lr: 0.000100\n",
            "2025-04-01 02:24:14 | INFO | epoch: 11 (step: 23950) | train loss: 0.000277 | lr: 0.000100\n",
            "2025-04-01 02:24:20 | INFO | epoch: 11 (step: 24000) | train loss: 0.000268 | lr: 0.000100\n",
            "2025-04-01 02:24:26 | INFO | epoch: 11 (step: 24050) | train loss: 0.000302 | lr: 0.000100\n",
            "2025-04-01 02:24:32 | INFO | epoch: 11 (step: 24100) | train loss: 0.000295 | lr: 0.000100\n",
            "2025-04-01 02:24:38 | INFO | epoch: 11 (step: 24150) | train loss: 0.000306 | lr: 0.000100\n",
            "2025-04-01 02:24:44 | INFO | epoch: 11 (step: 24200) | train loss: 0.000275 | lr: 0.000100\n",
            "2025-04-01 02:24:51 | INFO | epoch: 11 (step: 24250) | train loss: 0.000269 | lr: 0.000100\n",
            "2025-04-01 02:24:57 | INFO | epoch: 11 (step: 24300) | train loss: 0.000270 | lr: 0.000100\n",
            "2025-04-01 02:25:03 | INFO | epoch: 11 (step: 24350) | train loss: 0.000341 | lr: 0.000100\n",
            "2025-04-01 02:25:09 | INFO | epoch: 11 (step: 24400) | train loss: 0.000269 | lr: 0.000100\n",
            "2025-04-01 02:25:15 | INFO | epoch: 11 (step: 24450) | train loss: 0.000284 | lr: 0.000100\n",
            "2025-04-01 02:25:21 | INFO | epoch: 11 (step: 24500) | train loss: 0.000250 | lr: 0.000100\n",
            "2025-04-01 02:25:28 | INFO | epoch: 11 (step: 24550) | train loss: 0.000197 | lr: 0.000100\n",
            "2025-04-01 02:25:34 | INFO | epoch: 11 (step: 24600) | train loss: 0.000287 | lr: 0.000100\n",
            "2025-04-01 02:25:40 | INFO | epoch: 11 (step: 24650) | train loss: 0.000183 | lr: 0.000100\n",
            "2025-04-01 02:25:46 | INFO | epoch: 11 (step: 24700) | train loss: 0.000248 | lr: 0.000100\n",
            "2025-04-01 02:25:52 | INFO | epoch: 11 (step: 24750) | train loss: 0.000322 | lr: 0.000100\n",
            "2025-04-01 02:25:58 | INFO | epoch: 11 (step: 24800) | train loss: 0.000253 | lr: 0.000100\n",
            "2025-04-01 02:26:05 | INFO | epoch: 11 (step: 24850) | train loss: 0.000177 | lr: 0.000100\n",
            "2025-04-01 02:26:11 | INFO | epoch: 11 (step: 24900) | train loss: 0.000266 | lr: 0.000100\n",
            "2025-04-01 02:26:17 | INFO | epoch: 11 (step: 24950) | train loss: 0.000251 | lr: 0.000100\n",
            "2025-04-01 02:26:23 | INFO | epoch: 11 (step: 25000) | train loss: 0.000313 | lr: 0.000100\n",
            "2025-04-01 02:26:30 | INFO | epoch: 11 (step: 25000) | valid_loss: 0.003869 | valid_loss: 0.003869\n",
            "2025-04-01 02:26:37 | INFO | epoch: 11 (step: 25000) | best metric updated (loss) - 0.003869\n",
            "2025-04-01 02:26:43 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:26:49 | INFO | epoch: 11 (step: 25050) | train loss: 0.000218 | lr: 0.000100\n",
            "2025-04-01 02:26:55 | INFO | epoch: 11 (step: 25100) | train loss: 0.000232 | lr: 0.000100\n",
            "2025-04-01 02:27:01 | INFO | epoch: 11 (step: 25150) | train loss: 0.000280 | lr: 0.000100\n",
            "2025-04-01 02:27:07 | INFO | epoch: 11 (step: 25200) | train loss: 0.000339 | lr: 0.000100\n",
            "2025-04-01 02:27:13 | INFO | epoch: 11 (step: 25250) | train loss: 0.000216 | lr: 0.000100\n",
            "2025-04-01 02:27:19 | INFO | epoch: 11 (step: 25300) | train loss: 0.000334 | lr: 0.000100\n",
            "2025-04-01 02:27:25 | INFO | epoch: 11 (step: 25350) | train loss: 0.000263 | lr: 0.000100\n",
            "2025-04-01 02:27:31 | INFO | epoch: 11 (step: 25400) | train loss: 0.000246 | lr: 0.000100\n",
            "2025-04-01 02:27:37 | INFO | epoch: 11 (step: 25450) | train loss: 0.000225 | lr: 0.000100\n",
            "2025-04-01 02:27:43 | INFO | epoch: 11 (step: 25500) | train loss: 0.000305 | lr: 0.000100\n",
            "2025-04-01 02:27:49 | INFO | epoch: 11 (step: 25550) | train loss: 0.000262 | lr: 0.000100\n",
            "2025-04-01 02:27:56 | INFO | epoch: 11 (step: 25600) | train loss: 0.000311 | lr: 0.000100\n",
            "2025-04-01 02:28:03 | INFO | epoch: 12 (step: 25650) | train loss: 0.000237 | lr: 0.000100\n",
            "2025-04-01 02:28:09 | INFO | epoch: 12 (step: 25700) | train loss: 0.000319 | lr: 0.000100\n",
            "2025-04-01 02:28:15 | INFO | epoch: 12 (step: 25750) | train loss: 0.000219 | lr: 0.000100\n",
            "2025-04-01 02:28:21 | INFO | epoch: 12 (step: 25800) | train loss: 0.000243 | lr: 0.000100\n",
            "2025-04-01 02:28:27 | INFO | epoch: 12 (step: 25850) | train loss: 0.000267 | lr: 0.000100\n",
            "2025-04-01 02:28:33 | INFO | epoch: 12 (step: 25900) | train loss: 0.000222 | lr: 0.000100\n",
            "2025-04-01 02:28:39 | INFO | epoch: 12 (step: 25950) | train loss: 0.000283 | lr: 0.000100\n",
            "2025-04-01 02:28:45 | INFO | epoch: 12 (step: 26000) | train loss: 0.000246 | lr: 0.000100\n",
            "2025-04-01 02:28:52 | INFO | epoch: 12 (step: 26050) | train loss: 0.000286 | lr: 0.000100\n",
            "2025-04-01 02:28:58 | INFO | epoch: 12 (step: 26100) | train loss: 0.000173 | lr: 0.000100\n",
            "2025-04-01 02:29:04 | INFO | epoch: 12 (step: 26150) | train loss: 0.000209 | lr: 0.000100\n",
            "2025-04-01 02:29:10 | INFO | epoch: 12 (step: 26200) | train loss: 0.000193 | lr: 0.000100\n",
            "2025-04-01 02:29:16 | INFO | epoch: 12 (step: 26250) | train loss: 0.000216 | lr: 0.000100\n",
            "2025-04-01 02:29:22 | INFO | epoch: 12 (step: 26300) | train loss: 0.000231 | lr: 0.000100\n",
            "2025-04-01 02:29:28 | INFO | epoch: 12 (step: 26350) | train loss: 0.000174 | lr: 0.000100\n",
            "2025-04-01 02:29:35 | INFO | epoch: 12 (step: 26400) | train loss: 0.000194 | lr: 0.000100\n",
            "2025-04-01 02:29:41 | INFO | epoch: 12 (step: 26450) | train loss: 0.000176 | lr: 0.000100\n",
            "2025-04-01 02:29:47 | INFO | epoch: 12 (step: 26500) | train loss: 0.000183 | lr: 0.000100\n",
            "2025-04-01 02:29:53 | INFO | epoch: 12 (step: 26550) | train loss: 0.000220 | lr: 0.000100\n",
            "2025-04-01 02:29:59 | INFO | epoch: 12 (step: 26600) | train loss: 0.000224 | lr: 0.000100\n",
            "2025-04-01 02:30:05 | INFO | epoch: 12 (step: 26650) | train loss: 0.000196 | lr: 0.000100\n",
            "2025-04-01 02:30:11 | INFO | epoch: 12 (step: 26700) | train loss: 0.000220 | lr: 0.000100\n",
            "2025-04-01 02:30:17 | INFO | epoch: 12 (step: 26750) | train loss: 0.000179 | lr: 0.000100\n",
            "2025-04-01 02:30:24 | INFO | epoch: 12 (step: 26800) | train loss: 0.000194 | lr: 0.000100\n",
            "2025-04-01 02:30:30 | INFO | epoch: 12 (step: 26850) | train loss: 0.000205 | lr: 0.000100\n",
            "2025-04-01 02:30:36 | INFO | epoch: 12 (step: 26900) | train loss: 0.000200 | lr: 0.000100\n",
            "2025-04-01 02:30:42 | INFO | epoch: 12 (step: 26950) | train loss: 0.000228 | lr: 0.000100\n",
            "2025-04-01 02:30:48 | INFO | epoch: 12 (step: 27000) | train loss: 0.000182 | lr: 0.000100\n",
            "2025-04-01 02:30:54 | INFO | epoch: 12 (step: 27050) | train loss: 0.000195 | lr: 0.000100\n",
            "2025-04-01 02:31:00 | INFO | epoch: 12 (step: 27100) | train loss: 0.000191 | lr: 0.000100\n",
            "2025-04-01 02:31:06 | INFO | epoch: 12 (step: 27150) | train loss: 0.000237 | lr: 0.000100\n",
            "2025-04-01 02:31:12 | INFO | epoch: 12 (step: 27200) | train loss: 0.000289 | lr: 0.000100\n",
            "2025-04-01 02:31:18 | INFO | epoch: 12 (step: 27250) | train loss: 0.000263 | lr: 0.000100\n",
            "2025-04-01 02:31:24 | INFO | epoch: 12 (step: 27300) | train loss: 0.000265 | lr: 0.000100\n",
            "2025-04-01 02:31:30 | INFO | epoch: 12 (step: 27350) | train loss: 0.000229 | lr: 0.000100\n",
            "2025-04-01 02:31:36 | INFO | epoch: 12 (step: 27400) | train loss: 0.000214 | lr: 0.000100\n",
            "2025-04-01 02:31:43 | INFO | epoch: 12 (step: 27450) | train loss: 0.000272 | lr: 0.000100\n",
            "2025-04-01 02:31:49 | INFO | epoch: 12 (step: 27500) | train loss: 0.000309 | lr: 0.000100\n",
            "2025-04-01 02:31:55 | INFO | epoch: 12 (step: 27550) | train loss: 0.000194 | lr: 0.000100\n",
            "2025-04-01 02:32:01 | INFO | epoch: 12 (step: 27600) | train loss: 0.000222 | lr: 0.000100\n",
            "2025-04-01 02:32:07 | INFO | epoch: 12 (step: 27650) | train loss: 0.000192 | lr: 0.000100\n",
            "2025-04-01 02:32:13 | INFO | epoch: 12 (step: 27700) | train loss: 0.000243 | lr: 0.000100\n",
            "2025-04-01 02:32:19 | INFO | epoch: 12 (step: 27750) | train loss: 0.000202 | lr: 0.000100\n",
            "2025-04-01 02:32:25 | INFO | epoch: 12 (step: 27800) | train loss: 0.000254 | lr: 0.000100\n",
            "2025-04-01 02:32:31 | INFO | epoch: 12 (step: 27850) | train loss: 0.000267 | lr: 0.000100\n",
            "2025-04-01 02:32:37 | INFO | epoch: 12 (step: 27900) | train loss: 0.000242 | lr: 0.000100\n",
            "2025-04-01 02:32:45 | INFO | epoch: 13 (step: 27950) | train loss: 0.000222 | lr: 0.000100\n",
            "2025-04-01 02:32:51 | INFO | epoch: 13 (step: 28000) | train loss: 0.000276 | lr: 0.000100\n",
            "2025-04-01 02:32:57 | INFO | epoch: 13 (step: 28050) | train loss: 0.000182 | lr: 0.000100\n",
            "2025-04-01 02:33:03 | INFO | epoch: 13 (step: 28100) | train loss: 0.000193 | lr: 0.000100\n",
            "2025-04-01 02:33:09 | INFO | epoch: 13 (step: 28150) | train loss: 0.000207 | lr: 0.000100\n",
            "2025-04-01 02:33:15 | INFO | epoch: 13 (step: 28200) | train loss: 0.000293 | lr: 0.000100\n",
            "2025-04-01 02:33:21 | INFO | epoch: 13 (step: 28250) | train loss: 0.000283 | lr: 0.000100\n",
            "2025-04-01 02:33:27 | INFO | epoch: 13 (step: 28300) | train loss: 0.000231 | lr: 0.000100\n",
            "2025-04-01 02:33:33 | INFO | epoch: 13 (step: 28350) | train loss: 0.000200 | lr: 0.000100\n",
            "2025-04-01 02:33:40 | INFO | epoch: 13 (step: 28400) | train loss: 0.000239 | lr: 0.000100\n",
            "2025-04-01 02:33:46 | INFO | epoch: 13 (step: 28450) | train loss: 0.000185 | lr: 0.000100\n",
            "2025-04-01 02:33:52 | INFO | epoch: 13 (step: 28500) | train loss: 0.000233 | lr: 0.000100\n",
            "2025-04-01 02:33:58 | INFO | epoch: 13 (step: 28550) | train loss: 0.000209 | lr: 0.000100\n",
            "2025-04-01 02:34:04 | INFO | epoch: 13 (step: 28600) | train loss: 0.000212 | lr: 0.000100\n",
            "2025-04-01 02:34:10 | INFO | epoch: 13 (step: 28650) | train loss: 0.000232 | lr: 0.000100\n",
            "2025-04-01 02:34:16 | INFO | epoch: 13 (step: 28700) | train loss: 0.000142 | lr: 0.000100\n",
            "2025-04-01 02:34:22 | INFO | epoch: 13 (step: 28750) | train loss: 0.000254 | lr: 0.000100\n",
            "2025-04-01 02:34:28 | INFO | epoch: 13 (step: 28800) | train loss: 0.000174 | lr: 0.000100\n",
            "2025-04-01 02:34:34 | INFO | epoch: 13 (step: 28850) | train loss: 0.000190 | lr: 0.000100\n",
            "2025-04-01 02:34:41 | INFO | epoch: 13 (step: 28900) | train loss: 0.000188 | lr: 0.000100\n",
            "2025-04-01 02:34:47 | INFO | epoch: 13 (step: 28950) | train loss: 0.000276 | lr: 0.000100\n",
            "2025-04-01 02:34:53 | INFO | epoch: 13 (step: 29000) | train loss: 0.000178 | lr: 0.000100\n",
            "2025-04-01 02:34:59 | INFO | epoch: 13 (step: 29050) | train loss: 0.000329 | lr: 0.000100\n",
            "2025-04-01 02:35:05 | INFO | epoch: 13 (step: 29100) | train loss: 0.000166 | lr: 0.000100\n",
            "2025-04-01 02:35:11 | INFO | epoch: 13 (step: 29150) | train loss: 0.000188 | lr: 0.000100\n",
            "2025-04-01 02:35:17 | INFO | epoch: 13 (step: 29200) | train loss: 0.000195 | lr: 0.000100\n",
            "2025-04-01 02:35:23 | INFO | epoch: 13 (step: 29250) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:35:29 | INFO | epoch: 13 (step: 29300) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 02:35:35 | INFO | epoch: 13 (step: 29350) | train loss: 0.000148 | lr: 0.000100\n",
            "2025-04-01 02:35:42 | INFO | epoch: 13 (step: 29400) | train loss: 0.000179 | lr: 0.000100\n",
            "2025-04-01 02:35:48 | INFO | epoch: 13 (step: 29450) | train loss: 0.000190 | lr: 0.000100\n",
            "2025-04-01 02:35:54 | INFO | epoch: 13 (step: 29500) | train loss: 0.000201 | lr: 0.000100\n",
            "2025-04-01 02:36:00 | INFO | epoch: 13 (step: 29550) | train loss: 0.000167 | lr: 0.000100\n",
            "2025-04-01 02:36:06 | INFO | epoch: 13 (step: 29600) | train loss: 0.000156 | lr: 0.000100\n",
            "2025-04-01 02:36:12 | INFO | epoch: 13 (step: 29650) | train loss: 0.000174 | lr: 0.000100\n",
            "2025-04-01 02:36:18 | INFO | epoch: 13 (step: 29700) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:36:24 | INFO | epoch: 13 (step: 29750) | train loss: 0.000165 | lr: 0.000100\n",
            "2025-04-01 02:36:30 | INFO | epoch: 13 (step: 29800) | train loss: 0.000180 | lr: 0.000100\n",
            "2025-04-01 02:36:36 | INFO | epoch: 13 (step: 29850) | train loss: 0.000150 | lr: 0.000100\n",
            "2025-04-01 02:36:42 | INFO | epoch: 13 (step: 29900) | train loss: 0.000264 | lr: 0.000100\n",
            "2025-04-01 02:36:48 | INFO | epoch: 13 (step: 29950) | train loss: 0.000212 | lr: 0.000100\n",
            "2025-04-01 02:36:54 | INFO | epoch: 13 (step: 30000) | train loss: 0.000175 | lr: 0.000100\n",
            "2025-04-01 02:37:01 | INFO | epoch: 13 (step: 30000) | valid_loss: 0.004010 | valid_loss: 0.004010\n",
            "2025-04-01 02:37:06 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:37:12 | INFO | epoch: 13 (step: 30050) | train loss: 0.000197 | lr: 0.000100\n",
            "2025-04-01 02:37:18 | INFO | epoch: 13 (step: 30100) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 02:37:24 | INFO | epoch: 13 (step: 30150) | train loss: 0.000262 | lr: 0.000100\n",
            "2025-04-01 02:37:30 | INFO | epoch: 13 (step: 30200) | train loss: 0.000170 | lr: 0.000100\n",
            "2025-04-01 02:37:37 | INFO | epoch: 13 (step: 30250) | train loss: 0.000197 | lr: 0.000100\n",
            "2025-04-01 02:37:44 | INFO | epoch: 14 (step: 30300) | train loss: 0.000197 | lr: 0.000100\n",
            "2025-04-01 02:37:51 | INFO | epoch: 14 (step: 30350) | train loss: 0.000161 | lr: 0.000100\n",
            "2025-04-01 02:37:57 | INFO | epoch: 14 (step: 30400) | train loss: 0.000225 | lr: 0.000100\n",
            "2025-04-01 02:38:03 | INFO | epoch: 14 (step: 30450) | train loss: 0.000231 | lr: 0.000100\n",
            "2025-04-01 02:38:09 | INFO | epoch: 14 (step: 30500) | train loss: 0.000140 | lr: 0.000100\n",
            "2025-04-01 02:38:15 | INFO | epoch: 14 (step: 30550) | train loss: 0.000193 | lr: 0.000100\n",
            "2025-04-01 02:38:21 | INFO | epoch: 14 (step: 30600) | train loss: 0.000208 | lr: 0.000100\n",
            "2025-04-01 02:38:27 | INFO | epoch: 14 (step: 30650) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 02:38:33 | INFO | epoch: 14 (step: 30700) | train loss: 0.000175 | lr: 0.000100\n",
            "2025-04-01 02:38:40 | INFO | epoch: 14 (step: 30750) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:38:46 | INFO | epoch: 14 (step: 30800) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 02:38:52 | INFO | epoch: 14 (step: 30850) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 02:38:58 | INFO | epoch: 14 (step: 30900) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 02:39:04 | INFO | epoch: 14 (step: 30950) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 02:39:10 | INFO | epoch: 14 (step: 31000) | train loss: 0.000156 | lr: 0.000100\n",
            "2025-04-01 02:39:16 | INFO | epoch: 14 (step: 31050) | train loss: 0.000172 | lr: 0.000100\n",
            "2025-04-01 02:39:22 | INFO | epoch: 14 (step: 31100) | train loss: 0.000166 | lr: 0.000100\n",
            "2025-04-01 02:39:28 | INFO | epoch: 14 (step: 31150) | train loss: 0.000138 | lr: 0.000100\n",
            "2025-04-01 02:39:34 | INFO | epoch: 14 (step: 31200) | train loss: 0.000205 | lr: 0.000100\n",
            "2025-04-01 02:39:40 | INFO | epoch: 14 (step: 31250) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 02:39:46 | INFO | epoch: 14 (step: 31300) | train loss: 0.000170 | lr: 0.000100\n",
            "2025-04-01 02:39:52 | INFO | epoch: 14 (step: 31350) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:39:58 | INFO | epoch: 14 (step: 31400) | train loss: 0.000168 | lr: 0.000100\n",
            "2025-04-01 02:40:04 | INFO | epoch: 14 (step: 31450) | train loss: 0.000208 | lr: 0.000100\n",
            "2025-04-01 02:40:11 | INFO | epoch: 14 (step: 31500) | train loss: 0.000163 | lr: 0.000100\n",
            "2025-04-01 02:40:17 | INFO | epoch: 14 (step: 31550) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 02:40:23 | INFO | epoch: 14 (step: 31600) | train loss: 0.000160 | lr: 0.000100\n",
            "2025-04-01 02:40:29 | INFO | epoch: 14 (step: 31650) | train loss: 0.000162 | lr: 0.000100\n",
            "2025-04-01 02:40:35 | INFO | epoch: 14 (step: 31700) | train loss: 0.000178 | lr: 0.000100\n",
            "2025-04-01 02:40:41 | INFO | epoch: 14 (step: 31750) | train loss: 0.000178 | lr: 0.000100\n",
            "2025-04-01 02:40:47 | INFO | epoch: 14 (step: 31800) | train loss: 0.000180 | lr: 0.000100\n",
            "2025-04-01 02:40:53 | INFO | epoch: 14 (step: 31850) | train loss: 0.000155 | lr: 0.000100\n",
            "2025-04-01 02:40:59 | INFO | epoch: 14 (step: 31900) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:41:06 | INFO | epoch: 14 (step: 31950) | train loss: 0.000168 | lr: 0.000100\n",
            "2025-04-01 02:41:12 | INFO | epoch: 14 (step: 32000) | train loss: 0.000215 | lr: 0.000100\n",
            "2025-04-01 02:41:18 | INFO | epoch: 14 (step: 32050) | train loss: 0.000190 | lr: 0.000100\n",
            "2025-04-01 02:41:24 | INFO | epoch: 14 (step: 32100) | train loss: 0.000213 | lr: 0.000100\n",
            "2025-04-01 02:41:30 | INFO | epoch: 14 (step: 32150) | train loss: 0.000117 | lr: 0.000100\n",
            "2025-04-01 02:41:36 | INFO | epoch: 14 (step: 32200) | train loss: 0.000117 | lr: 0.000100\n",
            "2025-04-01 02:41:42 | INFO | epoch: 14 (step: 32250) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 02:41:48 | INFO | epoch: 14 (step: 32300) | train loss: 0.000158 | lr: 0.000100\n",
            "2025-04-01 02:41:54 | INFO | epoch: 14 (step: 32350) | train loss: 0.000129 | lr: 0.000100\n",
            "2025-04-01 02:42:00 | INFO | epoch: 14 (step: 32400) | train loss: 0.000145 | lr: 0.000100\n",
            "2025-04-01 02:42:07 | INFO | epoch: 14 (step: 32450) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 02:42:13 | INFO | epoch: 14 (step: 32500) | train loss: 0.000182 | lr: 0.000100\n",
            "2025-04-01 02:42:19 | INFO | epoch: 14 (step: 32550) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 02:42:25 | INFO | epoch: 14 (step: 32600) | train loss: 0.000138 | lr: 0.000100\n",
            "2025-04-01 02:42:32 | INFO | epoch: 15 (step: 32650) | train loss: 0.000154 | lr: 0.000100\n",
            "2025-04-01 02:42:38 | INFO | epoch: 15 (step: 32700) | train loss: 0.000107 | lr: 0.000100\n",
            "2025-04-01 02:42:44 | INFO | epoch: 15 (step: 32750) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 02:42:50 | INFO | epoch: 15 (step: 32800) | train loss: 0.000153 | lr: 0.000100\n",
            "2025-04-01 02:42:56 | INFO | epoch: 15 (step: 32850) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 02:43:03 | INFO | epoch: 15 (step: 32900) | train loss: 0.000135 | lr: 0.000100\n",
            "2025-04-01 02:43:09 | INFO | epoch: 15 (step: 32950) | train loss: 0.000160 | lr: 0.000100\n",
            "2025-04-01 02:43:15 | INFO | epoch: 15 (step: 33000) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 02:43:21 | INFO | epoch: 15 (step: 33050) | train loss: 0.000121 | lr: 0.000100\n",
            "2025-04-01 02:43:27 | INFO | epoch: 15 (step: 33100) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 02:43:33 | INFO | epoch: 15 (step: 33150) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 02:43:39 | INFO | epoch: 15 (step: 33200) | train loss: 0.000202 | lr: 0.000100\n",
            "2025-04-01 02:43:45 | INFO | epoch: 15 (step: 33250) | train loss: 0.000153 | lr: 0.000100\n",
            "2025-04-01 02:43:51 | INFO | epoch: 15 (step: 33300) | train loss: 0.000207 | lr: 0.000100\n",
            "2025-04-01 02:43:57 | INFO | epoch: 15 (step: 33350) | train loss: 0.000168 | lr: 0.000100\n",
            "2025-04-01 02:44:03 | INFO | epoch: 15 (step: 33400) | train loss: 0.000149 | lr: 0.000100\n",
            "2025-04-01 02:44:09 | INFO | epoch: 15 (step: 33450) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 02:44:15 | INFO | epoch: 15 (step: 33500) | train loss: 0.000228 | lr: 0.000100\n",
            "2025-04-01 02:44:21 | INFO | epoch: 15 (step: 33550) | train loss: 0.000204 | lr: 0.000100\n",
            "2025-04-01 02:44:28 | INFO | epoch: 15 (step: 33600) | train loss: 0.000195 | lr: 0.000100\n",
            "2025-04-01 02:44:34 | INFO | epoch: 15 (step: 33650) | train loss: 0.000166 | lr: 0.000100\n",
            "2025-04-01 02:44:39 | INFO | epoch: 15 (step: 33700) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 02:44:46 | INFO | epoch: 15 (step: 33750) | train loss: 0.000157 | lr: 0.000100\n",
            "2025-04-01 02:44:51 | INFO | epoch: 15 (step: 33800) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 02:44:58 | INFO | epoch: 15 (step: 33850) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:45:03 | INFO | epoch: 15 (step: 33900) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 02:45:09 | INFO | epoch: 15 (step: 33950) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 02:45:15 | INFO | epoch: 15 (step: 34000) | train loss: 0.000153 | lr: 0.000100\n",
            "2025-04-01 02:45:21 | INFO | epoch: 15 (step: 34050) | train loss: 0.000127 | lr: 0.000100\n",
            "2025-04-01 02:45:28 | INFO | epoch: 15 (step: 34100) | train loss: 0.000171 | lr: 0.000100\n",
            "2025-04-01 02:45:34 | INFO | epoch: 15 (step: 34150) | train loss: 0.000165 | lr: 0.000100\n",
            "2025-04-01 02:45:40 | INFO | epoch: 15 (step: 34200) | train loss: 0.000132 | lr: 0.000100\n",
            "2025-04-01 02:45:46 | INFO | epoch: 15 (step: 34250) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 02:45:52 | INFO | epoch: 15 (step: 34300) | train loss: 0.000150 | lr: 0.000100\n",
            "2025-04-01 02:45:58 | INFO | epoch: 15 (step: 34350) | train loss: 0.000125 | lr: 0.000100\n",
            "2025-04-01 02:46:05 | INFO | epoch: 15 (step: 34400) | train loss: 0.000165 | lr: 0.000100\n",
            "2025-04-01 02:46:11 | INFO | epoch: 15 (step: 34450) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 02:46:17 | INFO | epoch: 15 (step: 34500) | train loss: 0.000114 | lr: 0.000100\n",
            "2025-04-01 02:46:23 | INFO | epoch: 15 (step: 34550) | train loss: 0.000181 | lr: 0.000100\n",
            "2025-04-01 02:46:29 | INFO | epoch: 15 (step: 34600) | train loss: 0.000174 | lr: 0.000100\n",
            "2025-04-01 02:46:35 | INFO | epoch: 15 (step: 34650) | train loss: 0.000163 | lr: 0.000100\n",
            "2025-04-01 02:46:41 | INFO | epoch: 15 (step: 34700) | train loss: 0.000179 | lr: 0.000100\n",
            "2025-04-01 02:46:47 | INFO | epoch: 15 (step: 34750) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 02:46:54 | INFO | epoch: 15 (step: 34800) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 02:47:00 | INFO | epoch: 15 (step: 34850) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 02:47:06 | INFO | epoch: 15 (step: 34900) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 02:47:14 | INFO | epoch: 16 (step: 34950) | train loss: 0.000171 | lr: 0.000100\n",
            "2025-04-01 02:47:20 | INFO | epoch: 16 (step: 35000) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 02:47:27 | INFO | epoch: 16 (step: 35000) | valid_loss: 0.002906 | valid_loss: 0.002906\n",
            "2025-04-01 02:47:35 | INFO | epoch: 16 (step: 35000) | best metric updated (loss) - 0.002906\n",
            "2025-04-01 02:47:39 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:47:45 | INFO | epoch: 16 (step: 35050) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 02:47:51 | INFO | epoch: 16 (step: 35100) | train loss: 0.000111 | lr: 0.000100\n",
            "2025-04-01 02:47:57 | INFO | epoch: 16 (step: 35150) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 02:48:03 | INFO | epoch: 16 (step: 35200) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 02:48:09 | INFO | epoch: 16 (step: 35250) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 02:48:15 | INFO | epoch: 16 (step: 35300) | train loss: 0.000102 | lr: 0.000100\n",
            "2025-04-01 02:48:21 | INFO | epoch: 16 (step: 35350) | train loss: 0.000142 | lr: 0.000100\n",
            "2025-04-01 02:48:27 | INFO | epoch: 16 (step: 35400) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 02:48:33 | INFO | epoch: 16 (step: 35450) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 02:48:39 | INFO | epoch: 16 (step: 35500) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 02:48:45 | INFO | epoch: 16 (step: 35550) | train loss: 0.000135 | lr: 0.000100\n",
            "2025-04-01 02:48:51 | INFO | epoch: 16 (step: 35600) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 02:48:57 | INFO | epoch: 16 (step: 35650) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:49:04 | INFO | epoch: 16 (step: 35700) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 02:49:09 | INFO | epoch: 16 (step: 35750) | train loss: 0.000111 | lr: 0.000100\n",
            "2025-04-01 02:49:15 | INFO | epoch: 16 (step: 35800) | train loss: 0.000118 | lr: 0.000100\n",
            "2025-04-01 02:49:21 | INFO | epoch: 16 (step: 35850) | train loss: 0.000109 | lr: 0.000100\n",
            "2025-04-01 02:49:28 | INFO | epoch: 16 (step: 35900) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 02:49:34 | INFO | epoch: 16 (step: 35950) | train loss: 0.000129 | lr: 0.000100\n",
            "2025-04-01 02:49:40 | INFO | epoch: 16 (step: 36000) | train loss: 0.000121 | lr: 0.000100\n",
            "2025-04-01 02:49:46 | INFO | epoch: 16 (step: 36050) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 02:49:52 | INFO | epoch: 16 (step: 36100) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 02:49:58 | INFO | epoch: 16 (step: 36150) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 02:50:04 | INFO | epoch: 16 (step: 36200) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 02:50:10 | INFO | epoch: 16 (step: 36250) | train loss: 0.000125 | lr: 0.000100\n",
            "2025-04-01 02:50:16 | INFO | epoch: 16 (step: 36300) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 02:50:22 | INFO | epoch: 16 (step: 36350) | train loss: 0.000187 | lr: 0.000100\n",
            "2025-04-01 02:50:28 | INFO | epoch: 16 (step: 36400) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 02:50:34 | INFO | epoch: 16 (step: 36450) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:50:40 | INFO | epoch: 16 (step: 36500) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 02:50:46 | INFO | epoch: 16 (step: 36550) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 02:50:52 | INFO | epoch: 16 (step: 36600) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 02:50:58 | INFO | epoch: 16 (step: 36650) | train loss: 0.000187 | lr: 0.000100\n",
            "2025-04-01 02:51:04 | INFO | epoch: 16 (step: 36700) | train loss: 0.000217 | lr: 0.000100\n",
            "2025-04-01 02:51:11 | INFO | epoch: 16 (step: 36750) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 02:51:17 | INFO | epoch: 16 (step: 36800) | train loss: 0.000211 | lr: 0.000100\n",
            "2025-04-01 02:51:23 | INFO | epoch: 16 (step: 36850) | train loss: 0.000127 | lr: 0.000100\n",
            "2025-04-01 02:51:29 | INFO | epoch: 16 (step: 36900) | train loss: 0.000225 | lr: 0.000100\n",
            "2025-04-01 02:51:35 | INFO | epoch: 16 (step: 36950) | train loss: 0.000128 | lr: 0.000100\n",
            "2025-04-01 02:51:41 | INFO | epoch: 16 (step: 37000) | train loss: 0.000197 | lr: 0.000100\n",
            "2025-04-01 02:51:48 | INFO | epoch: 16 (step: 37050) | train loss: 0.000217 | lr: 0.000100\n",
            "2025-04-01 02:51:54 | INFO | epoch: 16 (step: 37100) | train loss: 0.000295 | lr: 0.000100\n",
            "2025-04-01 02:52:00 | INFO | epoch: 16 (step: 37150) | train loss: 0.000221 | lr: 0.000100\n",
            "2025-04-01 02:52:06 | INFO | epoch: 16 (step: 37200) | train loss: 0.000226 | lr: 0.000100\n",
            "2025-04-01 02:52:12 | INFO | epoch: 16 (step: 37250) | train loss: 0.000142 | lr: 0.000100\n",
            "2025-04-01 02:52:19 | INFO | epoch: 17 (step: 37300) | train loss: 0.000174 | lr: 0.000100\n",
            "2025-04-01 02:52:25 | INFO | epoch: 17 (step: 37350) | train loss: 0.000173 | lr: 0.000100\n",
            "2025-04-01 02:52:31 | INFO | epoch: 17 (step: 37400) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:52:37 | INFO | epoch: 17 (step: 37450) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 02:52:43 | INFO | epoch: 17 (step: 37500) | train loss: 0.000132 | lr: 0.000100\n",
            "2025-04-01 02:52:50 | INFO | epoch: 17 (step: 37550) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 02:52:56 | INFO | epoch: 17 (step: 37600) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 02:53:02 | INFO | epoch: 17 (step: 37650) | train loss: 0.000140 | lr: 0.000100\n",
            "2025-04-01 02:53:08 | INFO | epoch: 17 (step: 37700) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 02:53:14 | INFO | epoch: 17 (step: 37750) | train loss: 0.000219 | lr: 0.000100\n",
            "2025-04-01 02:53:20 | INFO | epoch: 17 (step: 37800) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 02:53:26 | INFO | epoch: 17 (step: 37850) | train loss: 0.000159 | lr: 0.000100\n",
            "2025-04-01 02:53:32 | INFO | epoch: 17 (step: 37900) | train loss: 0.000192 | lr: 0.000100\n",
            "2025-04-01 02:53:38 | INFO | epoch: 17 (step: 37950) | train loss: 0.000127 | lr: 0.000100\n",
            "2025-04-01 02:53:44 | INFO | epoch: 17 (step: 38000) | train loss: 0.000160 | lr: 0.000100\n",
            "2025-04-01 02:53:50 | INFO | epoch: 17 (step: 38050) | train loss: 0.000132 | lr: 0.000100\n",
            "2025-04-01 02:53:56 | INFO | epoch: 17 (step: 38100) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 02:54:02 | INFO | epoch: 17 (step: 38150) | train loss: 0.000188 | lr: 0.000100\n",
            "2025-04-01 02:54:09 | INFO | epoch: 17 (step: 38200) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 02:54:15 | INFO | epoch: 17 (step: 38250) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 02:54:21 | INFO | epoch: 17 (step: 38300) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 02:54:27 | INFO | epoch: 17 (step: 38350) | train loss: 0.000138 | lr: 0.000100\n",
            "2025-04-01 02:54:33 | INFO | epoch: 17 (step: 38400) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 02:54:40 | INFO | epoch: 17 (step: 38450) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 02:54:46 | INFO | epoch: 17 (step: 38500) | train loss: 0.000155 | lr: 0.000100\n",
            "2025-04-01 02:54:52 | INFO | epoch: 17 (step: 38550) | train loss: 0.000160 | lr: 0.000100\n",
            "2025-04-01 02:54:58 | INFO | epoch: 17 (step: 38600) | train loss: 0.000173 | lr: 0.000100\n",
            "2025-04-01 02:55:04 | INFO | epoch: 17 (step: 38650) | train loss: 0.000132 | lr: 0.000100\n",
            "2025-04-01 02:55:10 | INFO | epoch: 17 (step: 38700) | train loss: 0.000188 | lr: 0.000100\n",
            "2025-04-01 02:55:16 | INFO | epoch: 17 (step: 38750) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 02:55:23 | INFO | epoch: 17 (step: 38800) | train loss: 0.000155 | lr: 0.000100\n",
            "2025-04-01 02:55:29 | INFO | epoch: 17 (step: 38850) | train loss: 0.000179 | lr: 0.000100\n",
            "2025-04-01 02:55:35 | INFO | epoch: 17 (step: 38900) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 02:55:41 | INFO | epoch: 17 (step: 38950) | train loss: 0.000171 | lr: 0.000100\n",
            "2025-04-01 02:55:47 | INFO | epoch: 17 (step: 39000) | train loss: 0.000094 | lr: 0.000100\n",
            "2025-04-01 02:55:53 | INFO | epoch: 17 (step: 39050) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 02:56:00 | INFO | epoch: 17 (step: 39100) | train loss: 0.000145 | lr: 0.000100\n",
            "2025-04-01 02:56:05 | INFO | epoch: 17 (step: 39150) | train loss: 0.000150 | lr: 0.000100\n",
            "2025-04-01 02:56:12 | INFO | epoch: 17 (step: 39200) | train loss: 0.000191 | lr: 0.000100\n",
            "2025-04-01 02:56:18 | INFO | epoch: 17 (step: 39250) | train loss: 0.000132 | lr: 0.000100\n",
            "2025-04-01 02:56:24 | INFO | epoch: 17 (step: 39300) | train loss: 0.000156 | lr: 0.000100\n",
            "2025-04-01 02:56:30 | INFO | epoch: 17 (step: 39350) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 02:56:36 | INFO | epoch: 17 (step: 39400) | train loss: 0.000181 | lr: 0.000100\n",
            "2025-04-01 02:56:42 | INFO | epoch: 17 (step: 39450) | train loss: 0.000127 | lr: 0.000100\n",
            "2025-04-01 02:56:48 | INFO | epoch: 17 (step: 39500) | train loss: 0.000159 | lr: 0.000100\n",
            "2025-04-01 02:56:54 | INFO | epoch: 17 (step: 39550) | train loss: 0.000139 | lr: 0.000100\n",
            "2025-04-01 02:57:02 | INFO | epoch: 18 (step: 39600) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 02:57:08 | INFO | epoch: 18 (step: 39650) | train loss: 0.000087 | lr: 0.000100\n",
            "2025-04-01 02:57:14 | INFO | epoch: 18 (step: 39700) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 02:57:21 | INFO | epoch: 18 (step: 39750) | train loss: 0.000123 | lr: 0.000100\n",
            "2025-04-01 02:57:27 | INFO | epoch: 18 (step: 39800) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 02:57:33 | INFO | epoch: 18 (step: 39850) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 02:57:39 | INFO | epoch: 18 (step: 39900) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 02:57:45 | INFO | epoch: 18 (step: 39950) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 02:57:51 | INFO | epoch: 18 (step: 40000) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 02:57:58 | INFO | epoch: 18 (step: 40000) | valid_loss: 0.002764 | valid_loss: 0.002764\n",
            "2025-04-01 02:58:03 | INFO | epoch: 18 (step: 40000) | best metric updated (loss) - 0.002764\n",
            "2025-04-01 02:58:07 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 02:58:13 | INFO | epoch: 18 (step: 40050) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 02:58:19 | INFO | epoch: 18 (step: 40100) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 02:58:25 | INFO | epoch: 18 (step: 40150) | train loss: 0.000140 | lr: 0.000100\n",
            "2025-04-01 02:58:31 | INFO | epoch: 18 (step: 40200) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 02:58:37 | INFO | epoch: 18 (step: 40250) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 02:58:43 | INFO | epoch: 18 (step: 40300) | train loss: 0.000104 | lr: 0.000100\n",
            "2025-04-01 02:58:49 | INFO | epoch: 18 (step: 40350) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 02:58:55 | INFO | epoch: 18 (step: 40400) | train loss: 0.000149 | lr: 0.000100\n",
            "2025-04-01 02:59:01 | INFO | epoch: 18 (step: 40450) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 02:59:07 | INFO | epoch: 18 (step: 40500) | train loss: 0.000167 | lr: 0.000100\n",
            "2025-04-01 02:59:13 | INFO | epoch: 18 (step: 40550) | train loss: 0.000106 | lr: 0.000100\n",
            "2025-04-01 02:59:19 | INFO | epoch: 18 (step: 40600) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 02:59:25 | INFO | epoch: 18 (step: 40650) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:59:31 | INFO | epoch: 18 (step: 40700) | train loss: 0.000177 | lr: 0.000100\n",
            "2025-04-01 02:59:37 | INFO | epoch: 18 (step: 40750) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 02:59:44 | INFO | epoch: 18 (step: 40800) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 02:59:50 | INFO | epoch: 18 (step: 40850) | train loss: 0.000167 | lr: 0.000100\n",
            "2025-04-01 02:59:56 | INFO | epoch: 18 (step: 40900) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 03:00:02 | INFO | epoch: 18 (step: 40950) | train loss: 0.000104 | lr: 0.000100\n",
            "2025-04-01 03:00:08 | INFO | epoch: 18 (step: 41000) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:00:14 | INFO | epoch: 18 (step: 41050) | train loss: 0.000133 | lr: 0.000100\n",
            "2025-04-01 03:00:20 | INFO | epoch: 18 (step: 41100) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 03:00:26 | INFO | epoch: 18 (step: 41150) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:00:32 | INFO | epoch: 18 (step: 41200) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:00:38 | INFO | epoch: 18 (step: 41250) | train loss: 0.000109 | lr: 0.000100\n",
            "2025-04-01 03:00:44 | INFO | epoch: 18 (step: 41300) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:00:51 | INFO | epoch: 18 (step: 41350) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:00:57 | INFO | epoch: 18 (step: 41400) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:01:03 | INFO | epoch: 18 (step: 41450) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:01:09 | INFO | epoch: 18 (step: 41500) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:01:15 | INFO | epoch: 18 (step: 41550) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:01:21 | INFO | epoch: 18 (step: 41600) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 03:01:27 | INFO | epoch: 18 (step: 41650) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:01:33 | INFO | epoch: 18 (step: 41700) | train loss: 0.000142 | lr: 0.000100\n",
            "2025-04-01 03:01:39 | INFO | epoch: 18 (step: 41750) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:01:45 | INFO | epoch: 18 (step: 41800) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 03:01:51 | INFO | epoch: 18 (step: 41850) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 03:01:57 | INFO | epoch: 18 (step: 41900) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:02:05 | INFO | epoch: 19 (step: 41950) | train loss: 0.000107 | lr: 0.000100\n",
            "2025-04-01 03:02:11 | INFO | epoch: 19 (step: 42000) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:02:17 | INFO | epoch: 19 (step: 42050) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:02:23 | INFO | epoch: 19 (step: 42100) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:02:30 | INFO | epoch: 19 (step: 42150) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:02:36 | INFO | epoch: 19 (step: 42200) | train loss: 0.000168 | lr: 0.000100\n",
            "2025-04-01 03:02:42 | INFO | epoch: 19 (step: 42250) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:02:48 | INFO | epoch: 19 (step: 42300) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 03:02:54 | INFO | epoch: 19 (step: 42350) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:03:00 | INFO | epoch: 19 (step: 42400) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 03:03:06 | INFO | epoch: 19 (step: 42450) | train loss: 0.000114 | lr: 0.000100\n",
            "2025-04-01 03:03:12 | INFO | epoch: 19 (step: 42500) | train loss: 0.000117 | lr: 0.000100\n",
            "2025-04-01 03:03:18 | INFO | epoch: 19 (step: 42550) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 03:03:24 | INFO | epoch: 19 (step: 42600) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:03:30 | INFO | epoch: 19 (step: 42650) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 03:03:36 | INFO | epoch: 19 (step: 42700) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 03:03:42 | INFO | epoch: 19 (step: 42750) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 03:03:48 | INFO | epoch: 19 (step: 42800) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:03:54 | INFO | epoch: 19 (step: 42850) | train loss: 0.000128 | lr: 0.000100\n",
            "2025-04-01 03:04:00 | INFO | epoch: 19 (step: 42900) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:04:07 | INFO | epoch: 19 (step: 42950) | train loss: 0.000109 | lr: 0.000100\n",
            "2025-04-01 03:04:13 | INFO | epoch: 19 (step: 43000) | train loss: 0.000114 | lr: 0.000100\n",
            "2025-04-01 03:04:18 | INFO | epoch: 19 (step: 43050) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:04:25 | INFO | epoch: 19 (step: 43100) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:04:31 | INFO | epoch: 19 (step: 43150) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:04:37 | INFO | epoch: 19 (step: 43200) | train loss: 0.000163 | lr: 0.000100\n",
            "2025-04-01 03:04:43 | INFO | epoch: 19 (step: 43250) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 03:04:49 | INFO | epoch: 19 (step: 43300) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 03:04:55 | INFO | epoch: 19 (step: 43350) | train loss: 0.000130 | lr: 0.000100\n",
            "2025-04-01 03:05:01 | INFO | epoch: 19 (step: 43400) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:05:07 | INFO | epoch: 19 (step: 43450) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:05:14 | INFO | epoch: 19 (step: 43500) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:05:20 | INFO | epoch: 19 (step: 43550) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 03:05:26 | INFO | epoch: 19 (step: 43600) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:05:32 | INFO | epoch: 19 (step: 43650) | train loss: 0.000157 | lr: 0.000100\n",
            "2025-04-01 03:05:38 | INFO | epoch: 19 (step: 43700) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:05:44 | INFO | epoch: 19 (step: 43750) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:05:50 | INFO | epoch: 19 (step: 43800) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:05:56 | INFO | epoch: 19 (step: 43850) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:06:02 | INFO | epoch: 19 (step: 43900) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 03:06:09 | INFO | epoch: 19 (step: 43950) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:06:14 | INFO | epoch: 19 (step: 44000) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:06:21 | INFO | epoch: 19 (step: 44050) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:06:27 | INFO | epoch: 19 (step: 44100) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:06:33 | INFO | epoch: 19 (step: 44150) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 03:06:39 | INFO | epoch: 19 (step: 44200) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:06:45 | INFO | epoch: 19 (step: 44250) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:06:53 | INFO | epoch: 20 (step: 44300) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:06:59 | INFO | epoch: 20 (step: 44350) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 03:07:05 | INFO | epoch: 20 (step: 44400) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 03:07:11 | INFO | epoch: 20 (step: 44450) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:07:18 | INFO | epoch: 20 (step: 44500) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:07:24 | INFO | epoch: 20 (step: 44550) | train loss: 0.000106 | lr: 0.000100\n",
            "2025-04-01 03:07:30 | INFO | epoch: 20 (step: 44600) | train loss: 0.000059 | lr: 0.000100\n",
            "2025-04-01 03:07:36 | INFO | epoch: 20 (step: 44650) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:07:42 | INFO | epoch: 20 (step: 44700) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:07:48 | INFO | epoch: 20 (step: 44750) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:07:55 | INFO | epoch: 20 (step: 44800) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:08:01 | INFO | epoch: 20 (step: 44850) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:08:07 | INFO | epoch: 20 (step: 44900) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 03:08:13 | INFO | epoch: 20 (step: 44950) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 03:08:19 | INFO | epoch: 20 (step: 45000) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:08:26 | INFO | epoch: 20 (step: 45000) | valid_loss: 0.002282 | valid_loss: 0.002282\n",
            "2025-04-01 03:08:35 | INFO | epoch: 20 (step: 45000) | best metric updated (loss) - 0.002282\n",
            "2025-04-01 03:08:38 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 03:08:44 | INFO | epoch: 20 (step: 45050) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 03:08:50 | INFO | epoch: 20 (step: 45100) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:08:56 | INFO | epoch: 20 (step: 45150) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:09:03 | INFO | epoch: 20 (step: 45200) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:09:08 | INFO | epoch: 20 (step: 45250) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:09:15 | INFO | epoch: 20 (step: 45300) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 03:09:21 | INFO | epoch: 20 (step: 45350) | train loss: 0.000128 | lr: 0.000100\n",
            "2025-04-01 03:09:27 | INFO | epoch: 20 (step: 45400) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:09:33 | INFO | epoch: 20 (step: 45450) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:09:39 | INFO | epoch: 20 (step: 45500) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:09:45 | INFO | epoch: 20 (step: 45550) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:09:51 | INFO | epoch: 20 (step: 45600) | train loss: 0.000142 | lr: 0.000100\n",
            "2025-04-01 03:09:57 | INFO | epoch: 20 (step: 45650) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 03:10:04 | INFO | epoch: 20 (step: 45700) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:10:10 | INFO | epoch: 20 (step: 45750) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:10:16 | INFO | epoch: 20 (step: 45800) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:10:22 | INFO | epoch: 20 (step: 45850) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:10:28 | INFO | epoch: 20 (step: 45900) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:10:34 | INFO | epoch: 20 (step: 45950) | train loss: 0.000107 | lr: 0.000100\n",
            "2025-04-01 03:10:40 | INFO | epoch: 20 (step: 46000) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:10:46 | INFO | epoch: 20 (step: 46050) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 03:10:52 | INFO | epoch: 20 (step: 46100) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:10:58 | INFO | epoch: 20 (step: 46150) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:11:04 | INFO | epoch: 20 (step: 46200) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:11:11 | INFO | epoch: 20 (step: 46250) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:11:16 | INFO | epoch: 20 (step: 46300) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:11:22 | INFO | epoch: 20 (step: 46350) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:11:29 | INFO | epoch: 20 (step: 46400) | train loss: 0.000122 | lr: 0.000100\n",
            "2025-04-01 03:11:35 | INFO | epoch: 20 (step: 46450) | train loss: 0.000155 | lr: 0.000100\n",
            "2025-04-01 03:11:40 | INFO | epoch: 20 (step: 46500) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:11:46 | INFO | epoch: 20 (step: 46550) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:11:54 | INFO | epoch: 21 (step: 46600) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:12:00 | INFO | epoch: 21 (step: 46650) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:12:06 | INFO | epoch: 21 (step: 46700) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:12:12 | INFO | epoch: 21 (step: 46750) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:12:18 | INFO | epoch: 21 (step: 46800) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:12:24 | INFO | epoch: 21 (step: 46850) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 03:12:30 | INFO | epoch: 21 (step: 46900) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:12:36 | INFO | epoch: 21 (step: 46950) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 03:12:42 | INFO | epoch: 21 (step: 47000) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:12:48 | INFO | epoch: 21 (step: 47050) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:12:55 | INFO | epoch: 21 (step: 47100) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:13:01 | INFO | epoch: 21 (step: 47150) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 03:13:07 | INFO | epoch: 21 (step: 47200) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:13:13 | INFO | epoch: 21 (step: 47250) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:13:19 | INFO | epoch: 21 (step: 47300) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 03:13:25 | INFO | epoch: 21 (step: 47350) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:13:31 | INFO | epoch: 21 (step: 47400) | train loss: 0.000166 | lr: 0.000100\n",
            "2025-04-01 03:13:37 | INFO | epoch: 21 (step: 47450) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 03:13:43 | INFO | epoch: 21 (step: 47500) | train loss: 0.000128 | lr: 0.000100\n",
            "2025-04-01 03:13:49 | INFO | epoch: 21 (step: 47550) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:13:55 | INFO | epoch: 21 (step: 47600) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:14:01 | INFO | epoch: 21 (step: 47650) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:14:07 | INFO | epoch: 21 (step: 47700) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:14:13 | INFO | epoch: 21 (step: 47750) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:14:19 | INFO | epoch: 21 (step: 47800) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:14:25 | INFO | epoch: 21 (step: 47850) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:14:31 | INFO | epoch: 21 (step: 47900) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:14:37 | INFO | epoch: 21 (step: 47950) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:14:43 | INFO | epoch: 21 (step: 48000) | train loss: 0.000083 | lr: 0.000100\n",
            "2025-04-01 03:14:49 | INFO | epoch: 21 (step: 48050) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:14:56 | INFO | epoch: 21 (step: 48100) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:15:02 | INFO | epoch: 21 (step: 48150) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:15:08 | INFO | epoch: 21 (step: 48200) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 03:15:14 | INFO | epoch: 21 (step: 48250) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:15:20 | INFO | epoch: 21 (step: 48300) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 03:15:26 | INFO | epoch: 21 (step: 48350) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 03:15:32 | INFO | epoch: 21 (step: 48400) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:15:38 | INFO | epoch: 21 (step: 48450) | train loss: 0.000171 | lr: 0.000100\n",
            "2025-04-01 03:15:44 | INFO | epoch: 21 (step: 48500) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 03:15:51 | INFO | epoch: 21 (step: 48550) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:15:57 | INFO | epoch: 21 (step: 48600) | train loss: 0.000121 | lr: 0.000100\n",
            "2025-04-01 03:16:03 | INFO | epoch: 21 (step: 48650) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:16:09 | INFO | epoch: 21 (step: 48700) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:16:15 | INFO | epoch: 21 (step: 48750) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:16:21 | INFO | epoch: 21 (step: 48800) | train loss: 0.000107 | lr: 0.000100\n",
            "2025-04-01 03:16:27 | INFO | epoch: 21 (step: 48850) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:16:33 | INFO | epoch: 21 (step: 48900) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:16:41 | INFO | epoch: 22 (step: 48950) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 03:16:47 | INFO | epoch: 22 (step: 49000) | train loss: 0.000121 | lr: 0.000100\n",
            "2025-04-01 03:16:53 | INFO | epoch: 22 (step: 49050) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:16:59 | INFO | epoch: 22 (step: 49100) | train loss: 0.000076 | lr: 0.000100\n",
            "2025-04-01 03:17:06 | INFO | epoch: 22 (step: 49150) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:17:12 | INFO | epoch: 22 (step: 49200) | train loss: 0.000127 | lr: 0.000100\n",
            "2025-04-01 03:17:18 | INFO | epoch: 22 (step: 49250) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 03:17:24 | INFO | epoch: 22 (step: 49300) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:17:30 | INFO | epoch: 22 (step: 49350) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:17:36 | INFO | epoch: 22 (step: 49400) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 03:17:42 | INFO | epoch: 22 (step: 49450) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:17:48 | INFO | epoch: 22 (step: 49500) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:17:54 | INFO | epoch: 22 (step: 49550) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:18:00 | INFO | epoch: 22 (step: 49600) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 03:18:06 | INFO | epoch: 22 (step: 49650) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:18:12 | INFO | epoch: 22 (step: 49700) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 03:18:19 | INFO | epoch: 22 (step: 49750) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:18:25 | INFO | epoch: 22 (step: 49800) | train loss: 0.000076 | lr: 0.000100\n",
            "2025-04-01 03:18:31 | INFO | epoch: 22 (step: 49850) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:18:37 | INFO | epoch: 22 (step: 49900) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:18:43 | INFO | epoch: 22 (step: 49950) | train loss: 0.000145 | lr: 0.000100\n",
            "2025-04-01 03:18:49 | INFO | epoch: 22 (step: 50000) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:18:56 | INFO | epoch: 22 (step: 50000) | valid_loss: 0.002501 | valid_loss: 0.002501\n",
            "2025-04-01 03:19:00 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 03:19:06 | INFO | epoch: 22 (step: 50050) | train loss: 0.000102 | lr: 0.000100\n",
            "2025-04-01 03:19:12 | INFO | epoch: 22 (step: 50100) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:19:19 | INFO | epoch: 22 (step: 50150) | train loss: 0.000095 | lr: 0.000100\n",
            "2025-04-01 03:19:25 | INFO | epoch: 22 (step: 50200) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:19:31 | INFO | epoch: 22 (step: 50250) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 03:19:37 | INFO | epoch: 22 (step: 50300) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:19:43 | INFO | epoch: 22 (step: 50350) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:19:49 | INFO | epoch: 22 (step: 50400) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:19:55 | INFO | epoch: 22 (step: 50450) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:20:01 | INFO | epoch: 22 (step: 50500) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:20:07 | INFO | epoch: 22 (step: 50550) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:20:13 | INFO | epoch: 22 (step: 50600) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:20:19 | INFO | epoch: 22 (step: 50650) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:20:25 | INFO | epoch: 22 (step: 50700) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:20:31 | INFO | epoch: 22 (step: 50750) | train loss: 0.000117 | lr: 0.000100\n",
            "2025-04-01 03:20:37 | INFO | epoch: 22 (step: 50800) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:20:43 | INFO | epoch: 22 (step: 50850) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:20:49 | INFO | epoch: 22 (step: 50900) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:20:55 | INFO | epoch: 22 (step: 50950) | train loss: 0.000146 | lr: 0.000100\n",
            "2025-04-01 03:21:01 | INFO | epoch: 22 (step: 51000) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:21:07 | INFO | epoch: 22 (step: 51050) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 03:21:13 | INFO | epoch: 22 (step: 51100) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:21:19 | INFO | epoch: 22 (step: 51150) | train loss: 0.000150 | lr: 0.000100\n",
            "2025-04-01 03:21:25 | INFO | epoch: 22 (step: 51200) | train loss: 0.000290 | lr: 0.000100\n",
            "2025-04-01 03:21:33 | INFO | epoch: 23 (step: 51250) | train loss: 0.000106 | lr: 0.000100\n",
            "2025-04-01 03:21:39 | INFO | epoch: 23 (step: 51300) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:21:45 | INFO | epoch: 23 (step: 51350) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 03:21:51 | INFO | epoch: 23 (step: 51400) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:21:57 | INFO | epoch: 23 (step: 51450) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:22:03 | INFO | epoch: 23 (step: 51500) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:22:09 | INFO | epoch: 23 (step: 51550) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:22:15 | INFO | epoch: 23 (step: 51600) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:22:21 | INFO | epoch: 23 (step: 51650) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:22:28 | INFO | epoch: 23 (step: 51700) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:22:34 | INFO | epoch: 23 (step: 51750) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:22:40 | INFO | epoch: 23 (step: 51800) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:22:46 | INFO | epoch: 23 (step: 51850) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:22:51 | INFO | epoch: 23 (step: 51900) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:22:58 | INFO | epoch: 23 (step: 51950) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:23:04 | INFO | epoch: 23 (step: 52000) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:23:10 | INFO | epoch: 23 (step: 52050) | train loss: 0.000125 | lr: 0.000100\n",
            "2025-04-01 03:23:16 | INFO | epoch: 23 (step: 52100) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:23:22 | INFO | epoch: 23 (step: 52150) | train loss: 0.000102 | lr: 0.000100\n",
            "2025-04-01 03:23:28 | INFO | epoch: 23 (step: 52200) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:23:34 | INFO | epoch: 23 (step: 52250) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:23:40 | INFO | epoch: 23 (step: 52300) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:23:46 | INFO | epoch: 23 (step: 52350) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:23:52 | INFO | epoch: 23 (step: 52400) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 03:23:59 | INFO | epoch: 23 (step: 52450) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 03:24:05 | INFO | epoch: 23 (step: 52500) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 03:24:11 | INFO | epoch: 23 (step: 52550) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:24:17 | INFO | epoch: 23 (step: 52600) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:24:23 | INFO | epoch: 23 (step: 52650) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:24:29 | INFO | epoch: 23 (step: 52700) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:24:35 | INFO | epoch: 23 (step: 52750) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:24:40 | INFO | epoch: 23 (step: 52800) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 03:24:46 | INFO | epoch: 23 (step: 52850) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 03:24:53 | INFO | epoch: 23 (step: 52900) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:24:59 | INFO | epoch: 23 (step: 52950) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:25:05 | INFO | epoch: 23 (step: 53000) | train loss: 0.000083 | lr: 0.000100\n",
            "2025-04-01 03:25:11 | INFO | epoch: 23 (step: 53050) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:25:17 | INFO | epoch: 23 (step: 53100) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 03:25:23 | INFO | epoch: 23 (step: 53150) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:25:29 | INFO | epoch: 23 (step: 53200) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:25:35 | INFO | epoch: 23 (step: 53250) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 03:25:41 | INFO | epoch: 23 (step: 53300) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:25:47 | INFO | epoch: 23 (step: 53350) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:25:53 | INFO | epoch: 23 (step: 53400) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:26:00 | INFO | epoch: 23 (step: 53450) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:26:06 | INFO | epoch: 23 (step: 53500) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:26:12 | INFO | epoch: 23 (step: 53550) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:26:20 | INFO | epoch: 24 (step: 53600) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:26:26 | INFO | epoch: 24 (step: 53650) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:26:32 | INFO | epoch: 24 (step: 53700) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:26:38 | INFO | epoch: 24 (step: 53750) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 03:26:44 | INFO | epoch: 24 (step: 53800) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:26:50 | INFO | epoch: 24 (step: 53850) | train loss: 0.000076 | lr: 0.000100\n",
            "2025-04-01 03:26:57 | INFO | epoch: 24 (step: 53900) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:27:03 | INFO | epoch: 24 (step: 53950) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:27:09 | INFO | epoch: 24 (step: 54000) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:27:15 | INFO | epoch: 24 (step: 54050) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 03:27:21 | INFO | epoch: 24 (step: 54100) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:27:27 | INFO | epoch: 24 (step: 54150) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:27:33 | INFO | epoch: 24 (step: 54200) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 03:27:39 | INFO | epoch: 24 (step: 54250) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:27:45 | INFO | epoch: 24 (step: 54300) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:27:51 | INFO | epoch: 24 (step: 54350) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:27:57 | INFO | epoch: 24 (step: 54400) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:28:03 | INFO | epoch: 24 (step: 54450) | train loss: 0.000094 | lr: 0.000100\n",
            "2025-04-01 03:28:09 | INFO | epoch: 24 (step: 54500) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:28:15 | INFO | epoch: 24 (step: 54550) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:28:21 | INFO | epoch: 24 (step: 54600) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 03:28:27 | INFO | epoch: 24 (step: 54650) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:28:33 | INFO | epoch: 24 (step: 54700) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 03:28:40 | INFO | epoch: 24 (step: 54750) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:28:46 | INFO | epoch: 24 (step: 54800) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:28:52 | INFO | epoch: 24 (step: 54850) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:28:58 | INFO | epoch: 24 (step: 54900) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:29:04 | INFO | epoch: 24 (step: 54950) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:29:10 | INFO | epoch: 24 (step: 55000) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:29:18 | INFO | epoch: 24 (step: 55000) | valid_loss: 0.002354 | valid_loss: 0.002354\n",
            "2025-04-01 03:29:23 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 03:29:29 | INFO | epoch: 24 (step: 55050) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:29:35 | INFO | epoch: 24 (step: 55100) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:29:41 | INFO | epoch: 24 (step: 55150) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:29:47 | INFO | epoch: 24 (step: 55200) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:29:53 | INFO | epoch: 24 (step: 55250) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:29:59 | INFO | epoch: 24 (step: 55300) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:30:05 | INFO | epoch: 24 (step: 55350) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:30:11 | INFO | epoch: 24 (step: 55400) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:30:17 | INFO | epoch: 24 (step: 55450) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:30:24 | INFO | epoch: 24 (step: 55500) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:30:30 | INFO | epoch: 24 (step: 55550) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 03:30:36 | INFO | epoch: 24 (step: 55600) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:30:42 | INFO | epoch: 24 (step: 55650) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:30:48 | INFO | epoch: 24 (step: 55700) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:30:54 | INFO | epoch: 24 (step: 55750) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 03:31:00 | INFO | epoch: 24 (step: 55800) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:31:06 | INFO | epoch: 24 (step: 55850) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:31:14 | INFO | epoch: 25 (step: 55900) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:31:20 | INFO | epoch: 25 (step: 55950) | train loss: 0.000090 | lr: 0.000100\n",
            "2025-04-01 03:31:26 | INFO | epoch: 25 (step: 56000) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:31:32 | INFO | epoch: 25 (step: 56050) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 03:31:38 | INFO | epoch: 25 (step: 56100) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:31:44 | INFO | epoch: 25 (step: 56150) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 03:31:50 | INFO | epoch: 25 (step: 56200) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 03:31:56 | INFO | epoch: 25 (step: 56250) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:32:02 | INFO | epoch: 25 (step: 56300) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:32:09 | INFO | epoch: 25 (step: 56350) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:32:15 | INFO | epoch: 25 (step: 56400) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:32:21 | INFO | epoch: 25 (step: 56450) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 03:32:27 | INFO | epoch: 25 (step: 56500) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 03:32:33 | INFO | epoch: 25 (step: 56550) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:32:39 | INFO | epoch: 25 (step: 56600) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:32:45 | INFO | epoch: 25 (step: 56650) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:32:51 | INFO | epoch: 25 (step: 56700) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:32:57 | INFO | epoch: 25 (step: 56750) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:33:03 | INFO | epoch: 25 (step: 56800) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:33:09 | INFO | epoch: 25 (step: 56850) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 03:33:15 | INFO | epoch: 25 (step: 56900) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 03:33:21 | INFO | epoch: 25 (step: 56950) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:33:28 | INFO | epoch: 25 (step: 57000) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:33:34 | INFO | epoch: 25 (step: 57050) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:33:40 | INFO | epoch: 25 (step: 57100) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 03:33:46 | INFO | epoch: 25 (step: 57150) | train loss: 0.000094 | lr: 0.000100\n",
            "2025-04-01 03:33:52 | INFO | epoch: 25 (step: 57200) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 03:33:58 | INFO | epoch: 25 (step: 57250) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:34:04 | INFO | epoch: 25 (step: 57300) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:34:10 | INFO | epoch: 25 (step: 57350) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 03:34:16 | INFO | epoch: 25 (step: 57400) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:34:22 | INFO | epoch: 25 (step: 57450) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:34:28 | INFO | epoch: 25 (step: 57500) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:34:35 | INFO | epoch: 25 (step: 57550) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:34:41 | INFO | epoch: 25 (step: 57600) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:34:47 | INFO | epoch: 25 (step: 57650) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:34:53 | INFO | epoch: 25 (step: 57700) | train loss: 0.000182 | lr: 0.000100\n",
            "2025-04-01 03:34:59 | INFO | epoch: 25 (step: 57750) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:35:05 | INFO | epoch: 25 (step: 57800) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 03:35:11 | INFO | epoch: 25 (step: 57850) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:35:17 | INFO | epoch: 25 (step: 57900) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:35:23 | INFO | epoch: 25 (step: 57950) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:35:30 | INFO | epoch: 25 (step: 58000) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:35:36 | INFO | epoch: 25 (step: 58050) | train loss: 0.000206 | lr: 0.000100\n",
            "2025-04-01 03:35:42 | INFO | epoch: 25 (step: 58100) | train loss: 0.000137 | lr: 0.000100\n",
            "2025-04-01 03:35:48 | INFO | epoch: 25 (step: 58150) | train loss: 0.000225 | lr: 0.000100\n",
            "2025-04-01 03:35:54 | INFO | epoch: 25 (step: 58200) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:36:02 | INFO | epoch: 26 (step: 58250) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:36:08 | INFO | epoch: 26 (step: 58300) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:36:14 | INFO | epoch: 26 (step: 58350) | train loss: 0.000114 | lr: 0.000100\n",
            "2025-04-01 03:36:20 | INFO | epoch: 26 (step: 58400) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:36:26 | INFO | epoch: 26 (step: 58450) | train loss: 0.000111 | lr: 0.000100\n",
            "2025-04-01 03:36:33 | INFO | epoch: 26 (step: 58500) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:36:39 | INFO | epoch: 26 (step: 58550) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 03:36:45 | INFO | epoch: 26 (step: 58600) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:36:51 | INFO | epoch: 26 (step: 58650) | train loss: 0.000165 | lr: 0.000100\n",
            "2025-04-01 03:36:57 | INFO | epoch: 26 (step: 58700) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:37:03 | INFO | epoch: 26 (step: 58750) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:37:09 | INFO | epoch: 26 (step: 58800) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 03:37:15 | INFO | epoch: 26 (step: 58850) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:37:21 | INFO | epoch: 26 (step: 58900) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:37:27 | INFO | epoch: 26 (step: 58950) | train loss: 0.000112 | lr: 0.000100\n",
            "2025-04-01 03:37:33 | INFO | epoch: 26 (step: 59000) | train loss: 0.000100 | lr: 0.000100\n",
            "2025-04-01 03:37:39 | INFO | epoch: 26 (step: 59050) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:37:45 | INFO | epoch: 26 (step: 59100) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:37:51 | INFO | epoch: 26 (step: 59150) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:37:57 | INFO | epoch: 26 (step: 59200) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:38:04 | INFO | epoch: 26 (step: 59250) | train loss: 0.000119 | lr: 0.000100\n",
            "2025-04-01 03:38:10 | INFO | epoch: 26 (step: 59300) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 03:38:16 | INFO | epoch: 26 (step: 59350) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:38:22 | INFO | epoch: 26 (step: 59400) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:38:28 | INFO | epoch: 26 (step: 59450) | train loss: 0.000147 | lr: 0.000100\n",
            "2025-04-01 03:38:34 | INFO | epoch: 26 (step: 59500) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 03:38:40 | INFO | epoch: 26 (step: 59550) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 03:38:46 | INFO | epoch: 26 (step: 59600) | train loss: 0.000131 | lr: 0.000100\n",
            "2025-04-01 03:38:53 | INFO | epoch: 26 (step: 59650) | train loss: 0.000149 | lr: 0.000100\n",
            "2025-04-01 03:38:59 | INFO | epoch: 26 (step: 59700) | train loss: 0.000135 | lr: 0.000100\n",
            "2025-04-01 03:39:05 | INFO | epoch: 26 (step: 59750) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:39:11 | INFO | epoch: 26 (step: 59800) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 03:39:17 | INFO | epoch: 26 (step: 59850) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:39:22 | INFO | epoch: 26 (step: 59900) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:39:28 | INFO | epoch: 26 (step: 59950) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:39:35 | INFO | epoch: 26 (step: 60000) | train loss: 0.000151 | lr: 0.000100\n",
            "2025-04-01 03:39:42 | INFO | epoch: 26 (step: 60000) | valid_loss: 0.002878 | valid_loss: 0.002878\n",
            "2025-04-01 03:39:46 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 03:39:52 | INFO | epoch: 26 (step: 60050) | train loss: 0.000106 | lr: 0.000100\n",
            "2025-04-01 03:39:58 | INFO | epoch: 26 (step: 60100) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:40:04 | INFO | epoch: 26 (step: 60150) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:40:10 | INFO | epoch: 26 (step: 60200) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 03:40:16 | INFO | epoch: 26 (step: 60250) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:40:22 | INFO | epoch: 26 (step: 60300) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 03:40:28 | INFO | epoch: 26 (step: 60350) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:40:34 | INFO | epoch: 26 (step: 60400) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:40:40 | INFO | epoch: 26 (step: 60450) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:40:46 | INFO | epoch: 26 (step: 60500) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 03:40:53 | INFO | epoch: 26 (step: 60550) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:41:01 | INFO | epoch: 27 (step: 60600) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 03:41:07 | INFO | epoch: 27 (step: 60650) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:41:13 | INFO | epoch: 27 (step: 60700) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:41:19 | INFO | epoch: 27 (step: 60750) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:41:25 | INFO | epoch: 27 (step: 60800) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:41:31 | INFO | epoch: 27 (step: 60850) | train loss: 0.000115 | lr: 0.000100\n",
            "2025-04-01 03:41:37 | INFO | epoch: 27 (step: 60900) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 03:41:43 | INFO | epoch: 27 (step: 60950) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:41:49 | INFO | epoch: 27 (step: 61000) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 03:41:55 | INFO | epoch: 27 (step: 61050) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:42:01 | INFO | epoch: 27 (step: 61100) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:42:07 | INFO | epoch: 27 (step: 61150) | train loss: 0.000129 | lr: 0.000100\n",
            "2025-04-01 03:42:13 | INFO | epoch: 27 (step: 61200) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:42:19 | INFO | epoch: 27 (step: 61250) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:42:26 | INFO | epoch: 27 (step: 61300) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 03:42:32 | INFO | epoch: 27 (step: 61350) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:42:38 | INFO | epoch: 27 (step: 61400) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:42:44 | INFO | epoch: 27 (step: 61450) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:42:50 | INFO | epoch: 27 (step: 61500) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:42:56 | INFO | epoch: 27 (step: 61550) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:43:02 | INFO | epoch: 27 (step: 61600) | train loss: 0.000125 | lr: 0.000100\n",
            "2025-04-01 03:43:08 | INFO | epoch: 27 (step: 61650) | train loss: 0.000145 | lr: 0.000100\n",
            "2025-04-01 03:43:14 | INFO | epoch: 27 (step: 61700) | train loss: 0.000116 | lr: 0.000100\n",
            "2025-04-01 03:43:20 | INFO | epoch: 27 (step: 61750) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:43:27 | INFO | epoch: 27 (step: 61800) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 03:43:33 | INFO | epoch: 27 (step: 61850) | train loss: 0.000087 | lr: 0.000100\n",
            "2025-04-01 03:43:38 | INFO | epoch: 27 (step: 61900) | train loss: 0.000107 | lr: 0.000100\n",
            "2025-04-01 03:43:44 | INFO | epoch: 27 (step: 61950) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:43:50 | INFO | epoch: 27 (step: 62000) | train loss: 0.000102 | lr: 0.000100\n",
            "2025-04-01 03:43:57 | INFO | epoch: 27 (step: 62050) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:44:03 | INFO | epoch: 27 (step: 62100) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:44:09 | INFO | epoch: 27 (step: 62150) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 03:44:15 | INFO | epoch: 27 (step: 62200) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 03:44:21 | INFO | epoch: 27 (step: 62250) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:44:27 | INFO | epoch: 27 (step: 62300) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:44:33 | INFO | epoch: 27 (step: 62350) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:44:39 | INFO | epoch: 27 (step: 62400) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:44:45 | INFO | epoch: 27 (step: 62450) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 03:44:51 | INFO | epoch: 27 (step: 62500) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:44:57 | INFO | epoch: 27 (step: 62550) | train loss: 0.000144 | lr: 0.000100\n",
            "2025-04-01 03:45:03 | INFO | epoch: 27 (step: 62600) | train loss: 0.000094 | lr: 0.000100\n",
            "2025-04-01 03:45:10 | INFO | epoch: 27 (step: 62650) | train loss: 0.000143 | lr: 0.000100\n",
            "2025-04-01 03:45:16 | INFO | epoch: 27 (step: 62700) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:45:22 | INFO | epoch: 27 (step: 62750) | train loss: 0.000134 | lr: 0.000100\n",
            "2025-04-01 03:45:28 | INFO | epoch: 27 (step: 62800) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:45:34 | INFO | epoch: 27 (step: 62850) | train loss: 0.000110 | lr: 0.000100\n",
            "2025-04-01 03:45:41 | INFO | epoch: 28 (step: 62900) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:45:48 | INFO | epoch: 28 (step: 62950) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 03:45:54 | INFO | epoch: 28 (step: 63000) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:46:00 | INFO | epoch: 28 (step: 63050) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:46:06 | INFO | epoch: 28 (step: 63100) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 03:46:12 | INFO | epoch: 28 (step: 63150) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 03:46:18 | INFO | epoch: 28 (step: 63200) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:46:24 | INFO | epoch: 28 (step: 63250) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 03:46:30 | INFO | epoch: 28 (step: 63300) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:46:36 | INFO | epoch: 28 (step: 63350) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:46:42 | INFO | epoch: 28 (step: 63400) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 03:46:49 | INFO | epoch: 28 (step: 63450) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 03:46:55 | INFO | epoch: 28 (step: 63500) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:47:01 | INFO | epoch: 28 (step: 63550) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 03:47:07 | INFO | epoch: 28 (step: 63600) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 03:47:13 | INFO | epoch: 28 (step: 63650) | train loss: 0.000138 | lr: 0.000100\n",
            "2025-04-01 03:47:19 | INFO | epoch: 28 (step: 63700) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:47:25 | INFO | epoch: 28 (step: 63750) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 03:47:31 | INFO | epoch: 28 (step: 63800) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:47:37 | INFO | epoch: 28 (step: 63850) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:47:44 | INFO | epoch: 28 (step: 63900) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 03:47:50 | INFO | epoch: 28 (step: 63950) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:47:56 | INFO | epoch: 28 (step: 64000) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:48:02 | INFO | epoch: 28 (step: 64050) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:48:08 | INFO | epoch: 28 (step: 64100) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 03:48:14 | INFO | epoch: 28 (step: 64150) | train loss: 0.000087 | lr: 0.000100\n",
            "2025-04-01 03:48:20 | INFO | epoch: 28 (step: 64200) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:48:27 | INFO | epoch: 28 (step: 64250) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 03:48:33 | INFO | epoch: 28 (step: 64300) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 03:48:38 | INFO | epoch: 28 (step: 64350) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:48:45 | INFO | epoch: 28 (step: 64400) | train loss: 0.000139 | lr: 0.000100\n",
            "2025-04-01 03:48:51 | INFO | epoch: 28 (step: 64450) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 03:48:57 | INFO | epoch: 28 (step: 64500) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 03:49:03 | INFO | epoch: 28 (step: 64550) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 03:49:09 | INFO | epoch: 28 (step: 64600) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:49:15 | INFO | epoch: 28 (step: 64650) | train loss: 0.000083 | lr: 0.000100\n",
            "2025-04-01 03:49:21 | INFO | epoch: 28 (step: 64700) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:49:27 | INFO | epoch: 28 (step: 64750) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:49:33 | INFO | epoch: 28 (step: 64800) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 03:49:39 | INFO | epoch: 28 (step: 64850) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:49:45 | INFO | epoch: 28 (step: 64900) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 03:49:51 | INFO | epoch: 28 (step: 64950) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 03:49:57 | INFO | epoch: 28 (step: 65000) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 03:50:04 | INFO | epoch: 28 (step: 65000) | valid_loss: 0.001959 | valid_loss: 0.001959\n",
            "2025-04-01 03:50:09 | INFO | epoch: 28 (step: 65000) | best metric updated (loss) - 0.001959\n",
            "2025-04-01 03:50:12 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 03:50:18 | INFO | epoch: 28 (step: 65050) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 03:50:24 | INFO | epoch: 28 (step: 65100) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 03:50:30 | INFO | epoch: 28 (step: 65150) | train loss: 0.000086 | lr: 0.000100\n",
            "2025-04-01 03:50:37 | INFO | epoch: 28 (step: 65200) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 03:50:44 | INFO | epoch: 29 (step: 65250) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:50:50 | INFO | epoch: 29 (step: 65300) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 03:50:56 | INFO | epoch: 29 (step: 65350) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:51:03 | INFO | epoch: 29 (step: 65400) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 03:51:09 | INFO | epoch: 29 (step: 65450) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 03:51:15 | INFO | epoch: 29 (step: 65500) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 03:51:21 | INFO | epoch: 29 (step: 65550) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 03:51:27 | INFO | epoch: 29 (step: 65600) | train loss: 0.000581 | lr: 0.000100\n",
            "2025-04-01 03:51:33 | INFO | epoch: 29 (step: 65650) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:51:39 | INFO | epoch: 29 (step: 65700) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:51:45 | INFO | epoch: 29 (step: 65750) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:51:51 | INFO | epoch: 29 (step: 65800) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:51:57 | INFO | epoch: 29 (step: 65850) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 03:52:04 | INFO | epoch: 29 (step: 65900) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 03:52:10 | INFO | epoch: 29 (step: 65950) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 03:52:16 | INFO | epoch: 29 (step: 66000) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 03:52:22 | INFO | epoch: 29 (step: 66050) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:52:28 | INFO | epoch: 29 (step: 66100) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 03:52:34 | INFO | epoch: 29 (step: 66150) | train loss: 0.000059 | lr: 0.000100\n",
            "2025-04-01 03:52:40 | INFO | epoch: 29 (step: 66200) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 03:52:46 | INFO | epoch: 29 (step: 66250) | train loss: 0.000101 | lr: 0.000100\n",
            "2025-04-01 03:52:52 | INFO | epoch: 29 (step: 66300) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 03:52:59 | INFO | epoch: 29 (step: 66350) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:53:05 | INFO | epoch: 29 (step: 66400) | train loss: 0.000124 | lr: 0.000100\n",
            "2025-04-01 03:53:11 | INFO | epoch: 29 (step: 66450) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:53:17 | INFO | epoch: 29 (step: 66500) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 03:53:23 | INFO | epoch: 29 (step: 66550) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:53:29 | INFO | epoch: 29 (step: 66600) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 03:53:35 | INFO | epoch: 29 (step: 66650) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 03:53:41 | INFO | epoch: 29 (step: 66700) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:53:47 | INFO | epoch: 29 (step: 66750) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:53:53 | INFO | epoch: 29 (step: 66800) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 03:53:59 | INFO | epoch: 29 (step: 66850) | train loss: 0.000103 | lr: 0.000100\n",
            "2025-04-01 03:54:05 | INFO | epoch: 29 (step: 66900) | train loss: 0.000108 | lr: 0.000100\n",
            "2025-04-01 03:54:12 | INFO | epoch: 29 (step: 66950) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 03:54:18 | INFO | epoch: 29 (step: 67000) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 03:54:24 | INFO | epoch: 29 (step: 67050) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 03:54:30 | INFO | epoch: 29 (step: 67100) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:54:36 | INFO | epoch: 29 (step: 67150) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:54:42 | INFO | epoch: 29 (step: 67200) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 03:54:48 | INFO | epoch: 29 (step: 67250) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:54:54 | INFO | epoch: 29 (step: 67300) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 03:55:00 | INFO | epoch: 29 (step: 67350) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 03:55:06 | INFO | epoch: 29 (step: 67400) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 03:55:12 | INFO | epoch: 29 (step: 67450) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 03:55:18 | INFO | epoch: 29 (step: 67500) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:55:26 | INFO | epoch: 30 (step: 67550) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 03:55:32 | INFO | epoch: 30 (step: 67600) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 03:55:38 | INFO | epoch: 30 (step: 67650) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:55:44 | INFO | epoch: 30 (step: 67700) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 03:55:50 | INFO | epoch: 30 (step: 67750) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 03:55:56 | INFO | epoch: 30 (step: 67800) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:56:02 | INFO | epoch: 30 (step: 67850) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 03:56:08 | INFO | epoch: 30 (step: 67900) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 03:56:14 | INFO | epoch: 30 (step: 67950) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 03:56:21 | INFO | epoch: 30 (step: 68000) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 03:56:27 | INFO | epoch: 30 (step: 68050) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 03:56:33 | INFO | epoch: 30 (step: 68100) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 03:56:39 | INFO | epoch: 30 (step: 68150) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 03:56:45 | INFO | epoch: 30 (step: 68200) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 03:56:51 | INFO | epoch: 30 (step: 68250) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:56:57 | INFO | epoch: 30 (step: 68300) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 03:57:03 | INFO | epoch: 30 (step: 68350) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 03:57:09 | INFO | epoch: 30 (step: 68400) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 03:57:15 | INFO | epoch: 30 (step: 68450) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 03:57:21 | INFO | epoch: 30 (step: 68500) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:57:27 | INFO | epoch: 30 (step: 68550) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 03:57:33 | INFO | epoch: 30 (step: 68600) | train loss: 0.000129 | lr: 0.000100\n",
            "2025-04-01 03:57:39 | INFO | epoch: 30 (step: 68650) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 03:57:45 | INFO | epoch: 30 (step: 68700) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 03:57:51 | INFO | epoch: 30 (step: 68750) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 03:57:57 | INFO | epoch: 30 (step: 68800) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 03:58:04 | INFO | epoch: 30 (step: 68850) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 03:58:10 | INFO | epoch: 30 (step: 68900) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:58:16 | INFO | epoch: 30 (step: 68950) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 03:58:22 | INFO | epoch: 30 (step: 69000) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 03:58:28 | INFO | epoch: 30 (step: 69050) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 03:58:34 | INFO | epoch: 30 (step: 69100) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 03:58:40 | INFO | epoch: 30 (step: 69150) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:58:46 | INFO | epoch: 30 (step: 69200) | train loss: 0.000059 | lr: 0.000100\n",
            "2025-04-01 03:58:52 | INFO | epoch: 30 (step: 69250) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 03:58:59 | INFO | epoch: 30 (step: 69300) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 03:59:05 | INFO | epoch: 30 (step: 69350) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 03:59:11 | INFO | epoch: 30 (step: 69400) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 03:59:17 | INFO | epoch: 30 (step: 69450) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 03:59:23 | INFO | epoch: 30 (step: 69500) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 03:59:30 | INFO | epoch: 30 (step: 69550) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 03:59:36 | INFO | epoch: 30 (step: 69600) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 03:59:42 | INFO | epoch: 30 (step: 69650) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 03:59:48 | INFO | epoch: 30 (step: 69700) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 03:59:54 | INFO | epoch: 30 (step: 69750) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 04:00:00 | INFO | epoch: 30 (step: 69800) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:00:06 | INFO | epoch: 30 (step: 69850) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:00:13 | INFO | epoch: 31 (step: 69900) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 04:00:20 | INFO | epoch: 31 (step: 69950) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:00:26 | INFO | epoch: 31 (step: 70000) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:00:33 | INFO | epoch: 31 (step: 70000) | valid_loss: 0.001883 | valid_loss: 0.001883\n",
            "2025-04-01 04:00:39 | INFO | epoch: 31 (step: 70000) | best metric updated (loss) - 0.001883\n",
            "2025-04-01 04:00:43 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:00:50 | INFO | epoch: 31 (step: 70050) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:00:55 | INFO | epoch: 31 (step: 70100) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:01:01 | INFO | epoch: 31 (step: 70150) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 04:01:07 | INFO | epoch: 31 (step: 70200) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:01:14 | INFO | epoch: 31 (step: 70250) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:01:20 | INFO | epoch: 31 (step: 70300) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 04:01:26 | INFO | epoch: 31 (step: 70350) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:01:32 | INFO | epoch: 31 (step: 70400) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:01:38 | INFO | epoch: 31 (step: 70450) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 04:01:44 | INFO | epoch: 31 (step: 70500) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 04:01:50 | INFO | epoch: 31 (step: 70550) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:01:56 | INFO | epoch: 31 (step: 70600) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:02:02 | INFO | epoch: 31 (step: 70650) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:02:08 | INFO | epoch: 31 (step: 70700) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:02:14 | INFO | epoch: 31 (step: 70750) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:02:20 | INFO | epoch: 31 (step: 70800) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 04:02:27 | INFO | epoch: 31 (step: 70850) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:02:33 | INFO | epoch: 31 (step: 70900) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:02:39 | INFO | epoch: 31 (step: 70950) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:02:45 | INFO | epoch: 31 (step: 71000) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 04:02:51 | INFO | epoch: 31 (step: 71050) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:02:57 | INFO | epoch: 31 (step: 71100) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 04:03:03 | INFO | epoch: 31 (step: 71150) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:03:09 | INFO | epoch: 31 (step: 71200) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:03:15 | INFO | epoch: 31 (step: 71250) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:03:21 | INFO | epoch: 31 (step: 71300) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 04:03:27 | INFO | epoch: 31 (step: 71350) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:03:33 | INFO | epoch: 31 (step: 71400) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 04:03:39 | INFO | epoch: 31 (step: 71450) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:03:45 | INFO | epoch: 31 (step: 71500) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 04:03:51 | INFO | epoch: 31 (step: 71550) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:03:57 | INFO | epoch: 31 (step: 71600) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:04:04 | INFO | epoch: 31 (step: 71650) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:04:10 | INFO | epoch: 31 (step: 71700) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:04:16 | INFO | epoch: 31 (step: 71750) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 04:04:22 | INFO | epoch: 31 (step: 71800) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:04:28 | INFO | epoch: 31 (step: 71850) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:04:34 | INFO | epoch: 31 (step: 71900) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:04:40 | INFO | epoch: 31 (step: 71950) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:04:46 | INFO | epoch: 31 (step: 72000) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:04:52 | INFO | epoch: 31 (step: 72050) | train loss: 0.000076 | lr: 0.000100\n",
            "2025-04-01 04:04:58 | INFO | epoch: 31 (step: 72100) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:05:04 | INFO | epoch: 31 (step: 72150) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:05:12 | INFO | epoch: 32 (step: 72200) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:05:18 | INFO | epoch: 32 (step: 72250) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:05:24 | INFO | epoch: 32 (step: 72300) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:05:30 | INFO | epoch: 32 (step: 72350) | train loss: 0.000111 | lr: 0.000100\n",
            "2025-04-01 04:05:36 | INFO | epoch: 32 (step: 72400) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 04:05:42 | INFO | epoch: 32 (step: 72450) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:05:48 | INFO | epoch: 32 (step: 72500) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:05:54 | INFO | epoch: 32 (step: 72550) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:06:00 | INFO | epoch: 32 (step: 72600) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 04:06:06 | INFO | epoch: 32 (step: 72650) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:06:12 | INFO | epoch: 32 (step: 72700) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:06:18 | INFO | epoch: 32 (step: 72750) | train loss: 0.000084 | lr: 0.000100\n",
            "2025-04-01 04:06:24 | INFO | epoch: 32 (step: 72800) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:06:31 | INFO | epoch: 32 (step: 72850) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:06:37 | INFO | epoch: 32 (step: 72900) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:06:43 | INFO | epoch: 32 (step: 72950) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:06:49 | INFO | epoch: 32 (step: 73000) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:06:55 | INFO | epoch: 32 (step: 73050) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 04:07:01 | INFO | epoch: 32 (step: 73100) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:07:08 | INFO | epoch: 32 (step: 73150) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:07:14 | INFO | epoch: 32 (step: 73200) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:07:20 | INFO | epoch: 32 (step: 73250) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:07:26 | INFO | epoch: 32 (step: 73300) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:07:32 | INFO | epoch: 32 (step: 73350) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 04:07:38 | INFO | epoch: 32 (step: 73400) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:07:44 | INFO | epoch: 32 (step: 73450) | train loss: 0.000109 | lr: 0.000100\n",
            "2025-04-01 04:07:50 | INFO | epoch: 32 (step: 73500) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:07:56 | INFO | epoch: 32 (step: 73550) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:08:02 | INFO | epoch: 32 (step: 73600) | train loss: 0.000098 | lr: 0.000100\n",
            "2025-04-01 04:08:08 | INFO | epoch: 32 (step: 73650) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:08:14 | INFO | epoch: 32 (step: 73700) | train loss: 0.000088 | lr: 0.000100\n",
            "2025-04-01 04:08:20 | INFO | epoch: 32 (step: 73750) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:08:27 | INFO | epoch: 32 (step: 73800) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 04:08:33 | INFO | epoch: 32 (step: 73850) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:08:39 | INFO | epoch: 32 (step: 73900) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:08:45 | INFO | epoch: 32 (step: 73950) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 04:08:51 | INFO | epoch: 32 (step: 74000) | train loss: 0.000083 | lr: 0.000100\n",
            "2025-04-01 04:08:57 | INFO | epoch: 32 (step: 74050) | train loss: 0.000163 | lr: 0.000100\n",
            "2025-04-01 04:09:03 | INFO | epoch: 32 (step: 74100) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:09:09 | INFO | epoch: 32 (step: 74150) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:09:15 | INFO | epoch: 32 (step: 74200) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:09:21 | INFO | epoch: 32 (step: 74250) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 04:09:27 | INFO | epoch: 32 (step: 74300) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 04:09:33 | INFO | epoch: 32 (step: 74350) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:09:39 | INFO | epoch: 32 (step: 74400) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 04:09:45 | INFO | epoch: 32 (step: 74450) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:09:52 | INFO | epoch: 32 (step: 74500) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:09:59 | INFO | epoch: 33 (step: 74550) | train loss: 0.000117 | lr: 0.000100\n",
            "2025-04-01 04:10:05 | INFO | epoch: 33 (step: 74600) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 04:10:11 | INFO | epoch: 33 (step: 74650) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:10:17 | INFO | epoch: 33 (step: 74700) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:10:23 | INFO | epoch: 33 (step: 74750) | train loss: 0.000118 | lr: 0.000100\n",
            "2025-04-01 04:10:29 | INFO | epoch: 33 (step: 74800) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:10:35 | INFO | epoch: 33 (step: 74850) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:10:41 | INFO | epoch: 33 (step: 74900) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 04:10:48 | INFO | epoch: 33 (step: 74950) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:10:54 | INFO | epoch: 33 (step: 75000) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:11:01 | INFO | epoch: 33 (step: 75000) | valid_loss: 0.002295 | valid_loss: 0.002295\n",
            "2025-04-01 04:11:05 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:11:11 | INFO | epoch: 33 (step: 75050) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 04:11:17 | INFO | epoch: 33 (step: 75100) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:11:23 | INFO | epoch: 33 (step: 75150) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:11:29 | INFO | epoch: 33 (step: 75200) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:11:35 | INFO | epoch: 33 (step: 75250) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 04:11:41 | INFO | epoch: 33 (step: 75300) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:11:47 | INFO | epoch: 33 (step: 75350) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:11:53 | INFO | epoch: 33 (step: 75400) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:11:59 | INFO | epoch: 33 (step: 75450) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:12:05 | INFO | epoch: 33 (step: 75500) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:12:11 | INFO | epoch: 33 (step: 75550) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 04:12:17 | INFO | epoch: 33 (step: 75600) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 04:12:23 | INFO | epoch: 33 (step: 75650) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:12:29 | INFO | epoch: 33 (step: 75700) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:12:36 | INFO | epoch: 33 (step: 75750) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:12:42 | INFO | epoch: 33 (step: 75800) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:12:48 | INFO | epoch: 33 (step: 75850) | train loss: 0.000089 | lr: 0.000100\n",
            "2025-04-01 04:12:54 | INFO | epoch: 33 (step: 75900) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:13:00 | INFO | epoch: 33 (step: 75950) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:13:06 | INFO | epoch: 33 (step: 76000) | train loss: 0.000097 | lr: 0.000100\n",
            "2025-04-01 04:13:12 | INFO | epoch: 33 (step: 76050) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:13:18 | INFO | epoch: 33 (step: 76100) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 04:13:24 | INFO | epoch: 33 (step: 76150) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 04:13:30 | INFO | epoch: 33 (step: 76200) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:13:36 | INFO | epoch: 33 (step: 76250) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:13:43 | INFO | epoch: 33 (step: 76300) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:13:49 | INFO | epoch: 33 (step: 76350) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:13:55 | INFO | epoch: 33 (step: 76400) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:14:01 | INFO | epoch: 33 (step: 76450) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 04:14:07 | INFO | epoch: 33 (step: 76500) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:14:13 | INFO | epoch: 33 (step: 76550) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:14:19 | INFO | epoch: 33 (step: 76600) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:14:25 | INFO | epoch: 33 (step: 76650) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:14:31 | INFO | epoch: 33 (step: 76700) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:14:37 | INFO | epoch: 33 (step: 76750) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:14:43 | INFO | epoch: 33 (step: 76800) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:14:49 | INFO | epoch: 33 (step: 76850) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:14:57 | INFO | epoch: 34 (step: 76900) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 04:15:03 | INFO | epoch: 34 (step: 76950) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:15:09 | INFO | epoch: 34 (step: 77000) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:15:15 | INFO | epoch: 34 (step: 77050) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:15:21 | INFO | epoch: 34 (step: 77100) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:15:28 | INFO | epoch: 34 (step: 77150) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 04:15:34 | INFO | epoch: 34 (step: 77200) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:15:40 | INFO | epoch: 34 (step: 77250) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:15:46 | INFO | epoch: 34 (step: 77300) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:15:52 | INFO | epoch: 34 (step: 77350) | train loss: 0.000079 | lr: 0.000100\n",
            "2025-04-01 04:15:58 | INFO | epoch: 34 (step: 77400) | train loss: 0.000141 | lr: 0.000100\n",
            "2025-04-01 04:16:04 | INFO | epoch: 34 (step: 77450) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:16:10 | INFO | epoch: 34 (step: 77500) | train loss: 0.000113 | lr: 0.000100\n",
            "2025-04-01 04:16:16 | INFO | epoch: 34 (step: 77550) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:16:22 | INFO | epoch: 34 (step: 77600) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 04:16:28 | INFO | epoch: 34 (step: 77650) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:16:34 | INFO | epoch: 34 (step: 77700) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 04:16:40 | INFO | epoch: 34 (step: 77750) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:16:46 | INFO | epoch: 34 (step: 77800) | train loss: 0.000070 | lr: 0.000100\n",
            "2025-04-01 04:16:52 | INFO | epoch: 34 (step: 77850) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:16:59 | INFO | epoch: 34 (step: 77900) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:17:05 | INFO | epoch: 34 (step: 77950) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:17:11 | INFO | epoch: 34 (step: 78000) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 04:17:17 | INFO | epoch: 34 (step: 78050) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 04:17:23 | INFO | epoch: 34 (step: 78100) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:17:29 | INFO | epoch: 34 (step: 78150) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:17:35 | INFO | epoch: 34 (step: 78200) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 04:17:41 | INFO | epoch: 34 (step: 78250) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:17:47 | INFO | epoch: 34 (step: 78300) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:17:53 | INFO | epoch: 34 (step: 78350) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:17:59 | INFO | epoch: 34 (step: 78400) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:18:05 | INFO | epoch: 34 (step: 78450) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:18:11 | INFO | epoch: 34 (step: 78500) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:18:18 | INFO | epoch: 34 (step: 78550) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:18:24 | INFO | epoch: 34 (step: 78600) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:18:30 | INFO | epoch: 34 (step: 78650) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:18:36 | INFO | epoch: 34 (step: 78700) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:18:42 | INFO | epoch: 34 (step: 78750) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:18:48 | INFO | epoch: 34 (step: 78800) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:18:54 | INFO | epoch: 34 (step: 78850) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:19:00 | INFO | epoch: 34 (step: 78900) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:19:06 | INFO | epoch: 34 (step: 78950) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:19:12 | INFO | epoch: 34 (step: 79000) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:19:18 | INFO | epoch: 34 (step: 79050) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:19:24 | INFO | epoch: 34 (step: 79100) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:19:31 | INFO | epoch: 34 (step: 79150) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:19:38 | INFO | epoch: 35 (step: 79200) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:19:45 | INFO | epoch: 35 (step: 79250) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 04:19:51 | INFO | epoch: 35 (step: 79300) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 04:19:57 | INFO | epoch: 35 (step: 79350) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 04:20:03 | INFO | epoch: 35 (step: 79400) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:20:09 | INFO | epoch: 35 (step: 79450) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:20:15 | INFO | epoch: 35 (step: 79500) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:20:21 | INFO | epoch: 35 (step: 79550) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:20:27 | INFO | epoch: 35 (step: 79600) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:20:33 | INFO | epoch: 35 (step: 79650) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 04:20:40 | INFO | epoch: 35 (step: 79700) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:20:46 | INFO | epoch: 35 (step: 79750) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:20:52 | INFO | epoch: 35 (step: 79800) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:20:58 | INFO | epoch: 35 (step: 79850) | train loss: 0.000087 | lr: 0.000100\n",
            "2025-04-01 04:21:04 | INFO | epoch: 35 (step: 79900) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:21:10 | INFO | epoch: 35 (step: 79950) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:21:16 | INFO | epoch: 35 (step: 80000) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:21:23 | INFO | epoch: 35 (step: 80000) | valid_loss: 0.001656 | valid_loss: 0.001656\n",
            "2025-04-01 04:21:28 | INFO | epoch: 35 (step: 80000) | best metric updated (loss) - 0.001656\n",
            "2025-04-01 04:21:32 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:21:38 | INFO | epoch: 35 (step: 80050) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:21:44 | INFO | epoch: 35 (step: 80100) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:21:50 | INFO | epoch: 35 (step: 80150) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:21:56 | INFO | epoch: 35 (step: 80200) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:22:02 | INFO | epoch: 35 (step: 80250) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 04:22:08 | INFO | epoch: 35 (step: 80300) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:22:14 | INFO | epoch: 35 (step: 80350) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:22:20 | INFO | epoch: 35 (step: 80400) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:22:26 | INFO | epoch: 35 (step: 80450) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:22:33 | INFO | epoch: 35 (step: 80500) | train loss: 0.000138 | lr: 0.000100\n",
            "2025-04-01 04:22:39 | INFO | epoch: 35 (step: 80550) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:22:45 | INFO | epoch: 35 (step: 80600) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:22:51 | INFO | epoch: 35 (step: 80650) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:22:57 | INFO | epoch: 35 (step: 80700) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:23:03 | INFO | epoch: 35 (step: 80750) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 04:23:09 | INFO | epoch: 35 (step: 80800) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:23:15 | INFO | epoch: 35 (step: 80850) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:23:21 | INFO | epoch: 35 (step: 80900) | train loss: 0.000156 | lr: 0.000100\n",
            "2025-04-01 04:23:27 | INFO | epoch: 35 (step: 80950) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:23:34 | INFO | epoch: 35 (step: 81000) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:23:40 | INFO | epoch: 35 (step: 81050) | train loss: 0.000067 | lr: 0.000100\n",
            "2025-04-01 04:23:46 | INFO | epoch: 35 (step: 81100) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:23:52 | INFO | epoch: 35 (step: 81150) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:23:58 | INFO | epoch: 35 (step: 81200) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 04:24:04 | INFO | epoch: 35 (step: 81250) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 04:24:10 | INFO | epoch: 35 (step: 81300) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:24:16 | INFO | epoch: 35 (step: 81350) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:24:22 | INFO | epoch: 35 (step: 81400) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:24:28 | INFO | epoch: 35 (step: 81450) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:24:34 | INFO | epoch: 35 (step: 81500) | train loss: 0.000105 | lr: 0.000100\n",
            "2025-04-01 04:24:42 | INFO | epoch: 36 (step: 81550) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:24:48 | INFO | epoch: 36 (step: 81600) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:24:54 | INFO | epoch: 36 (step: 81650) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:25:00 | INFO | epoch: 36 (step: 81700) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:25:06 | INFO | epoch: 36 (step: 81750) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:25:12 | INFO | epoch: 36 (step: 81800) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:25:19 | INFO | epoch: 36 (step: 81850) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 04:25:25 | INFO | epoch: 36 (step: 81900) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:25:31 | INFO | epoch: 36 (step: 81950) | train loss: 0.000099 | lr: 0.000100\n",
            "2025-04-01 04:25:37 | INFO | epoch: 36 (step: 82000) | train loss: 0.000059 | lr: 0.000100\n",
            "2025-04-01 04:25:43 | INFO | epoch: 36 (step: 82050) | train loss: 0.000061 | lr: 0.000100\n",
            "2025-04-01 04:25:49 | INFO | epoch: 36 (step: 82100) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:25:55 | INFO | epoch: 36 (step: 82150) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:26:01 | INFO | epoch: 36 (step: 82200) | train loss: 0.000126 | lr: 0.000100\n",
            "2025-04-01 04:26:07 | INFO | epoch: 36 (step: 82250) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 04:26:13 | INFO | epoch: 36 (step: 82300) | train loss: 0.000071 | lr: 0.000100\n",
            "2025-04-01 04:26:20 | INFO | epoch: 36 (step: 82350) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:26:26 | INFO | epoch: 36 (step: 82400) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:26:32 | INFO | epoch: 36 (step: 82450) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:26:38 | INFO | epoch: 36 (step: 82500) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:26:44 | INFO | epoch: 36 (step: 82550) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:26:50 | INFO | epoch: 36 (step: 82600) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:26:56 | INFO | epoch: 36 (step: 82650) | train loss: 0.000078 | lr: 0.000100\n",
            "2025-04-01 04:27:02 | INFO | epoch: 36 (step: 82700) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 04:27:08 | INFO | epoch: 36 (step: 82750) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 04:27:14 | INFO | epoch: 36 (step: 82800) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:27:20 | INFO | epoch: 36 (step: 82850) | train loss: 0.000063 | lr: 0.000100\n",
            "2025-04-01 04:27:26 | INFO | epoch: 36 (step: 82900) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:27:32 | INFO | epoch: 36 (step: 82950) | train loss: 0.000062 | lr: 0.000100\n",
            "2025-04-01 04:27:38 | INFO | epoch: 36 (step: 83000) | train loss: 0.000096 | lr: 0.000100\n",
            "2025-04-01 04:27:44 | INFO | epoch: 36 (step: 83050) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 04:27:50 | INFO | epoch: 36 (step: 83100) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 04:27:57 | INFO | epoch: 36 (step: 83150) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 04:28:03 | INFO | epoch: 36 (step: 83200) | train loss: 0.000076 | lr: 0.000100\n",
            "2025-04-01 04:28:09 | INFO | epoch: 36 (step: 83250) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:28:15 | INFO | epoch: 36 (step: 83300) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:28:21 | INFO | epoch: 36 (step: 83350) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:28:27 | INFO | epoch: 36 (step: 83400) | train loss: 0.000162 | lr: 0.000100\n",
            "2025-04-01 04:28:33 | INFO | epoch: 36 (step: 83450) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:28:39 | INFO | epoch: 36 (step: 83500) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 04:28:45 | INFO | epoch: 36 (step: 83550) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:28:51 | INFO | epoch: 36 (step: 83600) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:28:57 | INFO | epoch: 36 (step: 83650) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:29:03 | INFO | epoch: 36 (step: 83700) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:29:09 | INFO | epoch: 36 (step: 83750) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:29:15 | INFO | epoch: 36 (step: 83800) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 04:29:23 | INFO | epoch: 37 (step: 83850) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 04:29:29 | INFO | epoch: 37 (step: 83900) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:29:35 | INFO | epoch: 37 (step: 83950) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:29:41 | INFO | epoch: 37 (step: 84000) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:29:48 | INFO | epoch: 37 (step: 84050) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:29:54 | INFO | epoch: 37 (step: 84100) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:30:00 | INFO | epoch: 37 (step: 84150) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:30:06 | INFO | epoch: 37 (step: 84200) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 04:30:12 | INFO | epoch: 37 (step: 84250) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:30:18 | INFO | epoch: 37 (step: 84300) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:30:24 | INFO | epoch: 37 (step: 84350) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:30:31 | INFO | epoch: 37 (step: 84400) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:30:37 | INFO | epoch: 37 (step: 84450) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:30:43 | INFO | epoch: 37 (step: 84500) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:30:49 | INFO | epoch: 37 (step: 84550) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:30:55 | INFO | epoch: 37 (step: 84600) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:31:01 | INFO | epoch: 37 (step: 84650) | train loss: 0.000011 | lr: 0.000100\n",
            "2025-04-01 04:31:07 | INFO | epoch: 37 (step: 84700) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:31:13 | INFO | epoch: 37 (step: 84750) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:31:19 | INFO | epoch: 37 (step: 84800) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:31:25 | INFO | epoch: 37 (step: 84850) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:31:32 | INFO | epoch: 37 (step: 84900) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:31:38 | INFO | epoch: 37 (step: 84950) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:31:44 | INFO | epoch: 37 (step: 85000) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:31:51 | INFO | epoch: 37 (step: 85000) | valid_loss: 0.001654 | valid_loss: 0.001654\n",
            "2025-04-01 04:31:56 | INFO | epoch: 37 (step: 85000) | best metric updated (loss) - 0.001654\n",
            "2025-04-01 04:32:00 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:32:07 | INFO | epoch: 37 (step: 85050) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:32:13 | INFO | epoch: 37 (step: 85100) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:32:19 | INFO | epoch: 37 (step: 85150) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:32:25 | INFO | epoch: 37 (step: 85200) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:32:31 | INFO | epoch: 37 (step: 85250) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:32:37 | INFO | epoch: 37 (step: 85300) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:32:43 | INFO | epoch: 37 (step: 85350) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:32:49 | INFO | epoch: 37 (step: 85400) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:32:55 | INFO | epoch: 37 (step: 85450) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:33:01 | INFO | epoch: 37 (step: 85500) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:33:07 | INFO | epoch: 37 (step: 85550) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:33:13 | INFO | epoch: 37 (step: 85600) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:33:19 | INFO | epoch: 37 (step: 85650) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:33:25 | INFO | epoch: 37 (step: 85700) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:33:31 | INFO | epoch: 37 (step: 85750) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:33:37 | INFO | epoch: 37 (step: 85800) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:33:44 | INFO | epoch: 37 (step: 85850) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:33:49 | INFO | epoch: 37 (step: 85900) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:33:56 | INFO | epoch: 37 (step: 85950) | train loss: 0.000082 | lr: 0.000100\n",
            "2025-04-01 04:34:02 | INFO | epoch: 37 (step: 86000) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:34:08 | INFO | epoch: 37 (step: 86050) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:34:14 | INFO | epoch: 37 (step: 86100) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:34:20 | INFO | epoch: 37 (step: 86150) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:34:27 | INFO | epoch: 38 (step: 86200) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:34:33 | INFO | epoch: 38 (step: 86250) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:34:39 | INFO | epoch: 38 (step: 86300) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:34:45 | INFO | epoch: 38 (step: 86350) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:34:52 | INFO | epoch: 38 (step: 86400) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 04:34:58 | INFO | epoch: 38 (step: 86450) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:35:04 | INFO | epoch: 38 (step: 86500) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:35:10 | INFO | epoch: 38 (step: 86550) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:35:16 | INFO | epoch: 38 (step: 86600) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:35:22 | INFO | epoch: 38 (step: 86650) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:35:28 | INFO | epoch: 38 (step: 86700) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:35:34 | INFO | epoch: 38 (step: 86750) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:35:40 | INFO | epoch: 38 (step: 86800) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:35:46 | INFO | epoch: 38 (step: 86850) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:35:52 | INFO | epoch: 38 (step: 86900) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:35:58 | INFO | epoch: 38 (step: 86950) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 04:36:04 | INFO | epoch: 38 (step: 87000) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:36:10 | INFO | epoch: 38 (step: 87050) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:36:17 | INFO | epoch: 38 (step: 87100) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:36:23 | INFO | epoch: 38 (step: 87150) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:36:29 | INFO | epoch: 38 (step: 87200) | train loss: 0.000056 | lr: 0.000100\n",
            "2025-04-01 04:36:35 | INFO | epoch: 38 (step: 87250) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:36:41 | INFO | epoch: 38 (step: 87300) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 04:36:47 | INFO | epoch: 38 (step: 87350) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:36:53 | INFO | epoch: 38 (step: 87400) | train loss: 0.000085 | lr: 0.000100\n",
            "2025-04-01 04:36:59 | INFO | epoch: 38 (step: 87450) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:37:06 | INFO | epoch: 38 (step: 87500) | train loss: 0.000074 | lr: 0.000100\n",
            "2025-04-01 04:37:12 | INFO | epoch: 38 (step: 87550) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 04:37:18 | INFO | epoch: 38 (step: 87600) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:37:24 | INFO | epoch: 38 (step: 87650) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:37:30 | INFO | epoch: 38 (step: 87700) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:37:36 | INFO | epoch: 38 (step: 87750) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:37:42 | INFO | epoch: 38 (step: 87800) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:37:48 | INFO | epoch: 38 (step: 87850) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:37:54 | INFO | epoch: 38 (step: 87900) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:38:00 | INFO | epoch: 38 (step: 87950) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:38:06 | INFO | epoch: 38 (step: 88000) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:38:13 | INFO | epoch: 38 (step: 88050) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:38:19 | INFO | epoch: 38 (step: 88100) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:38:24 | INFO | epoch: 38 (step: 88150) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:38:31 | INFO | epoch: 38 (step: 88200) | train loss: 0.000013 | lr: 0.000100\n",
            "2025-04-01 04:38:37 | INFO | epoch: 38 (step: 88250) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:38:43 | INFO | epoch: 38 (step: 88300) | train loss: 0.000059 | lr: 0.000100\n",
            "2025-04-01 04:38:49 | INFO | epoch: 38 (step: 88350) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:38:55 | INFO | epoch: 38 (step: 88400) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:39:01 | INFO | epoch: 38 (step: 88450) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:39:07 | INFO | epoch: 38 (step: 88500) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:39:15 | INFO | epoch: 39 (step: 88550) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:39:21 | INFO | epoch: 39 (step: 88600) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:39:27 | INFO | epoch: 39 (step: 88650) | train loss: 0.000055 | lr: 0.000100\n",
            "2025-04-01 04:39:33 | INFO | epoch: 39 (step: 88700) | train loss: 0.000073 | lr: 0.000100\n",
            "2025-04-01 04:39:39 | INFO | epoch: 39 (step: 88750) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 04:39:45 | INFO | epoch: 39 (step: 88800) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:39:52 | INFO | epoch: 39 (step: 88850) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:39:58 | INFO | epoch: 39 (step: 88900) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:40:04 | INFO | epoch: 39 (step: 88950) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:40:10 | INFO | epoch: 39 (step: 89000) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:40:16 | INFO | epoch: 39 (step: 89050) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:40:22 | INFO | epoch: 39 (step: 89100) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:40:28 | INFO | epoch: 39 (step: 89150) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:40:34 | INFO | epoch: 39 (step: 89200) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:40:40 | INFO | epoch: 39 (step: 89250) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:40:47 | INFO | epoch: 39 (step: 89300) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:40:53 | INFO | epoch: 39 (step: 89350) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:40:59 | INFO | epoch: 39 (step: 89400) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:41:05 | INFO | epoch: 39 (step: 89450) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:41:11 | INFO | epoch: 39 (step: 89500) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:41:17 | INFO | epoch: 39 (step: 89550) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:41:23 | INFO | epoch: 39 (step: 89600) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:41:29 | INFO | epoch: 39 (step: 89650) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:41:35 | INFO | epoch: 39 (step: 89700) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:41:41 | INFO | epoch: 39 (step: 89750) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:41:47 | INFO | epoch: 39 (step: 89800) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:41:53 | INFO | epoch: 39 (step: 89850) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:41:59 | INFO | epoch: 39 (step: 89900) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:42:05 | INFO | epoch: 39 (step: 89950) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:42:11 | INFO | epoch: 39 (step: 90000) | train loss: 0.000091 | lr: 0.000100\n",
            "2025-04-01 04:42:19 | INFO | epoch: 39 (step: 90000) | valid_loss: 0.002040 | valid_loss: 0.002040\n",
            "2025-04-01 04:42:22 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:42:28 | INFO | epoch: 39 (step: 90050) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:42:34 | INFO | epoch: 39 (step: 90100) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:42:40 | INFO | epoch: 39 (step: 90150) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:42:46 | INFO | epoch: 39 (step: 90200) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:42:52 | INFO | epoch: 39 (step: 90250) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:42:58 | INFO | epoch: 39 (step: 90300) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:43:04 | INFO | epoch: 39 (step: 90350) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:43:11 | INFO | epoch: 39 (step: 90400) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:43:17 | INFO | epoch: 39 (step: 90450) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:43:23 | INFO | epoch: 39 (step: 90500) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:43:29 | INFO | epoch: 39 (step: 90550) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:43:35 | INFO | epoch: 39 (step: 90600) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:43:41 | INFO | epoch: 39 (step: 90650) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:43:47 | INFO | epoch: 39 (step: 90700) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:43:53 | INFO | epoch: 39 (step: 90750) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:43:59 | INFO | epoch: 39 (step: 90800) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:44:06 | INFO | epoch: 40 (step: 90850) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:44:12 | INFO | epoch: 40 (step: 90900) | train loss: 0.000035 | lr: 0.000100\n",
            "2025-04-01 04:44:18 | INFO | epoch: 40 (step: 90950) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:44:24 | INFO | epoch: 40 (step: 91000) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:44:30 | INFO | epoch: 40 (step: 91050) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:44:36 | INFO | epoch: 40 (step: 91100) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:44:42 | INFO | epoch: 40 (step: 91150) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:44:48 | INFO | epoch: 40 (step: 91200) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:44:54 | INFO | epoch: 40 (step: 91250) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:45:00 | INFO | epoch: 40 (step: 91300) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:45:06 | INFO | epoch: 40 (step: 91350) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:45:12 | INFO | epoch: 40 (step: 91400) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:45:19 | INFO | epoch: 40 (step: 91450) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:45:25 | INFO | epoch: 40 (step: 91500) | train loss: 0.000075 | lr: 0.000100\n",
            "2025-04-01 04:45:31 | INFO | epoch: 40 (step: 91550) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 04:45:37 | INFO | epoch: 40 (step: 91600) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 04:45:43 | INFO | epoch: 40 (step: 91650) | train loss: 0.000065 | lr: 0.000100\n",
            "2025-04-01 04:45:49 | INFO | epoch: 40 (step: 91700) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:45:55 | INFO | epoch: 40 (step: 91750) | train loss: 0.000066 | lr: 0.000100\n",
            "2025-04-01 04:46:02 | INFO | epoch: 40 (step: 91800) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:46:08 | INFO | epoch: 40 (step: 91850) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:46:14 | INFO | epoch: 40 (step: 91900) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:46:20 | INFO | epoch: 40 (step: 91950) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:46:26 | INFO | epoch: 40 (step: 92000) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:46:32 | INFO | epoch: 40 (step: 92050) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:46:38 | INFO | epoch: 40 (step: 92100) | train loss: 0.000136 | lr: 0.000100\n",
            "2025-04-01 04:46:44 | INFO | epoch: 40 (step: 92150) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:46:50 | INFO | epoch: 40 (step: 92200) | train loss: 0.000054 | lr: 0.000100\n",
            "2025-04-01 04:46:56 | INFO | epoch: 40 (step: 92250) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:47:02 | INFO | epoch: 40 (step: 92300) | train loss: 0.000081 | lr: 0.000100\n",
            "2025-04-01 04:47:09 | INFO | epoch: 40 (step: 92350) | train loss: 0.000068 | lr: 0.000100\n",
            "2025-04-01 04:47:15 | INFO | epoch: 40 (step: 92400) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:47:21 | INFO | epoch: 40 (step: 92450) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:47:27 | INFO | epoch: 40 (step: 92500) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:47:33 | INFO | epoch: 40 (step: 92550) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:47:39 | INFO | epoch: 40 (step: 92600) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:47:45 | INFO | epoch: 40 (step: 92650) | train loss: 0.000072 | lr: 0.000100\n",
            "2025-04-01 04:47:51 | INFO | epoch: 40 (step: 92700) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:47:57 | INFO | epoch: 40 (step: 92750) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:48:03 | INFO | epoch: 40 (step: 92800) | train loss: 0.000014 | lr: 0.000100\n",
            "2025-04-01 04:48:09 | INFO | epoch: 40 (step: 92850) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:48:15 | INFO | epoch: 40 (step: 92900) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:48:22 | INFO | epoch: 40 (step: 92950) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:48:28 | INFO | epoch: 40 (step: 93000) | train loss: 0.000043 | lr: 0.000100\n",
            "2025-04-01 04:48:34 | INFO | epoch: 40 (step: 93050) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:48:40 | INFO | epoch: 40 (step: 93100) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:48:46 | INFO | epoch: 40 (step: 93150) | train loss: 0.000037 | lr: 0.000100\n",
            "2025-04-01 04:48:54 | INFO | epoch: 41 (step: 93200) | train loss: 0.000032 | lr: 0.000100\n",
            "2025-04-01 04:49:00 | INFO | epoch: 41 (step: 93250) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:49:06 | INFO | epoch: 41 (step: 93300) | train loss: 0.000036 | lr: 0.000100\n",
            "2025-04-01 04:49:12 | INFO | epoch: 41 (step: 93350) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:49:18 | INFO | epoch: 41 (step: 93400) | train loss: 0.000049 | lr: 0.000100\n",
            "2025-04-01 04:49:24 | INFO | epoch: 41 (step: 93450) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:49:30 | INFO | epoch: 41 (step: 93500) | train loss: 0.000120 | lr: 0.000100\n",
            "2025-04-01 04:49:36 | INFO | epoch: 41 (step: 93550) | train loss: 0.000077 | lr: 0.000100\n",
            "2025-04-01 04:49:42 | INFO | epoch: 41 (step: 93600) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:49:48 | INFO | epoch: 41 (step: 93650) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:49:54 | INFO | epoch: 41 (step: 93700) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:50:01 | INFO | epoch: 41 (step: 93750) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:50:07 | INFO | epoch: 41 (step: 93800) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:50:13 | INFO | epoch: 41 (step: 93850) | train loss: 0.000050 | lr: 0.000100\n",
            "2025-04-01 04:50:19 | INFO | epoch: 41 (step: 93900) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:50:25 | INFO | epoch: 41 (step: 93950) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:50:31 | INFO | epoch: 41 (step: 94000) | train loss: 0.000038 | lr: 0.000100\n",
            "2025-04-01 04:50:37 | INFO | epoch: 41 (step: 94050) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:50:43 | INFO | epoch: 41 (step: 94100) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:50:49 | INFO | epoch: 41 (step: 94150) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:50:55 | INFO | epoch: 41 (step: 94200) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:51:01 | INFO | epoch: 41 (step: 94250) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:51:07 | INFO | epoch: 41 (step: 94300) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:51:14 | INFO | epoch: 41 (step: 94350) | train loss: 0.000092 | lr: 0.000100\n",
            "2025-04-01 04:51:20 | INFO | epoch: 41 (step: 94400) | train loss: 0.000053 | lr: 0.000100\n",
            "2025-04-01 04:51:26 | INFO | epoch: 41 (step: 94450) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:51:32 | INFO | epoch: 41 (step: 94500) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:51:38 | INFO | epoch: 41 (step: 94550) | train loss: 0.000051 | lr: 0.000100\n",
            "2025-04-01 04:51:44 | INFO | epoch: 41 (step: 94600) | train loss: 0.000057 | lr: 0.000100\n",
            "2025-04-01 04:51:50 | INFO | epoch: 41 (step: 94650) | train loss: 0.000048 | lr: 0.000100\n",
            "2025-04-01 04:51:56 | INFO | epoch: 41 (step: 94700) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:52:03 | INFO | epoch: 41 (step: 94750) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:52:09 | INFO | epoch: 41 (step: 94800) | train loss: 0.000109 | lr: 0.000100\n",
            "2025-04-01 04:52:15 | INFO | epoch: 41 (step: 94850) | train loss: 0.000023 | lr: 0.000100\n",
            "2025-04-01 04:52:21 | INFO | epoch: 41 (step: 94900) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:52:27 | INFO | epoch: 41 (step: 94950) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:52:33 | INFO | epoch: 41 (step: 95000) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:52:40 | INFO | epoch: 41 (step: 95000) | valid_loss: 0.002108 | valid_loss: 0.002108\n",
            "2025-04-01 04:52:43 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 04:52:49 | INFO | epoch: 41 (step: 95050) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:52:56 | INFO | epoch: 41 (step: 95100) | train loss: 0.000064 | lr: 0.000100\n",
            "2025-04-01 04:53:02 | INFO | epoch: 41 (step: 95150) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 04:53:08 | INFO | epoch: 41 (step: 95200) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:53:14 | INFO | epoch: 41 (step: 95250) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:53:20 | INFO | epoch: 41 (step: 95300) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 04:53:26 | INFO | epoch: 41 (step: 95350) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:53:32 | INFO | epoch: 41 (step: 95400) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:53:38 | INFO | epoch: 41 (step: 95450) | train loss: 0.000014 | lr: 0.000100\n",
            "2025-04-01 04:53:46 | INFO | epoch: 42 (step: 95500) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:53:52 | INFO | epoch: 42 (step: 95550) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:53:58 | INFO | epoch: 42 (step: 95600) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:54:04 | INFO | epoch: 42 (step: 95650) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:54:11 | INFO | epoch: 42 (step: 95700) | train loss: 0.000058 | lr: 0.000100\n",
            "2025-04-01 04:54:17 | INFO | epoch: 42 (step: 95750) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:54:23 | INFO | epoch: 42 (step: 95800) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:54:29 | INFO | epoch: 42 (step: 95850) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:54:35 | INFO | epoch: 42 (step: 95900) | train loss: 0.000014 | lr: 0.000100\n",
            "2025-04-01 04:54:41 | INFO | epoch: 42 (step: 95950) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:54:47 | INFO | epoch: 42 (step: 96000) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:54:53 | INFO | epoch: 42 (step: 96050) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 04:54:59 | INFO | epoch: 42 (step: 96100) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 04:55:05 | INFO | epoch: 42 (step: 96150) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:55:11 | INFO | epoch: 42 (step: 96200) | train loss: 0.000009 | lr: 0.000100\n",
            "2025-04-01 04:55:17 | INFO | epoch: 42 (step: 96250) | train loss: 0.000011 | lr: 0.000100\n",
            "2025-04-01 04:55:23 | INFO | epoch: 42 (step: 96300) | train loss: 0.000010 | lr: 0.000100\n",
            "2025-04-01 04:55:29 | INFO | epoch: 42 (step: 96350) | train loss: 0.000006 | lr: 0.000100\n",
            "2025-04-01 04:55:35 | INFO | epoch: 42 (step: 96400) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 04:55:42 | INFO | epoch: 42 (step: 96450) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:55:48 | INFO | epoch: 42 (step: 96500) | train loss: 0.000008 | lr: 0.000100\n",
            "2025-04-01 04:55:54 | INFO | epoch: 42 (step: 96550) | train loss: 0.000014 | lr: 0.000100\n",
            "2025-04-01 04:56:00 | INFO | epoch: 42 (step: 96600) | train loss: 0.000045 | lr: 0.000100\n",
            "2025-04-01 04:56:06 | INFO | epoch: 42 (step: 96650) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 04:56:12 | INFO | epoch: 42 (step: 96700) | train loss: 0.000029 | lr: 0.000100\n",
            "2025-04-01 04:56:18 | INFO | epoch: 42 (step: 96750) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:56:24 | INFO | epoch: 42 (step: 96800) | train loss: 0.000012 | lr: 0.000100\n",
            "2025-04-01 04:56:30 | INFO | epoch: 42 (step: 96850) | train loss: 0.000041 | lr: 0.000100\n",
            "2025-04-01 04:56:37 | INFO | epoch: 42 (step: 96900) | train loss: 0.000013 | lr: 0.000100\n",
            "2025-04-01 04:56:43 | INFO | epoch: 42 (step: 96950) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:56:49 | INFO | epoch: 42 (step: 97000) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:56:55 | INFO | epoch: 42 (step: 97050) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:57:01 | INFO | epoch: 42 (step: 97100) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:57:07 | INFO | epoch: 42 (step: 97150) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:57:14 | INFO | epoch: 42 (step: 97200) | train loss: 0.000007 | lr: 0.000100\n",
            "2025-04-01 04:57:20 | INFO | epoch: 42 (step: 97250) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 04:57:26 | INFO | epoch: 42 (step: 97300) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:57:32 | INFO | epoch: 42 (step: 97350) | train loss: 0.000027 | lr: 0.000100\n",
            "2025-04-01 04:57:38 | INFO | epoch: 42 (step: 97400) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 04:57:44 | INFO | epoch: 42 (step: 97450) | train loss: 0.000069 | lr: 0.000100\n",
            "2025-04-01 04:57:50 | INFO | epoch: 42 (step: 97500) | train loss: 0.000031 | lr: 0.000100\n",
            "2025-04-01 04:57:56 | INFO | epoch: 42 (step: 97550) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:58:02 | INFO | epoch: 42 (step: 97600) | train loss: 0.000011 | lr: 0.000100\n",
            "2025-04-01 04:58:08 | INFO | epoch: 42 (step: 97650) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:58:14 | INFO | epoch: 42 (step: 97700) | train loss: 0.000009 | lr: 0.000100\n",
            "2025-04-01 04:58:21 | INFO | epoch: 42 (step: 97750) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 04:58:27 | INFO | epoch: 42 (step: 97800) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 04:58:34 | INFO | epoch: 43 (step: 97850) | train loss: 0.000060 | lr: 0.000100\n",
            "2025-04-01 04:58:41 | INFO | epoch: 43 (step: 97900) | train loss: 0.000039 | lr: 0.000100\n",
            "2025-04-01 04:58:47 | INFO | epoch: 43 (step: 97950) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 04:58:53 | INFO | epoch: 43 (step: 98000) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 04:58:59 | INFO | epoch: 43 (step: 98050) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 04:59:05 | INFO | epoch: 43 (step: 98100) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 04:59:11 | INFO | epoch: 43 (step: 98150) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:59:17 | INFO | epoch: 43 (step: 98200) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 04:59:24 | INFO | epoch: 43 (step: 98250) | train loss: 0.000047 | lr: 0.000100\n",
            "2025-04-01 04:59:30 | INFO | epoch: 43 (step: 98300) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 04:59:36 | INFO | epoch: 43 (step: 98350) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 04:59:42 | INFO | epoch: 43 (step: 98400) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 04:59:48 | INFO | epoch: 43 (step: 98450) | train loss: 0.000034 | lr: 0.000100\n",
            "2025-04-01 04:59:54 | INFO | epoch: 43 (step: 98500) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 05:00:00 | INFO | epoch: 43 (step: 98550) | train loss: 0.000093 | lr: 0.000100\n",
            "2025-04-01 05:00:06 | INFO | epoch: 43 (step: 98600) | train loss: 0.000019 | lr: 0.000100\n",
            "2025-04-01 05:00:12 | INFO | epoch: 43 (step: 98650) | train loss: 0.000044 | lr: 0.000100\n",
            "2025-04-01 05:00:18 | INFO | epoch: 43 (step: 98700) | train loss: 0.000052 | lr: 0.000100\n",
            "2025-04-01 05:00:24 | INFO | epoch: 43 (step: 98750) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 05:00:30 | INFO | epoch: 43 (step: 98800) | train loss: 0.000046 | lr: 0.000100\n",
            "2025-04-01 05:00:36 | INFO | epoch: 43 (step: 98850) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 05:00:43 | INFO | epoch: 43 (step: 98900) | train loss: 0.000042 | lr: 0.000100\n",
            "2025-04-01 05:00:49 | INFO | epoch: 43 (step: 98950) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 05:00:55 | INFO | epoch: 43 (step: 99000) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 05:01:01 | INFO | epoch: 43 (step: 99050) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 05:01:07 | INFO | epoch: 43 (step: 99100) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 05:01:13 | INFO | epoch: 43 (step: 99150) | train loss: 0.000018 | lr: 0.000100\n",
            "2025-04-01 05:01:20 | INFO | epoch: 43 (step: 99200) | train loss: 0.000028 | lr: 0.000100\n",
            "2025-04-01 05:01:26 | INFO | epoch: 43 (step: 99250) | train loss: 0.000022 | lr: 0.000100\n",
            "2025-04-01 05:01:32 | INFO | epoch: 43 (step: 99300) | train loss: 0.000025 | lr: 0.000100\n",
            "2025-04-01 05:01:38 | INFO | epoch: 43 (step: 99350) | train loss: 0.000016 | lr: 0.000100\n",
            "2025-04-01 05:01:44 | INFO | epoch: 43 (step: 99400) | train loss: 0.000080 | lr: 0.000100\n",
            "2025-04-01 05:01:50 | INFO | epoch: 43 (step: 99450) | train loss: 0.000021 | lr: 0.000100\n",
            "2025-04-01 05:01:56 | INFO | epoch: 43 (step: 99500) | train loss: 0.000026 | lr: 0.000100\n",
            "2025-04-01 05:02:02 | INFO | epoch: 43 (step: 99550) | train loss: 0.000033 | lr: 0.000100\n",
            "2025-04-01 05:02:09 | INFO | epoch: 43 (step: 99600) | train loss: 0.000017 | lr: 0.000100\n",
            "2025-04-01 05:02:15 | INFO | epoch: 43 (step: 99650) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 05:02:21 | INFO | epoch: 43 (step: 99700) | train loss: 0.000030 | lr: 0.000100\n",
            "2025-04-01 05:02:27 | INFO | epoch: 43 (step: 99750) | train loss: 0.000024 | lr: 0.000100\n",
            "2025-04-01 05:02:33 | INFO | epoch: 43 (step: 99800) | train loss: 0.000015 | lr: 0.000100\n",
            "2025-04-01 05:02:39 | INFO | epoch: 43 (step: 99850) | train loss: 0.000010 | lr: 0.000100\n",
            "2025-04-01 05:02:45 | INFO | epoch: 43 (step: 99900) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 05:02:51 | INFO | epoch: 43 (step: 99950) | train loss: 0.000020 | lr: 0.000100\n",
            "2025-04-01 05:02:57 | INFO | epoch: 43 (step: 100000) | train loss: 0.000040 | lr: 0.000100\n",
            "2025-04-01 05:03:05 | INFO | epoch: 43 (step: 100000) | valid_loss: 0.002053 | valid_loss: 0.002053\n",
            "2025-04-01 05:03:08 | INFO | \n",
            "evaluation sample:\n",
            "input: show me the top four most common intakes until 1 year ago?\n",
            "real: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "pred: select d_items.label from d_items where d_items.itemid in ( select t1.itemid from ( select inputevents_cv.itemid, dense_rank() over ( order by count(*) desc ) as c1 from inputevents_cv where datetime(inputevents_cv.charttime) <= datetime(current_time,'-1 year') group by inputevents_cv.itemid ) as t1 where t1.c1 <= 4 )\n",
            "2025-04-01 05:03:08 | INFO | training completed!\n"
          ]
        }
      ],
      "source": [
        "!python T5/main.py --config T5/config/ehrsql/training/ehrsql_mimic3_t5_base.yaml --CUDA_VISIBLE_DEVICES 0 --model_name t5-base\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store Model Output to GCP Bucket"
      ],
      "metadata": {
        "id": "9wVS3IAD2NfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy model output to Google Cloud Bucket\n",
        "!cp -r outputs/ehrsql_mimic3_t5_base /content/cse6250_h1/outputs\n"
      ],
      "metadata": {
        "id": "Z9q2bXKS2XBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqu5fNALDD-l"
      },
      "source": [
        "## Step 4: Model Evaluation\n",
        "\n",
        "In this step, we will evaluate the model's performance across all queries, using the Reliability Score (RS) as our evaluation metric. This will provide a baseline understanding of the model's reliability scroe without filtering for unanswerable queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSixQWCI8n4t"
      },
      "source": [
        "### Prepare for Running Inference\n",
        "\n",
        "Load model from Google Drive to output directory, load SQL Database and get ready to run inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if directory outputs/ehrsql_mimic3_t5_base doesn't exist, create it\n",
        "!mkdir -p outputs/ehrsql_mimic3_t5_base\n",
        "\n",
        "#copy model from GCP bucket to output directory\n",
        "!cp -r /content/cse6250_h1/outputs/ehrsql_mimic3_t5_base/* outputs/ehrsql_mimic3_t5_base\n",
        "\n",
        "#copy SQL lite database from Google cloud\n",
        "!cp -r /content/cse6250_h1/mimic_iii.sql* dataset/ehrsql/mimic_iii/\n"
      ],
      "metadata": {
        "id": "TzlUQWuz2wlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List output\n",
        "!ls outputs/ehrsql_mimic3_t5_base/*\n",
        "\n",
        "# cleanup output directory for inferences\n",
        "!rm -rf outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid\n",
        "!rm -rf outputs/eval_ehrsql_mimic3_t5_base__mimic3_test\n",
        "\n",
        "!ls -al outputs/ehrsql_mimic3_t5_base\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa4BzlX7-h9l",
        "outputId": "96b6b9d0-0f35-4a77-8803-a40e489bc940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs/ehrsql_mimic3_t5_base/checkpoint_best.pth.tar  outputs/ehrsql_mimic3_t5_base/train.log\n",
            "total 2612708\n",
            "drwxr-xr-x 2 root root       4096 Apr  4 19:38 .\n",
            "drwxr-xr-x 4 root root       4096 Apr  4 20:41 ..\n",
            "-rw-r--r-- 1 root root 2675193510 Apr  4 19:38 checkpoint_best.pth.tar\n",
            "-rw-r--r-- 1 root root     201226 Apr  4 19:38 train.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference with Validation Data"
      ],
      "metadata": {
        "id": "DqsczeokxHvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inferences for validation data\n",
        "!python T5/main.py --config T5/config/ehrsql/eval/ehrsql_mimic3_t5_base__mimic3_valid.yaml --output_file prediction_raw.json --CUDA_VISIBLE_DEVICES 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPobXs9X-H62",
        "outputId": "95ebe1b4-dc31-4aaf-b3b3-3d581521af88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda:0\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 16.1MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 15.9MB/s]\n",
            "config.json: 100% 1.21k/1.21k [00:00<00:00, 11.0MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "100% 1122/1122 [00:00<00:00, 449948.28it/s]\n",
            "loaded 1122 test examples from dataset/ehrsql/mimic_iii/valid.json\n",
            "2025-04-04 19:39:08.825476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743795548.845436    1773 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743795548.851638    1773 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-04 19:39:08.871790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 892M/892M [00:08<00:00, 100MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.35MB/s]\n",
            "loading checkpoint outputs/ehrsql_mimic3_t5_base/checkpoint_best.pth.tar\n",
            "start inference\n",
            "inference took 1012.574834 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inference with Test Data"
      ],
      "metadata": {
        "id": "DnGrAXrTxPdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inferences for test data\n",
        "!python T5/main.py --config T5/config/ehrsql/eval/ehrsql_mimic3_t5_base__mimic3_test.yaml --output_file prediction_raw.json --CUDA_VISIBLE_DEVICES 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5swQdTpfu7o",
        "outputId": "f8afb834-5b4e-464e-f25e-e913c52b6acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda:0\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "100% 1786/1786 [00:00<00:00, 534996.93it/s]\n",
            "loaded 1786 test examples from dataset/ehrsql/mimic_iii/test.json\n",
            "2025-04-04 20:41:32.411255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743799292.438276   21907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743799292.446963   21907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-04 20:41:32.467736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "loading checkpoint outputs/ehrsql_mimic3_t5_base/checkpoint_best.pth.tar\n",
            "start inference\n",
            "inference took 1367.003721 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run abstention with or without threshold to generate final files\n",
        "\n",
        "We save these predictions to a JSON file in a designated results directory, creating the directory if necessary.  The JSON files are then moved to GCP to allow for generating performance metrics."
      ],
      "metadata": {
        "id": "cDU0OxnfZHy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run abstention wihtout threshold for validation data\n",
        "!python T5/abstain_with_entropy.py --inference_result_path outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid --input_file prediction_raw.json --output_file prediction.no_threshold.json --threshold -1\n",
        "\n",
        "# Run abstention with threshold of 0.14923561\n",
        "!python T5/abstain_with_entropy.py --inference_result_path outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid --input_file prediction_raw.json --output_file prediction.fixed_threshold.json --threshold 0.14923561\n",
        "\n",
        "# Backup all predictions to GCP buucket\n",
        "!cp -r outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid /content/cse6250_h1/outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUDFsQvdZE21",
        "outputId": "685998bb-19c0-4aaa-891e-23d2094527ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EHRSQL/T5/abstain_with_entropy.py:18: UserWarning: Threshold value is not set! All predictions are sent to the database.\n",
            "  warnings.warn(\"Threshold value is not set! All predictions are sent to the database.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run abstention wihtout threshold for test data\n",
        "!python T5/abstain_with_entropy.py --inference_result_path outputs/eval_ehrsql_mimic3_t5_base__mimic3_test --input_file prediction_raw.json --output_file prediction.no_threshold.json --threshold -1\n",
        "\n",
        "# Run abstention with threshold of 0.14923561\n",
        "!python T5/abstain_with_entropy.py --inference_result_path outputs/eval_ehrsql_mimic3_t5_base__mimic3_test --input_file prediction_raw.json --output_file prediction.fixed_threshold.json --threshold 0.14923561\n",
        "\n",
        "# Backup all predictions to GCP buucket\n",
        "!cp -r outputs/eval_ehrsql_mimic3_t5_base__mimic3_test /content/cse6250_h1/outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfzOSiy-niXN",
        "outputId": "60665d38-cae2-43d9-8b72-886dd55062ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EHRSQL/T5/abstain_with_entropy.py:18: UserWarning: Threshold value is not set! All predictions are sent to the database.\n",
            "  warnings.warn(\"Threshold value is not set! All predictions are sent to the database.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Performance\n"
      ],
      "metadata": {
        "id": "ZPkxqciK21lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Validation Data\n",
        "\n",
        "In this step we evaluate performance of the results produced in prior steps with the JSON produced for Validation Data"
      ],
      "metadata": {
        "id": "h7C8_HpLx1KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-nYXV6msJgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8810708b-afaf-4633-b232-b42ce7cccf2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key Metrics for Validation Data - without Scehma\n",
            "With No Threshold\n",
            "{\n",
            "  \"precision_ans\": 67.74,\n",
            "  \"recall_ans\": 100.0,\n",
            "  \"f1_ans\": 80.77,\n",
            "  \"precision_exec\": 65.51,\n",
            "  \"recall_exec\": 96.71,\n",
            "  \"f1_exec\": 78.11\n",
            "}\n",
            "With Fixed Threshold\n",
            "{\n",
            "  \"precision_ans\": 94.5,\n",
            "  \"recall_ans\": 94.87,\n",
            "  \"f1_ans\": 94.68,\n",
            "  \"precision_exec\": 93.58,\n",
            "  \"recall_exec\": 93.95,\n",
            "  \"f1_exec\": 93.76\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Check if the output already exists on\n",
        "# outputs/eval_ehrsql_mimic3_t5_base__mimic3_test if not copy it from GCP- this\n",
        "# step allows for repeatable runs without running inference\n",
        "\n",
        "!if [ ! -f \"outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid\" ]; then cp -r /content/cse6250_h1/outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid/ outputs/; fi\n",
        "\n",
        "!echo \"Key Metrics for Validation Data - without Scehma\"\n",
        "\n",
        "!echo \"With No Threshold\"\n",
        "!python evaluate.py --db_path ./dataset/ehrsql/mimic_iii/mimic_iii.sqlite --data_file dataset/ehrsql/mimic_iii/valid.json --pred_file ./outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid/prediction.no_threshold.json\n",
        "\n",
        "!echo \"With Fixed Threshold\"\n",
        "!python evaluate.py --db_path ./dataset/ehrsql/mimic_iii/mimic_iii.sqlite --data_file dataset/ehrsql/mimic_iii/valid.json --pred_file ./outputs/eval_ehrsql_mimic3_t5_base__mimic3_valid/prediction.fixed_threshold.json\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### With Test Data\n",
        "\n",
        "In this step we evaluate performance of the results produced in prior steps with the JSON produced for Test Data"
      ],
      "metadata": {
        "id": "6NPgaN3ooU59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the output already exists on\n",
        "# outputs/eval_ehrsql_mimic3_t5_base__mimic3_test if not copy it from GCP- this\n",
        "# step allows for repeatable runs without running inference\n",
        "\n",
        "!if [ ! -f \"outputs/eval_ehrsql_mimic3_t5_base__mimic3_test\" ]; then cp -r /content/cse6250_h1/outputs/eval_ehrsql_mimic3_t5_base__mimic3_test/ outputs/; fi\n",
        "\n",
        "!echo \"Key Metrics for Test Data - without Scehma\"\n",
        "\n",
        "!echo \"With No Threshold\"\n",
        "!python evaluate.py --db_path ./dataset/ehrsql/mimic_iii/mimic_iii.sqlite --data_file dataset/ehrsql/mimic_iii/test.json --pred_file ./outputs/eval_ehrsql_mimic3_t5_base__mimic3_test/prediction.no_threshold.json\n",
        "\n",
        "!echo \"With Fixed Threshold\"\n",
        "!python evaluate.py --db_path ./dataset/ehrsql/mimic_iii/mimic_iii.sqlite --data_file dataset/ehrsql/mimic_iii/test.json --pred_file ./outputs/eval_ehrsql_mimic3_t5_base__mimic3_test/prediction.fixed_threshold.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Km8zBQofgW",
        "outputId": "9c78ea1a-cfa2-470a-8e5a-8ffbba711796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key Metrics for Test Data - without Scehma\n",
            "With No Threshold\n",
            "{\n",
            "  \"precision_ans\": 67.08,\n",
            "  \"recall_ans\": 100.0,\n",
            "  \"f1_ans\": 80.29,\n",
            "  \"precision_exec\": 65.12,\n",
            "  \"recall_exec\": 97.08,\n",
            "  \"f1_exec\": 77.95\n",
            "}\n",
            "With Fixed Threshold\n",
            "{\n",
            "  \"precision_ans\": 90.37,\n",
            "  \"recall_ans\": 93.99,\n",
            "  \"f1_ans\": 92.14,\n",
            "  \"precision_exec\": 89.57,\n",
            "  \"recall_exec\": 93.16,\n",
            "  \"f1_exec\": 91.33\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}